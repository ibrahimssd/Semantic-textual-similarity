{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13262990",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab6a4f42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-12 21:59:14.504423: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-02-12 21:59:14.504448: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from test import evaluate_test_set\n",
    "import sts_data\n",
    "from importlib import reload\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b85ac35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from preprocess import Preprocess\n",
    "import logging\n",
    "import torch\n",
    "import re\n",
    "from dataset import STSDataset\n",
    "from datasets import load_dataset,Dataset\n",
    "import spacy\n",
    "import torchtext\n",
    "from torchtext.data import get_tokenizer\n",
    "from spacy.lang.en import English\n",
    "from torchtext.legacy.data import Field\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c1e7f6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values(['sentence_A', 'sentence_B', 'relatedness_score'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataset = load_dataset('text', data_files='SICK.txt')\n",
    "# dataset\n",
    "columns_mapping = {\n",
    "        \"sent1\": \"sentence_A\",\n",
    "        \"sent2\": \"sentence_B\",\n",
    "        \"label\": \"relatedness_score\",\n",
    "    }\n",
    "stopwords_path=\"stopwords-en.txt\"\n",
    "columns_mapping.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e44a157",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Using custom data configuration default\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset sick/default (download: 212.48 KiB, generated: 2.50 MiB, post-processed: Unknown size, total: 2.71 MiB) to /home/ibrahimssd/.cache/huggingface/datasets/sick/default/0.0.0/c6b3b0b44eb84b134851396d6d464e5cb8f026960519d640e087fe33472626db...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset sick downloaded and prepared to /home/ibrahimssd/.cache/huggingface/datasets/sick/default/0.0.0/c6b3b0b44eb84b134851396d6d464e5cb8f026960519d640e087fe33472626db. Subsequent calls will reuse this data.\n"
     ]
    }
   ],
   "source": [
    "sick_dataset = load_dataset('sick',download_mode='reuse_cache_if_exists')\n",
    "sick_dataset=sick_dataset.remove_columns(['label','id','entailment_AB', 'entailment_BA', 'sentence_A_original', 'sentence_B_original', 'sentence_A_dataset', 'sentence_B_dataset'])\n",
    "train_pd=pd.DataFrame.from_dict(sick_dataset['train'])\n",
    "validation_pd=pd.DataFrame.from_dict(sick_dataset['validation'])\n",
    "test_pd=pd.DataFrame.from_dict(sick_dataset['test'])\n",
    "sick_data=[train_pd,validation_pd,test_pd]\n",
    "# sick_df = pd.DataFrame(data=sick_dataset.data, columns=sick_dataset.column_names)\n",
    "# sen_A=columns_mapping['sent1']\n",
    "# sen_B= columns_mapping['sent2']\n",
    "# score=columns_mapping['label']\n",
    "# sick_df=sick_df[[sen_A,sen_B,score]]\n",
    "# pd.read_csv(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "be64544e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69.359375"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sick_dataset['train']\n",
    "# Dataset.from_pandas(tran_pd)\n",
    "splits= list(sick_dataset.keys())\n",
    "type(sick_dataset)\n",
    "len(train_data_df)\n",
    "4439/64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de534416",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "\"\"\"\n",
    "Performs basic text cleansing on the unstructured field \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class Preprocess:\n",
    "    def __init__(self, stpwds_file_path):\n",
    "        \"\"\"\n",
    "        Initializes regex patterns and loads stopwords\n",
    "        \"\"\"\n",
    "        # TODO implement\n",
    "        with open(stpwds_file_path) as fh:\n",
    "            self.stopwords=list(set(fh.read().split()))\n",
    "        self.noise_re = re.compile('\\\\b(%s)\\\\W'%('|'.join(map(re.escape,self.stopwords))),re.I)\n",
    "    \n",
    "\n",
    "    def perform_preprocessing(self, data, columns_mapping):\n",
    "        sen_A=columns_mapping['sent1']\n",
    "        sen_B= columns_mapping['sent2']\n",
    "        score=columns_mapping['label']\n",
    "        cleaned_data=[]\n",
    "        for data_frame in data:\n",
    "            groupA=list(data_frame[sen_A])\n",
    "            groupB=list(data_frame[sen_B])\n",
    "            ## normalize text to lower case\n",
    "            groupA=[x.lower() for x in groupA]\n",
    "            groupB=[x.lower() for x in groupB]\n",
    "            ## remove punctuations\n",
    "            groupA=[''.join(c for c in x if c not in string.punctuation) for x in groupA]\n",
    "            groupB=[''.join(c for c in x if c not in string.punctuation) for x in groupB]\n",
    "            ## remove stopwords\n",
    "            groupA=[self.noise_re.sub('',p) for p in groupA]\n",
    "            groupB=[self.noise_re.sub('',p) for p in groupB]\n",
    "            # Trim extra whitespace\n",
    "            groupA=[' '.join(x.split()) for x in groupA]\n",
    "            groupB=[' '.join(x.split()) for x in groupB]\n",
    "            # Remove numbers\n",
    "            groupA=[''.join(c for c in x if c not in '0123456789') for x in groupA]\n",
    "            groupB=[''.join(c for c in x if c not in '0123456789') for x in groupB]\n",
    "            ## return data_back to DataFrame\n",
    "            data_frame[sen_A]=groupA\n",
    "            data_frame[sen_B]=groupB\n",
    "            cleaned_data.append(data_frame)\n",
    "        \n",
    "        \n",
    "        sick_dataset={'train':Dataset.from_pandas(cleaned_data[0]),\n",
    "                      'validation':Dataset.from_pandas(cleaned_data[1]),\n",
    "                      'test':Dataset.from_pandas(cleaned_data[2])}\n",
    "        data_frame=pd.concat(cleaned_data, ignore_index=True)\n",
    "        \n",
    "        return sick_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4fde4a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "process=Preprocess(stopwords_path)\n",
    "formatted_data=process.perform_preprocessing(sick_data,columns_mapping)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "042f6b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data= pd.DataFrame(formatted_data['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6f762528",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4439\n"
     ]
    }
   ],
   "source": [
    "cols = list(columns_mapping.values())\n",
    "cols.pop()\n",
    "sen_list=list(train_data[cols].apply(lambda row: ' '.join(row.values.astype(str)), axis=1))\n",
    "sen_str = ' '.join(map(str, sen_list))\n",
    "print(len(sen_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "177d693f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word tokenization\n",
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "nlp = English()\n",
    "text = \"\"\"When learning data science, you shouldn't get discouraged!\n",
    "Challenges and setbacks aren't failures, they're just part of the journey. You've got this!\"\"\"\n",
    "\n",
    "#  \"nlp\" Object is used to create documents with linguistic annotations.\n",
    "my_doc = nlp(sen_str)\n",
    "\n",
    "# Create list of word tokens\n",
    "token_list = []\n",
    "for token in my_doc:\n",
    "    token_list.append(token.text)\n",
    "# print(token_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1172f105",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import vocab\n",
    "from collections import Counter, OrderedDict\n",
    "counter = Counter(token_list)\n",
    "sorted_by_freq_tuples = sorted(counter.items(), key=lambda x: x[1], reverse=True)\n",
    "ordered_dict = OrderedDict(sorted_by_freq_tuples)\n",
    "v= vocab(ordered_dict)\n",
    "\n",
    "PAD_token = 0   # Used for padding short sentences\n",
    "SOS_token = 1   # Start-of-sentence token\n",
    "EOS_token = 2   # End-of-sentence token\n",
    "special_words={PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"}\n",
    "default_index = -1\n",
    "v.set_default_index(default_index)\n",
    "for key , value in special_words.items():\n",
    "    if value not in v: v.insert_token(value, key)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2b7f19ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(stopwords_path) as fh:\n",
    "            stopwords=list(set(fh.read().split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7a252b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['sentenceA&B']=train_data[cols].apply(lambda row: ' '.join(row.values.astype(str)), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "534639c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:torchtext.vocab:Loading vectors from .vector_cache/wiki.simple.vec.pt\n"
     ]
    }
   ],
   "source": [
    "text_field = Field(\n",
    "#     tokenize='basic_english', \n",
    "    lower=True,\n",
    "    include_lengths=True,\n",
    "    pad_token='PAD',\n",
    "    pad_first='SOS',\n",
    "    stop_words=stopwords,\n",
    ")\n",
    "\n",
    "# label_field = Field(sequential=False, use_vocab=False)\n",
    "preprocessed_text = train_data['sentenceA&B'].apply(lambda x: text_field.preprocess(x))\n",
    "text_field.build_vocab(\n",
    "    preprocessed_text, \n",
    "    vectors='fasttext.simple.300d')\n",
    "\n",
    "# get the vocab instance\n",
    "vocab = text_field.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e50a07ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1962"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_text\n",
    "len(vocab.itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "634009d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def data2tensors(self, data):\n",
    "#         \"\"\"\n",
    "#         Converts raw data sequences into vectorized sequences as tensors\n",
    "#         \"\"\"\n",
    "#         pass\n",
    "#         # TODO implement\n",
    "\n",
    "# def get_data_loader(self, batch_size=8):\n",
    "#         pass\n",
    "#         # TODO implement\n",
    "\n",
    "# def sort_batch(self, batch, targets, lengths):\n",
    "#         \"\"\"\n",
    "#         Sorts the data, lengths and target tensors based on the lengths\n",
    "#         of the sequences from longest to shortest in batch\n",
    "#         \"\"\"\n",
    "#         sents1_lengths, perm_idx = lengths.sort(0, descending=True)\n",
    "#         sequence_tensor = batch[perm_idx]\n",
    "#         target_tensor = targets[perm_idx]\n",
    "#         return sequence_tensor.transpose(0, 1), target_tensor, sents1_lengths\n",
    "\n",
    "# def vectorize_sequence(self, sentence):\n",
    "#         \"\"\"\n",
    "#         Replaces tokens with their indices in vocabulary\n",
    "#         \"\"\"\n",
    "#         pass\n",
    "#         # TODO implement\n",
    "\n",
    "# def pad_sequences(self, vectorized_sents, sents_lengths):\n",
    "#         \"\"\"\n",
    "#         Pads zeros at the end of each sequence in data tensor till max\n",
    "#         length of sequence in that batch\n",
    "#         \"\"\"\n",
    "#         pass\n",
    "#         # TODO implement\n",
    "# def encode(vocab,string):\n",
    "#        return vocab[string]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ea2f9949",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data['sentenceA&B'][0].apply(lambda x: print(x) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f6da72d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_sequence(sentence):\n",
    "        \"\"\"\n",
    "        Replaces tokens with their indices in vocabulary\n",
    "        \"\"\"\n",
    "        splited_sentence=sentence.split()\n",
    "        encodes=[vocab[token] for token in splited_sentence]\n",
    "        return encodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c60c0459",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_A</th>\n",
       "      <th>sentence_B</th>\n",
       "      <th>relatedness_score</th>\n",
       "      <th>sentenceA&amp;B</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>boys playing outdoors smiling nearby</td>\n",
       "      <td>boy playing outdoors smiling</td>\n",
       "      <td>3.6</td>\n",
       "      <td>[51, 4, 111, 133, 1390, 7, 4, 111, 133]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>person black jacket tricks motorbike</td>\n",
       "      <td>skilled person riding bicycle wheel</td>\n",
       "      <td>3.4</td>\n",
       "      <td>[5, 11, 76, 427, 175, 0, 5, 18, 66, 332]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>children backbends gym</td>\n",
       "      <td>girls backbends playing outdoors</td>\n",
       "      <td>3.8</td>\n",
       "      <td>[35, 885, 476, 118, 885, 4, 111]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>player throwing ball</td>\n",
       "      <td>teams competing football match</td>\n",
       "      <td>2.9</td>\n",
       "      <td>[85, 166, 21, 541, 606, 84, 1224]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>children standing wooden hut</td>\n",
       "      <td>children standing wooden hut</td>\n",
       "      <td>4.2</td>\n",
       "      <td>[35, 6, 172, 1071, 35, 6, 172, 1071]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>490</th>\n",
       "      <td>biker riding fence</td>\n",
       "      <td>dancing road</td>\n",
       "      <td>1.2</td>\n",
       "      <td>[317, 18, 199, 25, 68]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491</th>\n",
       "      <td>woman playing electric guitar</td>\n",
       "      <td>kid playing guitar</td>\n",
       "      <td>3.0</td>\n",
       "      <td>[2, 4, 402, 20, 79, 4, 20]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492</th>\n",
       "      <td>animal grazing grass</td>\n",
       "      <td>cop sitting police bike</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[98, 522, 22, 910, 14, 969, 29]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>493</th>\n",
       "      <td>dog snapping droplets water</td>\n",
       "      <td>girl band playing instrument</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[3, 990, 735, 15, 8, 282, 4, 208]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>pushing motocross bike dirt hill</td>\n",
       "      <td>dog swimming tennis ball</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[240, 306, 29, 57, 80, 3, 74, 95, 21]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>495 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               sentence_A  \\\n",
       "0    boys playing outdoors smiling nearby   \n",
       "1    person black jacket tricks motorbike   \n",
       "2                  children backbends gym   \n",
       "3                    player throwing ball   \n",
       "4            children standing wooden hut   \n",
       "..                                    ...   \n",
       "490                    biker riding fence   \n",
       "491         woman playing electric guitar   \n",
       "492                  animal grazing grass   \n",
       "493           dog snapping droplets water   \n",
       "494      pushing motocross bike dirt hill   \n",
       "\n",
       "                              sentence_B  relatedness_score  \\\n",
       "0           boy playing outdoors smiling                3.6   \n",
       "1    skilled person riding bicycle wheel                3.4   \n",
       "2       girls backbends playing outdoors                3.8   \n",
       "3         teams competing football match                2.9   \n",
       "4           children standing wooden hut                4.2   \n",
       "..                                   ...                ...   \n",
       "490                         dancing road                1.2   \n",
       "491                   kid playing guitar                3.0   \n",
       "492              cop sitting police bike                1.0   \n",
       "493         girl band playing instrument                1.0   \n",
       "494             dog swimming tennis ball                1.0   \n",
       "\n",
       "                                  sentenceA&B  \n",
       "0     [51, 4, 111, 133, 1390, 7, 4, 111, 133]  \n",
       "1    [5, 11, 76, 427, 175, 0, 5, 18, 66, 332]  \n",
       "2            [35, 885, 476, 118, 885, 4, 111]  \n",
       "3           [85, 166, 21, 541, 606, 84, 1224]  \n",
       "4        [35, 6, 172, 1071, 35, 6, 172, 1071]  \n",
       "..                                        ...  \n",
       "490                    [317, 18, 199, 25, 68]  \n",
       "491                [2, 4, 402, 20, 79, 4, 20]  \n",
       "492           [98, 522, 22, 910, 14, 969, 29]  \n",
       "493         [3, 990, 735, 15, 8, 282, 4, 208]  \n",
       "494     [240, 306, 29, 57, 80, 3, 74, 95, 21]  \n",
       "\n",
       "[495 rows x 4 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = list(columns_mapping.values())\n",
    "cols.pop()\n",
    "train_data= pd.DataFrame(formatted_data['train'])\n",
    "val_data=pd.DataFrame(formatted_data['validation'])\n",
    "test_data=pd.DataFrame(formatted_data['test'])\n",
    "train_data['sentenceA&B']=train_data[cols].apply(lambda row: ' '.join(row.values.astype(str)), axis=1)\n",
    "val_data['sentenceA&B']=val_data[cols].apply(lambda row: ' '.join(row.values.astype(str)), axis=1)\n",
    "test_data['sentenceA&B']=test_data[cols].apply(lambda row: ' '.join(row.values.astype(str)), axis=1)\n",
    "\n",
    "codes=[]\n",
    "for sentence in val_data['sentenceA&B']:\n",
    "    encodes= vectorize_sequence(sentence)\n",
    "    codes.append(encodes)\n",
    "val_data['sentenceA&B'] = codes\n",
    "val_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "90fe4221",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data2tensors(data):\n",
    "        \"\"\"\n",
    "        Converts raw data sequences into vectorized sequences as tensors\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        \n",
    "        data['sent1_tensor']=data['sent1_tensor'].apply(lambda lis : torch.as_tensor(lis))\n",
    "        data['sent2_tensor']=data['sent2_tensor'].apply(lambda lis : torch.as_tensor(lis))\n",
    "           \n",
    "        \n",
    "        \n",
    "        return data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "334dd9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame(data2tensors(formatted_data)['train'])[\"sentenceA&B\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0e617eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders = {x:DataLoader(formatted_data[x],32, shuffle=True, num_workers=4) for x in ['train','validation','test']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3637178",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8eb20f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import DataLoader\n",
    "\n",
    "# train_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)\n",
    "# test_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)\n",
    "\n",
    "# train_features, train_labels = next(iter(dataloaders['train']))\n",
    "# from torch.utils import data\n",
    "# train_tensor = data.TensorDataset(train_data, train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "74915ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils import data\n",
    "# train_tensor = data.TensorDataset(train_data, train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5702913d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloaders = {'train': torch.utils.data.DataLoader(STSDataset(sts_train_df, batch_size=64)),\n",
    "#                'val': torch.utils.data.DataLoader(STSDataset(sts_dev_df, batch_size=64))}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b415d13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_target = torch.tensor(train['Target'].values.astype(np.float32))\n",
    "# train = torch.tensor(train.drop('Target', axis = 1).values.astype(np.float32)) \n",
    "# train_tensor = data_utils.TensorDataset(train, train_target) \n",
    "# train_loader = data_utils.DataLoader(dataset = train_tensor, batch_size = batch_size, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0612f9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_df= pd.DataFrame(formatted_data['train'])\n",
    "val_data_df=pd.DataFrame(formatted_data['validation'])\n",
    "test_data_df=pd.DataFrame(formatted_data['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4a697f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_df['sent1_tensor']=train_data_df['sentence_A'].apply(lambda sen: vectorize_sequence(sen))\n",
    "train_data_df['sent2_tensor']=train_data_df['sentence_B'].apply(lambda sen: vectorize_sequence(sen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8f33b74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data_df = torch.tensor(train_data_df['sentence_A'].values.astype(np.float32))\n",
    "train_data_df=data2tensors(train_data_df)\n",
    "train_data_df['sents1_length_tensor']=train_data_df['sent1_tensor'].apply(lambda tensor : torch.tensor(len(tensor)))\n",
    "train_data_df['sents2_length_tensor']=train_data_df['sent2_tensor'].apply(lambda tensor : torch.tensor(len(tensor)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da71456",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f851e252",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pad_sequences\n",
    "# from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "# padded=pad_sequences(train_data_df['sent1_tensor'],padding=\"post\",truncating=”post”,maxlen=8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7d5b046f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequences(sequences):\n",
    "        \"\"\"\n",
    "        :param sequences: list of tensors\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        num = len(sequences)\n",
    "        max_len = max([s.shape[0] for s in sequences])\n",
    "        out_dims = (num, max_len, *sequences[0].shape[1:])\n",
    "        out_tensor = sequences[0].data.new(*out_dims).fill_(0)\n",
    "        mask = sequences[0].data.new(*out_dims).fill_(0)\n",
    "        for i, tensor in enumerate(sequences):\n",
    "            length = tensor.size(0)\n",
    "            out_tensor[i, :length] = tensor\n",
    "            mask[i, :length] = 1\n",
    "        return list(out_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3c09b71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_df['sent1_tensor']=pad_sequences(train_data_df['sent1_tensor'])\n",
    "train_data_df['sent2_tensor']=pad_sequences(train_data_df['sent2_tensor'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ff76a379",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_df['relatedness_score']=train_data_df['relatedness_score'].apply(lambda score : torch.tensor(score/sum(train_data_df['relatedness_score'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01dd9d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "763635b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Standard Pytorch Dataset class for loading datasets.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class STSDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        sent1_tensor,\n",
    "        sent2_tensor,\n",
    "        target_tensor,\n",
    "        sents1_length_tensor,\n",
    "        sents2_length_tensor,\n",
    "        raw_sents_1,\n",
    "        raw_sents_2,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        initializes  and populates the the length, data and target tensors, and raw texts list\n",
    "        \"\"\"\n",
    "        \n",
    "        assert (\n",
    "            \n",
    "            \n",
    "            len(sent1_tensor)\n",
    "            == torch.tensor(list(target_tensor)).size(0)\n",
    "            == len(sent2_tensor)\n",
    "            == torch.tensor(list(sents1_length_tensor)).size(0)\n",
    "            == torch.tensor(list(sents2_length_tensor)).size(0)\n",
    "        )\n",
    "        self.sent1_tensor = sent1_tensor\n",
    "        self.sent2_tensor = sent2_tensor\n",
    "        self.target_tensor = target_tensor\n",
    "        self.sents1_length_tensor = sents1_length_tensor\n",
    "        self.sents2_length_tensor = sents2_length_tensor\n",
    "        self.raw_sents_1 = raw_sents_1\n",
    "        self.raw_sents_2 = raw_sents_2\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        returns the tuple of data tensor, targets, lengths of sequences tensor and raw texts list\n",
    "        \"\"\"\n",
    "        return (\n",
    "            self.sent1_tensor[index],\n",
    "            self.sent2_tensor[index],\n",
    "            self.sents1_length_tensor[index],\n",
    "            self.sents2_length_tensor[index],\n",
    "            self.target_tensor[index],\n",
    "            self.raw_sents_1[index],\n",
    "            self.raw_sents_2[index],\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        returns the length of the data tensor.\n",
    "        \"\"\"\n",
    "        return torch.tensor(list(self.target_tensor)).size(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ddd72037",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_A</th>\n",
       "      <th>sentence_B</th>\n",
       "      <th>relatedness_score</th>\n",
       "      <th>sent1_tensor</th>\n",
       "      <th>sent2_tensor</th>\n",
       "      <th>sents1_length_tensor</th>\n",
       "      <th>sents2_length_tensor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kids playing yard standing background</td>\n",
       "      <td>boys yard playing standing background</td>\n",
       "      <td>tensor(0.0003)</td>\n",
       "      <td>[tensor(65), tensor(4), tensor(144), tensor(6)...</td>\n",
       "      <td>[tensor(51), tensor(144), tensor(4), tensor(6)...</td>\n",
       "      <td>tensor(5)</td>\n",
       "      <td>tensor(5)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>children playing house standing background</td>\n",
       "      <td>kids playing yard standing background</td>\n",
       "      <td>tensor(0.0002)</td>\n",
       "      <td>[tensor(35), tensor(4), tensor(403), tensor(6)...</td>\n",
       "      <td>[tensor(65), tensor(4), tensor(144), tensor(6)...</td>\n",
       "      <td>tensor(5)</td>\n",
       "      <td>tensor(5)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>boys playing outdoors smiling nearby</td>\n",
       "      <td>kids playing outdoors smile</td>\n",
       "      <td>tensor(0.0003)</td>\n",
       "      <td>[tensor(51), tensor(4), tensor(111), tensor(13...</td>\n",
       "      <td>[tensor(65), tensor(4), tensor(111), tensor(86...</td>\n",
       "      <td>tensor(5)</td>\n",
       "      <td>tensor(4)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>kids playing outdoors smile</td>\n",
       "      <td>kids playing yard standing background</td>\n",
       "      <td>tensor(0.0002)</td>\n",
       "      <td>[tensor(65), tensor(4), tensor(111), tensor(86...</td>\n",
       "      <td>[tensor(65), tensor(4), tensor(144), tensor(6)...</td>\n",
       "      <td>tensor(4)</td>\n",
       "      <td>tensor(5)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>boys playing outdoors smiling nearby</td>\n",
       "      <td>kids playing yard standing background</td>\n",
       "      <td>tensor(0.0002)</td>\n",
       "      <td>[tensor(51), tensor(4), tensor(111), tensor(13...</td>\n",
       "      <td>[tensor(65), tensor(4), tensor(144), tensor(6)...</td>\n",
       "      <td>tensor(5)</td>\n",
       "      <td>tensor(5)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4434</th>\n",
       "      <td>door man</td>\n",
       "      <td>bald band playing guitar spotlight</td>\n",
       "      <td>tensor(7.0483e-05)</td>\n",
       "      <td>[tensor(349), tensor(0), tensor(0), tensor(0),...</td>\n",
       "      <td>[tensor(1480), tensor(282), tensor(4), tensor(...</td>\n",
       "      <td>tensor(2)</td>\n",
       "      <td>tensor(5)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4435</th>\n",
       "      <td>boiling okra pot</td>\n",
       "      <td>playing drums</td>\n",
       "      <td>tensor(6.4075e-05)</td>\n",
       "      <td>[tensor(187), tensor(359), tensor(86), tensor(...</td>\n",
       "      <td>[tensor(4), tensor(339), tensor(0), tensor(0),...</td>\n",
       "      <td>tensor(3)</td>\n",
       "      <td>tensor(2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4436</th>\n",
       "      <td>singing heartily playing guitar</td>\n",
       "      <td>bicyclist holding bike head people</td>\n",
       "      <td>tensor(6.4075e-05)</td>\n",
       "      <td>[tensor(47), tensor(1555), tensor(4), tensor(2...</td>\n",
       "      <td>[tensor(273), tensor(27), tensor(29), tensor(2...</td>\n",
       "      <td>tensor(4)</td>\n",
       "      <td>tensor(5)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4437</th>\n",
       "      <td>blue yellow ball mitt</td>\n",
       "      <td>jumping rope outside</td>\n",
       "      <td>tensor(7.6890e-05)</td>\n",
       "      <td>[tensor(24), tensor(39), tensor(21), tensor(13...</td>\n",
       "      <td>[tensor(16), tensor(60), tensor(0), tensor(0),...</td>\n",
       "      <td>tensor(4)</td>\n",
       "      <td>tensor(3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4438</th>\n",
       "      <td>dogs resting sidewalk</td>\n",
       "      <td>woman knife slicing pepper</td>\n",
       "      <td>tensor(6.4075e-05)</td>\n",
       "      <td>[tensor(33), tensor(244), tensor(129), tensor(...</td>\n",
       "      <td>[tensor(2), tensor(157), tensor(28), tensor(45...</td>\n",
       "      <td>tensor(3)</td>\n",
       "      <td>tensor(4)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4439 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      sentence_A  \\\n",
       "0          kids playing yard standing background   \n",
       "1     children playing house standing background   \n",
       "2           boys playing outdoors smiling nearby   \n",
       "3                    kids playing outdoors smile   \n",
       "4           boys playing outdoors smiling nearby   \n",
       "...                                          ...   \n",
       "4434                                    door man   \n",
       "4435                            boiling okra pot   \n",
       "4436             singing heartily playing guitar   \n",
       "4437                       blue yellow ball mitt   \n",
       "4438                       dogs resting sidewalk   \n",
       "\n",
       "                                 sentence_B   relatedness_score  \\\n",
       "0     boys yard playing standing background      tensor(0.0003)   \n",
       "1     kids playing yard standing background      tensor(0.0002)   \n",
       "2               kids playing outdoors smile      tensor(0.0003)   \n",
       "3     kids playing yard standing background      tensor(0.0002)   \n",
       "4     kids playing yard standing background      tensor(0.0002)   \n",
       "...                                     ...                 ...   \n",
       "4434     bald band playing guitar spotlight  tensor(7.0483e-05)   \n",
       "4435                          playing drums  tensor(6.4075e-05)   \n",
       "4436     bicyclist holding bike head people  tensor(6.4075e-05)   \n",
       "4437                   jumping rope outside  tensor(7.6890e-05)   \n",
       "4438             woman knife slicing pepper  tensor(6.4075e-05)   \n",
       "\n",
       "                                           sent1_tensor  \\\n",
       "0     [tensor(65), tensor(4), tensor(144), tensor(6)...   \n",
       "1     [tensor(35), tensor(4), tensor(403), tensor(6)...   \n",
       "2     [tensor(51), tensor(4), tensor(111), tensor(13...   \n",
       "3     [tensor(65), tensor(4), tensor(111), tensor(86...   \n",
       "4     [tensor(51), tensor(4), tensor(111), tensor(13...   \n",
       "...                                                 ...   \n",
       "4434  [tensor(349), tensor(0), tensor(0), tensor(0),...   \n",
       "4435  [tensor(187), tensor(359), tensor(86), tensor(...   \n",
       "4436  [tensor(47), tensor(1555), tensor(4), tensor(2...   \n",
       "4437  [tensor(24), tensor(39), tensor(21), tensor(13...   \n",
       "4438  [tensor(33), tensor(244), tensor(129), tensor(...   \n",
       "\n",
       "                                           sent2_tensor sents1_length_tensor  \\\n",
       "0     [tensor(51), tensor(144), tensor(4), tensor(6)...            tensor(5)   \n",
       "1     [tensor(65), tensor(4), tensor(144), tensor(6)...            tensor(5)   \n",
       "2     [tensor(65), tensor(4), tensor(111), tensor(86...            tensor(5)   \n",
       "3     [tensor(65), tensor(4), tensor(144), tensor(6)...            tensor(4)   \n",
       "4     [tensor(65), tensor(4), tensor(144), tensor(6)...            tensor(5)   \n",
       "...                                                 ...                  ...   \n",
       "4434  [tensor(1480), tensor(282), tensor(4), tensor(...            tensor(2)   \n",
       "4435  [tensor(4), tensor(339), tensor(0), tensor(0),...            tensor(3)   \n",
       "4436  [tensor(273), tensor(27), tensor(29), tensor(2...            tensor(4)   \n",
       "4437  [tensor(16), tensor(60), tensor(0), tensor(0),...            tensor(4)   \n",
       "4438  [tensor(2), tensor(157), tensor(28), tensor(45...            tensor(3)   \n",
       "\n",
       "     sents2_length_tensor  \n",
       "0               tensor(5)  \n",
       "1               tensor(5)  \n",
       "2               tensor(4)  \n",
       "3               tensor(5)  \n",
       "4               tensor(5)  \n",
       "...                   ...  \n",
       "4434            tensor(5)  \n",
       "4435            tensor(2)  \n",
       "4436            tensor(5)  \n",
       "4437            tensor(3)  \n",
       "4438            tensor(4)  \n",
       "\n",
       "[4439 rows x 7 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_df                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "243b82ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset=  STSDataset(train_data_df['sent1_tensor'],\n",
    "                         train_data_df['sent2_tensor'],\n",
    "                         train_data_df['relatedness_score'],\n",
    "                         train_data_df['sents1_length_tensor'],\n",
    "                         train_data_df['sents2_length_tensor'],\n",
    "                         train_data_df['sentence_A'],\n",
    "                         train_data_df['sentence_B']\n",
    "                         )\n",
    "\n",
    "train_loader = DataLoader(dataset = train_dataset, batch_size = 64, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2858ea5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(train_data_df['sent1_tensor'])\n",
    "# train_data_df['sents1_length_tensor'].size()\n",
    "# torch.tensor(list(target_tensor)).size(0)\n",
    "# train_data_df['sent1_tensor'][0]\n",
    "# train_data_df['relatedness_score'][0]\n",
    "# train_data_df['sents1_length_tensor'][0]\n",
    "# train_data_df['sentence_A'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c023118a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e699fbb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_df['relatedness_score'][0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "94cb6e85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x7f4611cddf90>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "448f4add",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataiter = iter(train_loader)\n",
    "len(dataiter.next())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "01def1b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[   2,  187,  254,   15,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [  53,  163,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [  33,   12,  510,   55,   21,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [   9,  145,  290,  461,   15,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [   2,   28,  799,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [   8,   74, 1935,   16,   38,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [   2,  371,   63,   49,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [  85,   55,  983,  221,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [  47,    4,   20,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [  66,  159,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [ 219,   78,   41,  410,   45,  647,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [  79,   10,   41,   19,  331,  100,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [  17,    9,   98,   16,  129,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [1243,  879,  605,  421,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [   5,  507,   36,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [  51, 1251, 1675,  128,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [ 580,  767,  262,  293,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [  10,  880,   18,  280,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [ 410, 1119,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [   3,    4,   89,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [   2,  858,   14,  370,  827,  343,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [  48,  329,  192,   40,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [ 132,  239,  627,  592,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [   3,   12,  784,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [  61,   25,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [   4,   20,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [  35,   12,   80,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [1415, 1722,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [   5,  165,  271,   86,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [   2,  432,   63,   49,  501,  545,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [  47,  635,    4,   20,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [ 314,   45,   10,  222,  236,  354,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [ 239,   20,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [   2,  432,   63,   49,  501,  545,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [   2,  187,   63,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [   6,  139,  595,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [  58,  688,    4,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [ 664,  166,  624,   56,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [  78,  107,  236,    6,   78,    4,   20,    0,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [  61,    4,   21,   38,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [ 830, 1921,    2,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [  28,  155,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [   2,  127,   62,  252,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [ 405,  123,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [ 278,  927,  380,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [   2,  362,  120,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [  29,  140,   11,   23,  108,    6,   57,   29,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [ 542,  160,    2,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [   2,   12,   27,  120,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [1024,  276, 1118,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [   2,   25, 1700,  318,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [   2,   42,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [   9,    3,   12,  130,  330,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [  64,  696,   18,   23,   11,   29,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [   5,  776,  170,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [  24, 1218, 1517,   81,  333,   64,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [  35,    4,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [   4, 1004,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [ 172,  999, 1503, 1071,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [ 294,  117,  173,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [   2,  328,  116,   54,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [ 263,   11,    3,  136,   17,    3,    4,  144,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [   8,  161,  838,   50,  112,  303,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [ 136,   17,    3,   12,   15,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0]]), tensor([[   5,  187,  254,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [  53,  163,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [ 319,  190,    4,  243,   89,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [   9,  804,  189,   15,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [   2,   28,  799,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [   8,  274,    4, 1531,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [ 251, 1083,   49,    5,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [  55,  211,  164,   55,   21,  221,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [   7,    4,   67,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [  18,   66,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [  45,  493,  641,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [   9,   41,  503,    7,   12,   38,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [  17,    9,    3,   12,  129,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [1243, 1478,  605,  421,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [ 507,   36,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [  17,  241,   19,  137,  128,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [  48,  310,  293,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [   5,   10,  880,   18,  280,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [ 410, 1119,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [  79,    4,   89,    3,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [  14,   78,  343,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [ 483,  329,  192,   40,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [ 132,  164,    5,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [   3,   12,   21,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [ 250, 1858,   47, 1811,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [  47,    4,   20,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [  35,   12,  232,  822,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [  46,   60,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [ 210,  271,   86,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [   2,  432,   63,   49,  501,  545,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [ 837,  821,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [ 236,  314,  692,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [   5,  239,   20,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [  63, 1680,   49,    5,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [  63, 1030,    2,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [  14,  595,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [ 688,   73,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [ 625,   92,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [   4,   20,   20, 1528,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [   4,   55,   38,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [   2, 1438,  830,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [  28,  155,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [   2,  127,   68, 1588,  499,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [1082,   81,  123,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [ 442,  380,  278,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [ 454,  362,  120,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [  11,   29,  140,   23,   29,  108,    6,   57,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [ 542,  160,    2,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [ 250,   12,  101,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [1024,    5,  276, 1118,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [   2,   25,  318,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [   2, 1515,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [   3,   12,  130,  191,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [ 209,  114,   64,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [  26,  170,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [ 333,   64,   99,  651,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [  79,  225,  101,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [   4, 1004,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [  35,    6,  172, 1071,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [  48,  117,  173,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [   2,  116,   54,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [ 263,   11,    3,  136,   17,    3,    4,  886,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [   8,  161,  838,   50,  112,  303,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [ 186,    3,  225,   15,  547,  128,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0]]), tensor([4, 3, 5, 5, 3, 5, 4, 4, 3, 3, 6, 6, 5, 4, 3, 4, 4, 4, 2, 3, 6, 4, 4, 3,\n",
      "        2, 2, 3, 2, 4, 6, 4, 6, 2, 6, 3, 3, 3, 4, 7, 4, 3, 2, 4, 2, 4, 3, 8, 3,\n",
      "        4, 3, 4, 2, 5, 6, 3, 6, 2, 2, 4, 3, 4, 9, 7, 5]), tensor([3, 3, 5, 4, 3, 4, 4, 6, 3, 2, 3, 6, 5, 4, 2, 5, 3, 5, 2, 4, 3, 4, 3, 3,\n",
      "        4, 3, 4, 2, 3, 6, 2, 3, 3, 4, 3, 2, 3, 2, 4, 3, 3, 2, 5, 3, 3, 3, 8, 3,\n",
      "        3, 4, 3, 2, 4, 3, 2, 4, 3, 2, 4, 3, 3, 8, 7, 6]), tensor([2.7552e-04, 1.9863e-04, 1.4097e-04, 2.2426e-04, 2.8193e-04, 2.0504e-04,\n",
      "        2.1145e-04, 2.7552e-04, 1.2078e-04, 3.0756e-04, 2.1786e-04, 2.1786e-04,\n",
      "        2.2426e-04, 3.0756e-04, 2.3067e-04, 1.8582e-04, 2.3067e-04, 2.8834e-04,\n",
      "        2.1145e-04, 1.4737e-04, 1.1534e-04, 2.5630e-04, 1.8582e-04, 2.8834e-04,\n",
      "        1.2174e-04, 2.6271e-04, 2.4349e-04, 1.9863e-04, 2.0504e-04, 2.5630e-04,\n",
      "        7.6890e-05, 2.5630e-04, 2.9475e-04, 2.7552e-04, 3.1397e-04, 2.0504e-04,\n",
      "        2.0504e-04, 1.6019e-04, 2.3708e-04, 1.9863e-04, 3.1397e-04, 2.1145e-04,\n",
      "        2.8193e-04, 1.9863e-04, 3.0115e-04, 3.0115e-04, 2.3708e-04, 3.1397e-04,\n",
      "        2.3067e-04, 3.0115e-04, 3.0756e-04, 1.7941e-04, 2.7552e-04, 1.7300e-04,\n",
      "        2.4989e-04, 2.6271e-04, 1.2815e-04, 2.4989e-04, 2.0504e-04, 2.1786e-04,\n",
      "        3.0115e-04, 3.1397e-04, 2.4989e-04, 2.4989e-04]), ('woman boiling noodles water', 'onion cut man', 'dogs running catch soccer ball', 'white bird landing swiftly water', 'woman slicing carrots', 'girl swimming suite jumping beach', 'woman breaking eggs bowl', 'player soccer scoring goal', 'singing playing guitar', 'bicycle ridden man', 'dyed purple hat operating camera videos', 'kid wearing hat walking wet sand', 'brown white animal jumping sidewalk', 'presentation watched classroom students', 'person brushing cat', 'boys rocking walk river', 'sprinkling shredded cheese pizza', 'wearing wetsuit riding surfboard', 'operating stenograph', 'dog playing toy', 'woman shoeless sitting blanket lavender umbrella', 'putting suitcases trunk car', 'monkey practicing martial arts', 'dog running volleyball', 'women dancing', 'playing guitar', 'children running hill', 'roped climb', 'person stirring vegetables pot', 'woman beating eggs bowl whisk wire', 'singing song playing guitar', 'surrounded camera wearing gray suit glasses', 'practicing guitar', 'woman beating eggs bowl whisk wire', 'woman boiling eggs', 'standing colorful birdcage', 'baby pandas playing', 'dangerously throwing knives tree', 'purple colored suit standing purple playing guitar', 'women playing ball beach', 'lemon squeezed woman', 'slicing tomato', 'woman driving street jeep', 'leaving stage', 'snake fed mouse man', 'woman shaking hands', 'bike rider black red uniform standing dirt bike', 'tofu sliced woman', 'woman running holding hands', 'athletes crowded stadium', 'woman dancing beautifully cage', 'woman eating', 'white dog running snowy trail', 'motorcycle racer riding red black bike', 'person tearing paper', 'blue jumpsuit courageously performing wheelie motorcycle', 'children playing', 'playing toad', 'wooden stands childs hut', 'removing food box', 'woman slowly peeling potato', 'dark black dog light brown dog playing yard back', 'girl wood panel wall posing funny faces', 'light brown dog running water'), ('person boiling noodles', 'onion cut man', 'cats lawn playing plastic toy', 'white crane flying water', 'woman slicing carrots', 'girl bikini playing dunes', 'ingredients mixed bowl person', 'soccer players kicking soccer ball goal', 'boy playing piano', 'riding bicycle', 'camera studying subject', 'white hat worn boy running beach', 'brown white dog running sidewalk', 'presentation attended classroom students', 'brushing cat', 'brown shorts walking rocks river', 'putting seasoning pizza', 'person wearing wetsuit riding surfboard', 'operating stenograph', 'kid playing toy dog', 'sitting purple umbrella', 'packing suitcases trunk car', 'monkey kicking person', 'dog running ball', 'couple policewomen singing karaoke', 'singing playing guitar', 'children running grassy hillside', 'climbing rope', 'mixing vegetables pot', 'woman beating eggs bowl whisk wire', 'package headphones', 'suit surrounded photographers', 'person practicing guitar', 'eggs whisked bowl person', 'eggs boiled woman', 'sitting birdcage', 'pandas lying together', 'magic trick', 'playing guitar guitar donations', 'playing soccer beach', 'woman squeezing lemon', 'slicing tomato', 'woman driving road openair vehicle', 'mimes performing stage', 'feeding mouse snake', 'persons shaking hands', 'black bike rider red bike uniform standing dirt', 'tofu sliced woman', 'couple running ocean', 'athletes person crowded stadium', 'woman dancing cage', 'woman conversation', 'dog running snowy path', 'leaning racing motorcycle', 'cutting paper', 'wheelie motorcycle ground barren', 'kid splashing ocean', 'playing toad', 'children standing wooden hut', 'putting food box', 'woman peeling potato', 'dark black dog light brown dog playing backyard', 'girl wood panel wall posing funny faces', 'tan dog splashing water bank river')]\n"
     ]
    }
   ],
   "source": [
    "for batch in train_loader:\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ce6d99c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   2,  187,  254,   15,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0],\n",
       "        [  53,  163,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0],\n",
       "        [  33,   12,  510,   55,   21,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0],\n",
       "        [   9,  145,  290,  461,   15,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0],\n",
       "        [   2,   28,  799,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0],\n",
       "        [   8,   74, 1935,   16,   38,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0],\n",
       "        [   2,  371,   63,   49,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0],\n",
       "        [  85,   55,  983,  221,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0],\n",
       "        [  47,    4,   20,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0],\n",
       "        [  66,  159,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0],\n",
       "        [ 219,   78,   41,  410,   45,  647,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0],\n",
       "        [  79,   10,   41,   19,  331,  100,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0],\n",
       "        [  17,    9,   98,   16,  129,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0],\n",
       "        [1243,  879,  605,  421,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0],\n",
       "        [   5,  507,   36,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0],\n",
       "        [  51, 1251, 1675,  128,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0],\n",
       "        [ 580,  767,  262,  293,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0],\n",
       "        [  10,  880,   18,  280,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0],\n",
       "        [ 410, 1119,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0],\n",
       "        [   3,    4,   89,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0],\n",
       "        [   2,  858,   14,  370,  827,  343,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0],\n",
       "        [  48,  329,  192,   40,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0],\n",
       "        [ 132,  239,  627,  592,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0],\n",
       "        [   3,   12,  784,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0],\n",
       "        [  61,   25,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0],\n",
       "        [   4,   20,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0],\n",
       "        [  35,   12,   80,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0],\n",
       "        [1415, 1722,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0],\n",
       "        [   5,  165,  271,   86,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0],\n",
       "        [   2,  432,   63,   49,  501,  545,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0],\n",
       "        [  47,  635,    4,   20,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0],\n",
       "        [ 314,   45,   10,  222,  236,  354,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0],\n",
       "        [ 239,   20,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0],\n",
       "        [   2,  432,   63,   49,  501,  545,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0],\n",
       "        [   2,  187,   63,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0],\n",
       "        [   6,  139,  595,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0],\n",
       "        [  58,  688,    4,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0],\n",
       "        [ 664,  166,  624,   56,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0],\n",
       "        [  78,  107,  236,    6,   78,    4,   20,    0,    0,    0,    0,    0,\n",
       "            0,    0],\n",
       "        [  61,    4,   21,   38,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0],\n",
       "        [ 830, 1921,    2,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0],\n",
       "        [  28,  155,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0],\n",
       "        [   2,  127,   62,  252,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0],\n",
       "        [ 405,  123,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0],\n",
       "        [ 278,  927,  380,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0],\n",
       "        [   2,  362,  120,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0],\n",
       "        [  29,  140,   11,   23,  108,    6,   57,   29,    0,    0,    0,    0,\n",
       "            0,    0],\n",
       "        [ 542,  160,    2,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0],\n",
       "        [   2,   12,   27,  120,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0],\n",
       "        [1024,  276, 1118,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0],\n",
       "        [   2,   25, 1700,  318,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0],\n",
       "        [   2,   42,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0],\n",
       "        [   9,    3,   12,  130,  330,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0],\n",
       "        [  64,  696,   18,   23,   11,   29,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0],\n",
       "        [   5,  776,  170,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0],\n",
       "        [  24, 1218, 1517,   81,  333,   64,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0],\n",
       "        [  35,    4,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0],\n",
       "        [   4, 1004,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0],\n",
       "        [ 172,  999, 1503, 1071,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0],\n",
       "        [ 294,  117,  173,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0],\n",
       "        [   2,  328,  116,   54,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0],\n",
       "        [ 263,   11,    3,  136,   17,    3,    4,  144,    0,    0,    0,    0,\n",
       "            0,    0],\n",
       "        [   8,  161,  838,   50,  112,  303,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0],\n",
       "        [ 136,   17,    3,   12,   15,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "642c240c",
   "metadata": {},
   "outputs": [],
   "source": [
    "n, d, m = 3, 5, 7\n",
    "embedding = torch.nn.Embedding(n, d, max_norm=True)\n",
    "W = torch.randn((m, d), requires_grad=True)\n",
    "idx = torch.tensor([1, 2])\n",
    "# a = embedding.weight.clone() @ W.t()  # weight must be cloned for this to be differentiable\n",
    "# b = embedding(idx) @ W.t()  # modifies weight in-place\n",
    "# out = (a.unsqueeze(0) + b.unsqueeze(1))\n",
    "# loss = out.sigmoid().prod()\n",
    "# loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "47872b9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3916, -0.5480,  0.4557,  0.3674, -0.4514],\n",
       "        [-0.4319, -0.0970,  0.3629,  0.6893,  0.4441]],\n",
       "       grad_fn=<EmbeddingBackward>)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1b642cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size= len(vocab)\n",
    "embedding_size= 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "87b5e9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = torch.nn.Embedding(vocab_size, embedding_size, max_norm=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7caadc40",
   "metadata": {},
   "outputs": [],
   "source": [
    "sen=torch.tensor([[121,  82,   5,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
    "        [ 36,   4, 404,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "74d74e36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 14, 300])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding(sen).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7fb1a9ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.2267,  0.0823, -0.0737,  ...,  0.2816,  0.0708, -0.1727],\n",
       "         [ 0.2845, -0.0039,  0.3620,  ..., -0.2849,  0.0120, -0.0268],\n",
       "         [ 0.2008, -0.0394,  0.3176,  ...,  0.0508, -0.2701, -0.2984],\n",
       "         ...,\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        [[ 0.1809, -0.3359, -0.2671,  ..., -0.1251,  0.1333, -0.3796],\n",
       "         [ 0.3038,  0.4029,  0.6689,  ..., -0.2629, -0.1546, -0.4536],\n",
       "         [ 0.0527, -0.0633, -0.4662,  ...,  0.3418, -0.0439, -0.1786],\n",
       "         ...,\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.vectors[sen]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "181e478d",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_weights= vocab.vectors\n",
    "net = torch.nn.LSTM(10, 20, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9ef22ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_score(input1, input2):\n",
    "    # Get similarity predictions:\n",
    "    dif = input1.squeeze() - input2.squeeze()\n",
    "\n",
    "    norm = torch.norm(dif, p=1, dim=dif.dim() - 1)\n",
    "    y_hat = torch.exp(-norm)\n",
    "    y_hat = torch.clamp(y_hat, min=1e-7, max=1.0 - 1e-7)\n",
    "    return y_hat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "35b65dcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:loading and preprocessing data...\n",
      "WARNING:datasets.builder:Using custom data configuration default\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset sick/default (download: 212.48 KiB, generated: 2.50 MiB, post-processed: Unknown size, total: 2.71 MiB) to /home/ibrahimssd/.cache/huggingface/datasets/sick/default/0.0.0/c6b3b0b44eb84b134851396d6d464e5cb8f026960519d640e087fe33472626db...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset sick downloaded and prepared to /home/ibrahimssd/.cache/huggingface/datasets/sick/default/0.0.0/c6b3b0b44eb84b134851396d6d464e5cb8f026960519d640e087fe33472626db. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:reading and preprocessing data completed...\n",
      "INFO:root:creating vocabulary...\n",
      "INFO:torchtext.vocab:Loading vectors from .vector_cache/wiki.simple.vec.pt\n",
      "INFO:root:creating vocabulary completed...\n"
     ]
    }
   ],
   "source": [
    "from sts_data import STSData\n",
    "\n",
    "columns_mapping = {\n",
    "        \"sent1\": \"sentence_A\",\n",
    "        \"sent2\": \"sentence_B\",\n",
    "        \"label\": \"relatedness_score\",\n",
    "    }\n",
    "dataset_name = \"sick\"\n",
    "sick_data = STSData(\n",
    "    dataset_name=dataset_name,\n",
    "    columns_mapping=columns_mapping,\n",
    "    normalize_labels=True,\n",
    "    normalization_const=5.0,\n",
    ")\n",
    "batch_size = 64\n",
    "sick_dataloaders = sick_data.get_data_loader(batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5d2448c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from utils import similarity_score\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Wrapper class using Pytorch nn.Module to create the architecture for our model\n",
    "Architecture is based on the paper: \n",
    "A STRUCTURED SELF-ATTENTIVE SENTENCE EMBEDDING\n",
    "https://arxiv.org/pdf/1703.03130.pdf\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class SiameseBiLSTMAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        batch_size,\n",
    "        output_size,\n",
    "        hidden_size,\n",
    "        vocab_size,\n",
    "        embedding_size,\n",
    "        embedding_weights,\n",
    "        lstm_layers,\n",
    "        device,\n",
    "        bidirectional,\n",
    "        self_attention_config,\n",
    "        fc_hidden_size,\n",
    "    ):\n",
    "        super(SiameseBiLSTMAttention, self).__init__()\n",
    "        \"\"\"\n",
    "        Initializes model layers and loads pre-trained embeddings from task 1\n",
    "        \"\"\"\n",
    "        ## model hyper parameters\n",
    "        self.batch_size = batch_size\n",
    "        self.output_size = output_size\n",
    "        self.lstm_hidden_size = hidden_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.lstm_layers = lstm_layers\n",
    "        self.device = device\n",
    "        self.bidirectional = bidirectional\n",
    "        self.fc_hidden_size = fc_hidden_size\n",
    "        self.lstm_directions = (\n",
    "            2 if self.bidirectional else 1\n",
    "        )  ## decide directions based on input flag\n",
    "        \n",
    "        \n",
    "        \n",
    "        ## model layers\n",
    "        # TODO initialize the look-up table.\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_size)\n",
    "        # TODO assign the look-up table to the pre-trained fasttext word embeddings.\n",
    "        \n",
    "        self.lookup= embedding_weights   #self.embeddings\n",
    "        \n",
    "        ## TODO initialize lstm layer\n",
    "        self.bi_lstm = torch.nn.LSTM(self.embedding_size, self.lstm_hidden_size, \n",
    "                            lstm_layers, batch_first=True , bias= True,bidirectional=True)\n",
    "        \n",
    "        ## TODO initialize self attention layers\n",
    "        self.SelfAtt= SelfAttention(2*self.lstm_hidden_size, self_attention_config['hidden_size'],\n",
    "                                    self_attention_config['output_size'])\n",
    "        \n",
    "        #Initialize fully connected layer\n",
    "        self.fc = nn.Linear(2*self.lstm_hidden_size * self_attention_config['output_size'], self.fc_hidden_size )\n",
    "        self.tanh = nn.Tanh()\n",
    "        ## incase we are using bi-directional lstm we'd have to take care of bi-directional outputs in\n",
    "        ## subsequent layers\n",
    "        \n",
    "        \n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        \"\"\"\n",
    "        Initializes hidden and context weight matrix before each\n",
    "                forward pass through LSTM\n",
    "        \"\"\"\n",
    "        h0 = torch.randn(self.lstm_directions*self.lstm_layers, batch_size, self.lstm_hidden_size)\n",
    "        c0 = torch.randn(self.lstm_directions*self.lstm_layers, batch_size, self.lstm_hidden_size)\n",
    "        \n",
    "        return h0, c0 \n",
    "\n",
    "    def forward_once(self, batch, lengths):\n",
    "        \"\"\"\n",
    "        Performs the forward pass for each batch\n",
    "        \"\"\"\n",
    "\n",
    "        ## batch shape: (batch_size, seq_len)\n",
    "        batch_size , sequence_len = batch.size()\n",
    "        ## embeddings shape: ( batch_size, seq_len, embedding_size)\n",
    "        \n",
    "        #h_init,c_init = self.init_hidden(batch_size)\n",
    "        input_batch_sequences= self.lookup[batch]\n",
    "        \n",
    "        output, (hn, cn) = self.bi_lstm(input_batch_sequences, (self.h_init, self.c_init))\n",
    "\n",
    "        return output , (hn , cn)\n",
    "\n",
    "    def forward(self, sent1_batch, sent2_batch, sent1_lengths, sent2_lengths):\n",
    "        \"\"\"\n",
    "        Performs the forward pass for each batch\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        ## init context and hidden weights for lstm cell\n",
    "        self.h_init,self.c_init = self.init_hidden(self.batch_size)\n",
    "        output1,_ = self.forward_once(sent1_batch,sent1_lengths)\n",
    "        self.h_init,self.c_init = self.init_hidden(self.batch_size)\n",
    "        output2,_ = self.forward_once(sent2_batch,sent2_lengths)\n",
    "        \n",
    "        ## Self attention Layer\n",
    "        attended_embeddings_sent1, attention_matrix_sent1 = self.SelfAtt.forward(output1)\n",
    "        attended_embeddings_sent2, attention_matrix_sent2 = self.SelfAtt.forward(output2)\n",
    "        \n",
    "        ## Fully connected layer \n",
    "        \n",
    "        final_embeddings_sent1= self.tanh(self.fc(attended_embeddings_sent1.reshape(output1.size(0),-1)))\n",
    "        final_embeddings_sent2= self.tanh(self.fc(attended_embeddings_sent2.reshape(output2.size(0),-1)))\n",
    "        \n",
    "        \n",
    "        #similarity score prediction\n",
    "        predictions = similarity_score(final_embeddings_sent1, final_embeddings_sent2)\n",
    "        \n",
    "        print(torch.cat((attention_matrix_sent1, attention_matrix_sent2), 1).size())\n",
    "        return predictions , \n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Implementation of the attention block\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        # TODO implement\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.ws1 = nn.Linear(input_size, hidden_size, bias=False)\n",
    "        self.ws2 = nn.Linear(hidden_size, output_size, bias=False)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.softmax = nn.Softmax()\n",
    "        \n",
    "        \n",
    "    ## the forward function would receive lstm's all hidden states as input\n",
    "    def forward(self, attention_input):\n",
    "        # TODO implement\n",
    "        #pass\n",
    "        \n",
    "        size = attention_input.size()\n",
    "        inp = attention_input.reshape(size[0]*size[1],size[2])\n",
    "        attention_matrix = self.softmax(self.ws2(self.tanh(self.ws1(inp))))\n",
    "        attention_matrix= attention_matrix.reshape(size[0], self.output_size, -1)\n",
    "        attended_embeddings_sent1= torch.bmm(attention_matrix , attention_input)\n",
    "        \n",
    "        return attended_embeddings_sent1, attention_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9639a56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "output_size = 1\n",
    "hidden_size = 128\n",
    "vocab_size = len(vocab)\n",
    "embedding_size = 300\n",
    "embedding_weights = vocab.vectors\n",
    "lstm_layers = 4\n",
    "learning_rate = 1e-1\n",
    "fc_hidden_size = 64\n",
    "max_epochs = 5\n",
    "bidirectional = True\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "## self attention config\n",
    "self_attention_config = {\n",
    "    \"hidden_size\": 150,  ## refers to variable 'da' in the ICLR paper\n",
    "    \"output_size\": 20,  ## refers to variable 'r' in the ICLR paper\n",
    "    \"penalty\": 0.0,  ## refers to penalty coefficient term in the ICLR paper\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2cff170e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SiameseBiLSTMAttention(\n",
       "  (embeddings): Embedding(1962, 300)\n",
       "  (bi_lstm): LSTM(300, 128, num_layers=4, batch_first=True, bidirectional=True)\n",
       "  (SelfAtt): SelfAttention(\n",
       "    (ws1): Linear(in_features=256, out_features=150, bias=False)\n",
       "    (ws2): Linear(in_features=150, out_features=20, bias=False)\n",
       "    (tanh): Tanh()\n",
       "    (softmax): Softmax(dim=None)\n",
       "  )\n",
       "  (fc): Linear(in_features=5120, out_features=64, bias=True)\n",
       "  (tanh): Tanh()\n",
       ")"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## init siamese lstm\n",
    "siamese_lstm_attention = SiameseBiLSTMAttention(\n",
    "    batch_size=batch_size,\n",
    "    output_size=output_size,\n",
    "    hidden_size=hidden_size,\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_size=embedding_size,\n",
    "    embedding_weights=embedding_weights,\n",
    "    lstm_layers=lstm_layers,\n",
    "    self_attention_config=self_attention_config,\n",
    "    fc_hidden_size=fc_hidden_size,\n",
    "    device=device,\n",
    "    bidirectional=bidirectional,\n",
    ")\n",
    "## move model to device\n",
    "optimizer = torch.optim.Adam(params=siamese_lstm_attention.parameters())\n",
    "siamese_lstm_attention.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fe08b333",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 40, 14])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ibrahimssd/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:152: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([0.3208, 0.2469, 0.3409, 0.3280, 0.3510, 0.3162, 0.3752, 0.3889, 0.3286,\n",
       "         0.2408, 0.3983, 0.2968, 0.3417, 0.3010, 0.4053, 0.3349, 0.2852, 0.3299,\n",
       "         0.3505, 0.2940, 0.3519, 0.2892, 0.3620, 0.2359, 0.3191, 0.3963, 0.2952,\n",
       "         0.3042, 0.3244, 0.3142, 0.3722, 0.2982, 0.2970, 0.2820, 0.2952, 0.3185,\n",
       "         0.3692, 0.2766, 0.3553, 0.3955, 0.3946, 0.2812, 0.3229, 0.3506, 0.3468,\n",
       "         0.3425, 0.4210, 0.2896, 0.3079, 0.3460, 0.3980, 0.3299, 0.3534, 0.3295,\n",
       "         0.2843, 0.4037, 0.4177, 0.3238, 0.3244, 0.3301, 0.3451, 0.3137, 0.3332,\n",
       "         0.3395], grad_fn=<ClampBackward1>),)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "siamese_lstm_attention.forward(batch[0],batch[1],7,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a04e6d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "\"\"\"\n",
    "Script for training the neural network and saving the better models \n",
    "while monitoring a metric like accuracy etc\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def train_model(model, optimizer, dataloader, data, max_epochs, config_dict):\n",
    "    device = config_dict[\"device\"]\n",
    "    criterion = nn.MSELoss()\n",
    "    max_accuracy = 5e-1\n",
    "    train_loader , val_loader , test_loader = dataloader\n",
    "    dictionary_info={}\n",
    "    for epoch in tqdm(range(max_epochs)):\n",
    "        \n",
    "        try:\n",
    "            # Samples the batch\n",
    "            sent1_batch, sent2_batch, sent1_lengths, sent2_lengths,targets,raw_sent1,raw_sent2= next(iter(dataloader))\n",
    "\n",
    "        except StopIteration:\n",
    "            # restart the generator if the previous generator is exhausted.\n",
    "            train_generator = iter(train_loader)\n",
    "            sent1_batch, sent2_batch, sent1_lengths, sent2_lengths,targets,raw_sent1,raw_sent2= next(iter(dataloader))\n",
    "        \n",
    "        \n",
    "        predictions , attention_matrix = model.forward(sent1_batch, sent2_batch, sent1_lengths, sent2_lengths)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        try:\n",
    "             train_loss = criterion(predictions,targets) + attention_penalty_loss(attention_matrix, \n",
    "                                                                  config_dict['self_attention_config']['penalty'], device)\n",
    "                       \n",
    "        except RuntimeError:\n",
    "            \n",
    "            raise Exception(\"Model Loss gets nan values on regularization.Either remove regularization or add very small values\")\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        # TODO: computing accuracy using sklearn's function\n",
    "        ## acc = \n",
    "        #accuracy = (torch.argmax(predictions, axis=-1) == targets).float().mean()\n",
    "        \n",
    "        acc=accuracy_score(targets, predictions)\n",
    "        \n",
    "        ## compute model metrics on dev set\n",
    "        val_acc, val_loss = evaluate_dev_set(\n",
    "            model, data, criterion, dataloader, config_dict, device\n",
    "        )\n",
    "\n",
    "        \n",
    "        \n",
    "        if val_acc > max_accuracy:\n",
    "            max_accuracy = val_acc\n",
    "            logging.info(\n",
    "                \"new model saved\")  \n",
    "            \n",
    "            ## save the model if it is better than the prior best\n",
    "            torch.save(model.state_dict(), \"{}.pth\".format(config_dict[\"model_name\"]))\n",
    "\n",
    "        logging.info(\n",
    "            \"Train loss: {} - acc: {} -- Validation loss: {} - acc: {}\".format(\n",
    "                torch.mean(train_loss.data.float()), acc, val_loss, val_acc\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        if epoch % 100 == 0:\n",
    "              print('[%d/%d] loss: %.3f, accuracy: %.3f' %\n",
    "                   (i , max_epochs - 1, loss.item(), acc.item()))\n",
    "        if epoch == max_epochs - 1:\n",
    "               print('Final accuracy: %.3f, expected %.3f' %\n",
    "                         (accuracy.item(), 1.0))\n",
    "        \n",
    "    return model\n",
    "\n",
    "\n",
    "def evaluate_dev_set(model, data, criterion, data_loader, config_dict, device):\n",
    "    \"\"\"\n",
    "    Evaluates the model performance on dev data\n",
    "    \"\"\"\n",
    "    logging.info(\"Evaluating accuracy on dev set\")\n",
    "\n",
    "    # TODO implement\n",
    "    pass\n",
    "\n",
    "def attention_penalty_loss(annotation_weight_matrix, penalty_coef, device):\n",
    "    \"\"\"\n",
    "    This function computes the loss from annotation/attention matrix\n",
    "    to reduce redundancy in annotation matrix and for attention\n",
    "    to focus on different parts of the sequence corresponding to the\n",
    "    penalty term 'P' in the ICLR paper\n",
    "    ----------------------------------\n",
    "    'annotation_weight_matrix' refers to matrix 'A' in the ICLR paper\n",
    "    annotation_weight_matrix shape: (batch_size, attention_out, seq_len)\n",
    "    \"\"\"\n",
    "    \n",
    "    batch_size, attention_out_size = annotation_weight_matrix.size(0), annotation_weight_matrix.size(1)\n",
    "    annotation_weight_matrix_trans = torch.transpose(annotation_weight_matrix, 0, 1)\n",
    "    identity = torch.eye(annotation_weight_matrix.size(0))\n",
    "    annotation_mul_difference=annotation_weight_matrix@annotation_weight_matrix_trans - identity\n",
    "    penalty = frobenius_norm(annotation_mul_difference)\n",
    "    return penalty_coef*penalty\n",
    "\n",
    "\n",
    "def frobenius_norm(annotation_mul_difference):\n",
    "    \"\"\"\n",
    "    Computes the frobenius norm of the annotation_mul_difference input as matrix\n",
    "    \"\"\"\n",
    "   \n",
    "    \"\"\"\n",
    "    \n",
    " \n",
    "    Args:\n",
    "      annotation_mul_difference= ||AAT - I||\n",
    " \n",
    "    Returns:\n",
    "            regularized value\n",
    " \n",
    "       \n",
    "        \"\"\"\n",
    "    \n",
    "#    torch.norm(annotation_mul_difference.float(), p='fro')\n",
    "#     torch.sum(torch.sum(torch.sum(annotation_mul_difference**2,1),1)**0.5).type(torch.DoubleTensor)\n",
    "    return torch.sqrt(torch.sum(annotation_mul_difference**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3d3ce048",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_8320/2170258921.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;34m\"device\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;34m\"model_name\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"siamese_lstm_attention\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0;34m\"self_attention_config\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself_attention_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     },\n\u001b[1;32m     12\u001b[0m )\n",
      "\u001b[0;32m/tmp/ipykernel_8320/1529067863.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, optimizer, dataloader, data, max_epochs, config_dict)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMSELoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mmax_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5e-1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mtrain_loader\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0mdictionary_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 3)"
     ]
    }
   ],
   "source": [
    "siamese_lstm_attention = train_model(\n",
    "    model=siamese_lstm_attention,\n",
    "    optimizer=optimizer,\n",
    "    dataloader=train_loader,\n",
    "    data=sick_data,\n",
    "    max_epochs=max_epochs,\n",
    "    config_dict={\n",
    "        \"device\": device,\n",
    "        \"model_name\": \"siamese_lstm_attention\",\n",
    "        \"self_attention_config\": self_attention_config,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2cd8aeda",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset=  STSDataset(train_data_df['sent1_tensor'],\n",
    "                         train_data_df['sent2_tensor'],\n",
    "                         train_data_df['relatedness_score'],\n",
    "                         train_data_df['sents1_length_tensor'],\n",
    "                         train_data_df['sents2_length_tensor'],\n",
    "                         train_data_df['sentence_A'],\n",
    "                         train_data_df['sentence_B']\n",
    "                         )\n",
    "\n",
    "train_loader = DataLoader(dataset = train_dataset, batch_size = 64, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d71f46f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.1534e-04, 7.6890e-05, 3.2038e-04, 1.2815e-04, 2.3708e-04, 2.1786e-04,\n",
      "        2.4349e-04, 1.9223e-04, 7.0483e-05, 2.1145e-04, 2.1786e-04, 2.2426e-04,\n",
      "        3.2038e-04, 2.9475e-04, 2.4349e-04, 1.0252e-04, 2.4989e-04, 2.9475e-04,\n",
      "        1.6660e-04, 2.4349e-04, 6.4075e-05, 2.4989e-04, 2.8834e-04, 2.9475e-04,\n",
      "        2.1145e-04, 2.5630e-04, 3.0756e-04, 1.9223e-04, 2.0504e-04, 1.7941e-04,\n",
      "        2.8834e-04, 2.6912e-04, 3.2038e-04, 1.9863e-04, 6.4075e-05, 2.3708e-04,\n",
      "        3.1397e-04, 1.1534e-04, 2.6912e-04, 2.0504e-04, 2.1145e-04, 2.5630e-04,\n",
      "        2.3708e-04, 1.6660e-04, 2.4349e-04, 2.1786e-04, 2.2010e-04, 1.0252e-04,\n",
      "        1.2815e-04, 2.1786e-04, 2.0504e-04, 2.1145e-04, 1.8582e-04, 1.4737e-04,\n",
      "        3.0756e-04, 2.1561e-04, 3.0756e-04, 2.6912e-04, 2.5630e-04, 2.9475e-04,\n",
      "        2.4989e-04, 1.9223e-04, 2.4349e-04, 3.1397e-04])\n"
     ]
    }
   ],
   "source": [
    "# main training loop\n",
    "# train_loader , val_loader , test_loader = sick_dataloaders\n",
    "train_generator = iter(train_loader)\n",
    "for i in range(100):\n",
    "    try:\n",
    "        # Samples the batch\n",
    "        sent1_batch, sent2_batch, sent1_lengths, sent2_lengths, targets,raw_sent1,raw_sent2= next(train_generator)\n",
    "        print(Variable(targets))\n",
    "        break\n",
    "    except StopIteration:\n",
    "        # restart the generator if the previous generator is exhausted.\n",
    "        train_generator = iter(train_loader)\n",
    "        sent1_batch, sent2_batch, sent1_lengths, sent2_lengths, targets,raw_sent1,raw_sent2= next(train_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "33b67d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.LongTensor([[1,2,4,5],[4,3,2,9]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "314a39f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.6529, -1.5669, -1.4430,  ...,  0.0196,  1.2238,  1.3934],\n",
       "         [-0.2017, -0.4662, -0.7783,  ..., -1.3892,  0.9228,  0.8104],\n",
       "         [ 1.2984, -1.1671,  1.9707,  ...,  0.2359,  1.2899, -0.3760],\n",
       "         [ 0.5230, -0.5592, -0.6737,  ...,  0.1191, -1.1626,  1.6210]],\n",
       "\n",
       "        [[ 1.2984, -1.1671,  1.9707,  ...,  0.2359,  1.2899, -0.3760],\n",
       "         [-0.2900, -0.8806,  0.4594,  ...,  1.0597, -0.0309, -1.4716],\n",
       "         [-0.2017, -0.4662, -0.7783,  ..., -1.3892,  0.9228,  0.8104],\n",
       "         [ 0.4516,  1.1141, -0.0854,  ...,  0.5649, -0.9711, -0.8820]]],\n",
       "       grad_fn=<EmbeddingBackward>)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.vectors[input]\n",
    "embedding = nn.Embedding(len(vocab), 300)\n",
    "embedding(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "37f91d3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1962, 300])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.vectors.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a6415540",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 40, 40])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# with torch.no_grad():\n",
    "a=torch.randn([64, 40, 14])\n",
    "at= np.transpose(a, (0, 2, 1)).clone().detach().requires_grad_(True)\n",
    "# frobenius_norm(a)\n",
    "diff=a@at-torch.eye(a.size(1))\n",
    "diff.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "adac8040",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 40, 40])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "att=torch.randn([64, 40, 14])\n",
    "attT = att.transpose(1,2)\n",
    "identity = torch.eye(att.size(1))\n",
    "identity = Variable(identity.unsqueeze(0).expand(train_loader.batch_size,att.size(1),att.size(1)))\n",
    "diff=att@attT - identity\n",
    "diff.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "85a9b1c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 1., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 1.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 1., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 1., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 1.]],\n",
       "\n",
       "        [[1., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 1., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 1.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 1., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 1., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 1.]],\n",
       "\n",
       "        [[1., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 1., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 1.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 1., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 1., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 1.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[1., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 1., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 1.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 1., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 1., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 1.]],\n",
       "\n",
       "        [[1., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 1., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 1.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 1., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 1., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 1.]],\n",
       "\n",
       "        [[1., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 1., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 1.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 1., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 1., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 1.]]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "identity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "87963ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1=torch.tensor([0.3724, 0.4367, 0.3584, 0.4131, 0.4151, 0.3754, 0.4444, 0.3806, 0.4540,\n",
    "        0.3293, 0.4363, 0.3715, 0.4190, 0.4442, 0.4099, 0.4260, 0.3768, 0.4290,\n",
    "        0.4401, 0.4283, 0.3629, 0.3899, 0.3863, 0.4223, 0.4615, 0.4136, 0.3793,\n",
    "        0.4060, 0.4051, 0.4540, 0.4548, 0.4677, 0.4346, 0.3802, 0.3646, 0.4234,\n",
    "        0.4135, 0.3769, 0.4404, 0.3910, 0.4340, 0.4046, 0.3871, 0.3655, 0.4430,\n",
    "        0.4434, 0.3824, 0.4189, 0.4324, 0.3946, 0.3496, 0.4714, 0.4147, 0.4114,\n",
    "        0.4263, 0.4088, 0.3955, 0.3722, 0.4222, 0.3962, 0.3961, 0.4276, 0.3328,\n",
    "        0.3668]) \n",
    "t2=torch.tensor([0.7000, 0.1750, 0.5750, 0.8250, 0.3250, 0.4750, 0.8500, 0.7750, 0.5750,\n",
    "        0.6000, 0.5750, 0.4500, 0.6500, 0.6750, 0.7750, 0.9250, 0.1000, 0.8750,\n",
    "        0.5250, 0.6500, 0.5750, 0.3000, 0.0000, 0.6750, 0.0250, 0.7750, 0.9500,\n",
    "        0.6250, 0.3250, 0.9500, 0.6250, 0.5750, 0.7250, 0.4000, 0.9000, 0.7250,\n",
    "        0.9750, 0.0000, 0.5500, 0.6000, 0.8250, 0.2500, 0.7000, 0.9750, 0.8750,\n",
    "        0.5750, 0.6750, 0.6500, 0.0750, 0.9000, 0.1500, 0.8750, 0.7250, 0.6500,\n",
    "        0.8250, 0.5750, 0.5000, 0.7750, 0.5250, 0.0250, 0.9000, 0.5750, 0.7500,\n",
    "        0.0500])\n",
    "# tensor(0.1044, dtype=torch.float64, grad_fn=<AddBackward0>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "cbea0269",
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy_score(list(t1.numpy()),list(t2.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a515f11",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'r2_score' from 'sklearn.metrics.pairwise' (/home/ibrahimssd/.local/lib/python3.7/site-packages/sklearn/metrics/pairwise.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1913/3445951950.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensemble\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRandomForestRegressor\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mrfr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpairwise\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcosine_similarity\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mr2_score\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mmax_error\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean_absolute_error\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexplained_variance_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mr2_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# DistanceMetric(list(t2),list(t1))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'r2_score' from 'sklearn.metrics.pairwise' (/home/ibrahimssd/.local/lib/python3.7/site-packages/sklearn/metrics/pairwise.py)"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor as rfr\n",
    "from sklearn.metrics import pairwise.cosine_similarity,DistanceMetric , r2_score , max_error, mean_absolute_error, explained_variance_score\n",
    "r2_score(list(t2),list(t1))\n",
    "# DistanceMetric(list(t2),list(t1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7479ec80",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'explained_variance_score' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1913/834185789.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mexplained_variance_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'explained_variance_score' is not defined"
     ]
    }
   ],
   "source": [
    "explained_variance_score(t1,t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e909aa79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.7976931348623157e+308"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_error(t1,t2)\n",
    "-1.7976931348623157e+308"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f740394f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 't1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1913/3108703608.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpairwise\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcosine_similarity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcosine_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdense_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 't1' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "cosine_similarity(t1, t2, dense_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ace2b51c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.sampler.BatchSampler at 0x7f46102e0310>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader.batch_sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "5fe9aed7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "5793bfb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4480"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "70*64\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5262269a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x400 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "x = np.linspace(0, 5, 10)\n",
    "y = x ** 2\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(8,4), dpi=100)\n",
    "\n",
    "# plot subplot 1\n",
    "axes[0].plot(x, x**2, color=\"green\", label=\"y = x**2\")\n",
    "axes[0].plot(x, x**3, color=\"red\", label=\"y = x**3\")\n",
    "axes[0].legend(loc=2); # upper left corner\n",
    "axes[0].set_xlabel('x')\n",
    "axes[0].set_ylabel('y')\n",
    "axes[0].set_title('Plot of y=x^2 and y=x^3')\n",
    "\n",
    "# plot subplot 2\n",
    "axes[1].plot(x, x**2, color=\"violet\", label=\"y = x**2\")\n",
    "axes[1].plot(x, x**3, color=\"blue\", label=\"y = x**3\")\n",
    "axes[1].legend(loc=2); # upper left corner\n",
    "axes[1].set_xlabel('x')\n",
    "axes[1].set_ylabel('y')\n",
    "axes[1].set_title('Plot of y=x^2 and y=x^3')\n",
    "\n",
    "# `fig.tight_layout()` automatically adjusts the positions of the axes on the figure canvas so that there is no overlapping content\n",
    "# comment this out to see the difference\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f93c28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
