{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13262990",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab6a4f42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-25 18:08:48.653045: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-02-25 18:08:48.653072: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from test import evaluate_test_set\n",
    "import sts_data\n",
    "from importlib import reload\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b85ac35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from preprocess import Preprocess\n",
    "import logging\n",
    "import torch\n",
    "import re\n",
    "from dataset import STSDataset\n",
    "from datasets import load_dataset,Dataset\n",
    "import spacy\n",
    "import torchtext\n",
    "from torchtext.data import get_tokenizer\n",
    "from spacy.lang.en import English\n",
    "from torchtext.legacy.data import Field\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c1e7f6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values(['sentence_A', 'sentence_B', 'relatedness_score'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataset = load_dataset('text', data_files='SICK.txt')\n",
    "# dataset\n",
    "columns_mapping = {\n",
    "        \"sent1\": \"sentence_A\",\n",
    "        \"sent2\": \"sentence_B\",\n",
    "        \"label\": \"relatedness_score\",\n",
    "    }\n",
    "stopwords_path=\"stopwords-en.txt\"\n",
    "columns_mapping.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e44a157",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Using custom data configuration default\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset sick/default (download: 212.48 KiB, generated: 2.50 MiB, post-processed: Unknown size, total: 2.71 MiB) to /home/ibrahimssd/.cache/huggingface/datasets/sick/default/0.0.0/c6b3b0b44eb84b134851396d6d464e5cb8f026960519d640e087fe33472626db...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset sick downloaded and prepared to /home/ibrahimssd/.cache/huggingface/datasets/sick/default/0.0.0/c6b3b0b44eb84b134851396d6d464e5cb8f026960519d640e087fe33472626db. Subsequent calls will reuse this data.\n"
     ]
    }
   ],
   "source": [
    "sick_dataset = load_dataset('sick',download_mode='reuse_cache_if_exists')\n",
    "sick_dataset=sick_dataset.remove_columns(['label','id','entailment_AB', 'entailment_BA', 'sentence_A_original', 'sentence_B_original', 'sentence_A_dataset', 'sentence_B_dataset'])\n",
    "train_pd=pd.DataFrame.from_dict(sick_dataset['train'])\n",
    "validation_pd=pd.DataFrame.from_dict(sick_dataset['validation'])\n",
    "test_pd=pd.DataFrame.from_dict(sick_dataset['test'])\n",
    "sick_data=[train_pd,validation_pd,test_pd]\n",
    "# sick_df = pd.DataFrame(data=sick_dataset.data, columns=sick_dataset.column_names)\n",
    "# sen_A=columns_mapping['sent1']\n",
    "# sen_B= columns_mapping['sent2']\n",
    "# score=columns_mapping['label']\n",
    "# sick_df=sick_df[[sen_A,sen_B,score]]\n",
    "# pd.read_csv(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be64544e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_data_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_9532/2781948741.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msplits\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msick_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msick_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;36m4439\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_data_df' is not defined"
     ]
    }
   ],
   "source": [
    "# sick_dataset['train']\n",
    "# Dataset.from_pandas(tran_pd)\n",
    "splits= list(sick_dataset.keys())\n",
    "type(sick_dataset)\n",
    "len(train_data_df)\n",
    "4439/64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de534416",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "\"\"\"\n",
    "Performs basic text cleansing on the unstructured field \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class Preprocess:\n",
    "    def __init__(self, stpwds_file_path):\n",
    "        \"\"\"\n",
    "        Initializes regex patterns and loads stopwords\n",
    "        \"\"\"\n",
    "        # TODO implement\n",
    "        with open(stpwds_file_path) as fh:\n",
    "            self.stopwords=list(set(fh.read().split()))\n",
    "        self.noise_re = re.compile('\\\\b(%s)\\\\W'%('|'.join(map(re.escape,self.stopwords))),re.I)\n",
    "    \n",
    "\n",
    "    def perform_preprocessing(self, data, columns_mapping):\n",
    "        sen_A=columns_mapping['sent1']\n",
    "        sen_B= columns_mapping['sent2']\n",
    "        score=columns_mapping['label']\n",
    "        cleaned_data=[]\n",
    "        for data_frame in data:\n",
    "            groupA=list(data_frame[sen_A])\n",
    "            groupB=list(data_frame[sen_B])\n",
    "            ## normalize text to lower case\n",
    "            groupA=[x.lower() for x in groupA]\n",
    "            groupB=[x.lower() for x in groupB]\n",
    "            ## remove punctuations\n",
    "            groupA=[''.join(c for c in x if c not in string.punctuation) for x in groupA]\n",
    "            groupB=[''.join(c for c in x if c not in string.punctuation) for x in groupB]\n",
    "            ## remove stopwords\n",
    "            groupA=[self.noise_re.sub('',p) for p in groupA]\n",
    "            groupB=[self.noise_re.sub('',p) for p in groupB]\n",
    "            # Trim extra whitespace\n",
    "            groupA=[' '.join(x.split()) for x in groupA]\n",
    "            groupB=[' '.join(x.split()) for x in groupB]\n",
    "            # Remove numbers\n",
    "            groupA=[''.join(c for c in x if c not in '0123456789') for x in groupA]\n",
    "            groupB=[''.join(c for c in x if c not in '0123456789') for x in groupB]\n",
    "            ## return data_back to DataFrame\n",
    "            data_frame[sen_A]=groupA\n",
    "            data_frame[sen_B]=groupB\n",
    "            cleaned_data.append(data_frame)\n",
    "        \n",
    "        \n",
    "        sick_dataset={'train':Dataset.from_pandas(cleaned_data[0]),\n",
    "                      'validation':Dataset.from_pandas(cleaned_data[1]),\n",
    "                      'test':Dataset.from_pandas(cleaned_data[2])}\n",
    "        data_frame=pd.concat(cleaned_data, ignore_index=True)\n",
    "        \n",
    "        return sick_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fde4a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "process=Preprocess(stopwords_path)\n",
    "formatted_data=process.perform_preprocessing(sick_data,columns_mapping)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042f6b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data= pd.DataFrame(formatted_data['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f762528",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = list(columns_mapping.values())\n",
    "cols.pop()\n",
    "sen_list=list(train_data[cols].apply(lambda row: ' '.join(row.values.astype(str)), axis=1))\n",
    "sen_str = ' '.join(map(str, sen_list))\n",
    "print(len(sen_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177d693f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word tokenization\n",
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "nlp = English()\n",
    "text = \"\"\"When learning data science, you shouldn't get discouraged!\n",
    "Challenges and setbacks aren't failures, they're just part of the journey. You've got this!\"\"\"\n",
    "\n",
    "#  \"nlp\" Object is used to create documents with linguistic annotations.\n",
    "my_doc = nlp(sen_str)\n",
    "\n",
    "# Create list of word tokens\n",
    "token_list = []\n",
    "for token in my_doc:\n",
    "    token_list.append(token.text)\n",
    "# print(token_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1172f105",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import vocab\n",
    "from collections import Counter, OrderedDict\n",
    "counter = Counter(token_list)\n",
    "sorted_by_freq_tuples = sorted(counter.items(), key=lambda x: x[1], reverse=True)\n",
    "ordered_dict = OrderedDict(sorted_by_freq_tuples)\n",
    "v= vocab(ordered_dict)\n",
    "\n",
    "PAD_token = 0   # Used for padding short sentences\n",
    "SOS_token = 1   # Start-of-sentence token\n",
    "EOS_token = 2   # End-of-sentence token\n",
    "special_words={PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"}\n",
    "default_index = -1\n",
    "v.set_default_index(default_index)\n",
    "for key , value in special_words.items():\n",
    "    if value not in v: v.insert_token(value, key)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7f19ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(stopwords_path) as fh:\n",
    "            stopwords=list(set(fh.read().split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a252b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['sentenceA&B']=train_data[cols].apply(lambda row: ' '.join(row.values.astype(str)), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534639c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_field = Field(\n",
    "#     tokenize='basic_english', \n",
    "    lower=True,\n",
    "    include_lengths=True,\n",
    "    pad_token='PAD',\n",
    "    pad_first='SOS',\n",
    "    stop_words=stopwords,\n",
    ")\n",
    "\n",
    "# label_field = Field(sequential=False, use_vocab=False)\n",
    "preprocessed_text = train_data['sentenceA&B'].apply(lambda x: text_field.preprocess(x))\n",
    "text_field.build_vocab(\n",
    "    preprocessed_text, \n",
    "    vectors='fasttext.simple.300d')\n",
    "\n",
    "# get the vocab instance\n",
    "vocab = text_field.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50a07ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_text\n",
    "len(vocab.itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634009d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def data2tensors(self, data):\n",
    "#         \"\"\"\n",
    "#         Converts raw data sequences into vectorized sequences as tensors\n",
    "#         \"\"\"\n",
    "#         pass\n",
    "#         # TODO implement\n",
    "\n",
    "# def get_data_loader(self, batch_size=8):\n",
    "#         pass\n",
    "#         # TODO implement\n",
    "\n",
    "# def sort_batch(self, batch, targets, lengths):\n",
    "#         \"\"\"\n",
    "#         Sorts the data, lengths and target tensors based on the lengths\n",
    "#         of the sequences from longest to shortest in batch\n",
    "#         \"\"\"\n",
    "#         sents1_lengths, perm_idx = lengths.sort(0, descending=True)\n",
    "#         sequence_tensor = batch[perm_idx]\n",
    "#         target_tensor = targets[perm_idx]\n",
    "#         return sequence_tensor.transpose(0, 1), target_tensor, sents1_lengths\n",
    "\n",
    "# def vectorize_sequence(self, sentence):\n",
    "#         \"\"\"\n",
    "#         Replaces tokens with their indices in vocabulary\n",
    "#         \"\"\"\n",
    "#         pass\n",
    "#         # TODO implement\n",
    "\n",
    "# def pad_sequences(self, vectorized_sents, sents_lengths):\n",
    "#         \"\"\"\n",
    "#         Pads zeros at the end of each sequence in data tensor till max\n",
    "#         length of sequence in that batch\n",
    "#         \"\"\"\n",
    "#         pass\n",
    "#         # TODO implement\n",
    "# def encode(vocab,string):\n",
    "#        return vocab[string]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2f9949",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data['sentenceA&B'][0].apply(lambda x: print(x) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6da72d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_sequence(sentence):\n",
    "        \"\"\"\n",
    "        Replaces tokens with their indices in vocabulary\n",
    "        \"\"\"\n",
    "        splited_sentence=sentence.split()\n",
    "        encodes=[vocab[token] for token in splited_sentence]\n",
    "        return encodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60c0459",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = list(columns_mapping.values())\n",
    "cols.pop()\n",
    "train_data= pd.DataFrame(formatted_data['train'])\n",
    "val_data=pd.DataFrame(formatted_data['validation'])\n",
    "test_data=pd.DataFrame(formatted_data['test'])\n",
    "train_data['sentenceA&B']=train_data[cols].apply(lambda row: ' '.join(row.values.astype(str)), axis=1)\n",
    "val_data['sentenceA&B']=val_data[cols].apply(lambda row: ' '.join(row.values.astype(str)), axis=1)\n",
    "test_data['sentenceA&B']=test_data[cols].apply(lambda row: ' '.join(row.values.astype(str)), axis=1)\n",
    "\n",
    "codes=[]\n",
    "for sentence in val_data['sentenceA&B']:\n",
    "    encodes= vectorize_sequence(sentence)\n",
    "    codes.append(encodes)\n",
    "val_data['sentenceA&B'] = codes\n",
    "val_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90fe4221",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data2tensors(data):\n",
    "        \"\"\"\n",
    "        Converts raw data sequences into vectorized sequences as tensors\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        \n",
    "        data['sent1_tensor']=data['sent1_tensor'].apply(lambda lis : torch.as_tensor(lis))\n",
    "        data['sent2_tensor']=data['sent2_tensor'].apply(lambda lis : torch.as_tensor(lis))\n",
    "           \n",
    "        \n",
    "        \n",
    "        return data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334dd9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame(data2tensors(formatted_data)['train'])[\"sentenceA&B\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e617eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders = {x:DataLoader(formatted_data[x],32, shuffle=True, num_workers=4) for x in ['train','validation','test']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3637178",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb20f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import DataLoader\n",
    "\n",
    "# train_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)\n",
    "# test_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)\n",
    "\n",
    "# train_features, train_labels = next(iter(dataloaders['train']))\n",
    "# from torch.utils import data\n",
    "# train_tensor = data.TensorDataset(train_data, train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74915ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils import data\n",
    "# train_tensor = data.TensorDataset(train_data, train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5702913d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloaders = {'train': torch.utils.data.DataLoader(STSDataset(sts_train_df, batch_size=64)),\n",
    "#                'val': torch.utils.data.DataLoader(STSDataset(sts_dev_df, batch_size=64))}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b415d13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_target = torch.tensor(train['Target'].values.astype(np.float32))\n",
    "# train = torch.tensor(train.drop('Target', axis = 1).values.astype(np.float32)) \n",
    "# train_tensor = data_utils.TensorDataset(train, train_target) \n",
    "# train_loader = data_utils.DataLoader(dataset = train_tensor, batch_size = batch_size, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0612f9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_df= pd.DataFrame(formatted_data['train'])\n",
    "val_data_df=pd.DataFrame(formatted_data['validation'])\n",
    "test_data_df=pd.DataFrame(formatted_data['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a697f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_df['sent1_tensor']=train_data_df['sentence_A'].apply(lambda sen: vectorize_sequence(sen))\n",
    "train_data_df['sent2_tensor']=train_data_df['sentence_B'].apply(lambda sen: vectorize_sequence(sen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f33b74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data_df = torch.tensor(train_data_df['sentence_A'].values.astype(np.float32))\n",
    "train_data_df=data2tensors(train_data_df)\n",
    "train_data_df['sents1_length_tensor']=train_data_df['sent1_tensor'].apply(lambda tensor : torch.tensor(len(tensor)))\n",
    "train_data_df['sents2_length_tensor']=train_data_df['sent2_tensor'].apply(lambda tensor : torch.tensor(len(tensor)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da71456",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f851e252",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pad_sequences\n",
    "# from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "# padded=pad_sequences(train_data_df['sent1_tensor'],padding=\"post\",truncating=”post”,maxlen=8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5b046f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequences(sequences):\n",
    "        \"\"\"\n",
    "        :param sequences: list of tensors\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        num = len(sequences)\n",
    "        max_len = max([s.shape[0] for s in sequences])\n",
    "        out_dims = (num, max_len, *sequences[0].shape[1:])\n",
    "        out_tensor = sequences[0].data.new(*out_dims).fill_(0)\n",
    "        mask = sequences[0].data.new(*out_dims).fill_(0)\n",
    "        for i, tensor in enumerate(sequences):\n",
    "            length = tensor.size(0)\n",
    "            out_tensor[i, :length] = tensor\n",
    "            mask[i, :length] = 1\n",
    "        return list(out_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c09b71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_df['sent1_tensor']=pad_sequences(train_data_df['sent1_tensor'])\n",
    "train_data_df['sent2_tensor']=pad_sequences(train_data_df['sent2_tensor'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff76a379",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_df['relatedness_score']=train_data_df['relatedness_score'].apply(lambda score : torch.tensor(score/sum(train_data_df['relatedness_score'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01dd9d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763635b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Standard Pytorch Dataset class for loading datasets.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class STSDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        sent1_tensor,\n",
    "        sent2_tensor,\n",
    "        target_tensor,\n",
    "        sents1_length_tensor,\n",
    "        sents2_length_tensor,\n",
    "        raw_sents_1,\n",
    "        raw_sents_2,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        initializes  and populates the the length, data and target tensors, and raw texts list\n",
    "        \"\"\"\n",
    "        \n",
    "        assert (\n",
    "            \n",
    "            \n",
    "            len(sent1_tensor)\n",
    "            == torch.tensor(list(target_tensor)).size(0)\n",
    "            == len(sent2_tensor)\n",
    "            == torch.tensor(list(sents1_length_tensor)).size(0)\n",
    "            == torch.tensor(list(sents2_length_tensor)).size(0)\n",
    "        )\n",
    "        self.sent1_tensor = sent1_tensor\n",
    "        self.sent2_tensor = sent2_tensor\n",
    "        self.target_tensor = target_tensor\n",
    "        self.sents1_length_tensor = sents1_length_tensor\n",
    "        self.sents2_length_tensor = sents2_length_tensor\n",
    "        self.raw_sents_1 = raw_sents_1\n",
    "        self.raw_sents_2 = raw_sents_2\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        returns the tuple of data tensor, targets, lengths of sequences tensor and raw texts list\n",
    "        \"\"\"\n",
    "        return (\n",
    "            self.sent1_tensor[index],\n",
    "            self.sent2_tensor[index],\n",
    "            self.sents1_length_tensor[index],\n",
    "            self.sents2_length_tensor[index],\n",
    "            self.target_tensor[index],\n",
    "            self.raw_sents_1[index],\n",
    "            self.raw_sents_2[index],\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        returns the length of the data tensor.\n",
    "        \"\"\"\n",
    "        return torch.tensor(list(self.target_tensor)).size(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd72037",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_df                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243b82ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset=  STSDataset(train_data_df['sent1_tensor'],\n",
    "                         train_data_df['sent2_tensor'],\n",
    "                         train_data_df['relatedness_score'],\n",
    "                         train_data_df['sents1_length_tensor'],\n",
    "                         train_data_df['sents2_length_tensor'],\n",
    "                         train_data_df['sentence_A'],\n",
    "                         train_data_df['sentence_B']\n",
    "                         )\n",
    "\n",
    "train_loader = DataLoader(dataset = train_dataset, batch_size = 64, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2858ea5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(train_data_df['sent1_tensor'])\n",
    "# train_data_df['sents1_length_tensor'].size()\n",
    "# torch.tensor(list(target_tensor)).size(0)\n",
    "# train_data_df['sent1_tensor'][0]\n",
    "# train_data_df['relatedness_score'][0]\n",
    "# train_data_df['sents1_length_tensor'][0]\n",
    "# train_data_df['sentence_A'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c023118a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e699fbb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_df['relatedness_score'][0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94cb6e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448f4add",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(train_loader)\n",
    "len(dataiter.next())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01def1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_loader:\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6d99c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642c240c",
   "metadata": {},
   "outputs": [],
   "source": [
    "n, d, m = 3, 5, 7\n",
    "embedding = torch.nn.Embedding(n, d, max_norm=True)\n",
    "W = torch.randn((m, d), requires_grad=True)\n",
    "idx = torch.tensor([1, 2])\n",
    "# a = embedding.weight.clone() @ W.t()  # weight must be cloned for this to be differentiable\n",
    "# b = embedding(idx) @ W.t()  # modifies weight in-place\n",
    "# out = (a.unsqueeze(0) + b.unsqueeze(1))\n",
    "# loss = out.sigmoid().prod()\n",
    "# loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47872b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b642cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size= len(vocab)\n",
    "embedding_size= 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b5e9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = torch.nn.Embedding(vocab_size, embedding_size, max_norm=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7caadc40",
   "metadata": {},
   "outputs": [],
   "source": [
    "sen=torch.tensor([[121,  82,   5,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
    "        [ 36,   4, 404,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d74e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding(sen).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb1a9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab.vectors[sen]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181e478d",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_weights= vocab.vectors\n",
    "net = torch.nn.LSTM(10, 20, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef22ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_score(input1, input2):\n",
    "    # Get similarity predictions:\n",
    "    dif = input1.squeeze() - input2.squeeze()\n",
    "\n",
    "    norm = torch.norm(dif, p=1, dim=dif.dim() - 1)\n",
    "    y_hat = torch.exp(-norm)\n",
    "    y_hat = torch.clamp(y_hat, min=1e-7, max=1.0 - 1e-7)\n",
    "    return y_hat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b65dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sts_data import STSData\n",
    "\n",
    "columns_mapping = {\n",
    "        \"sent1\": \"sentence_A\",\n",
    "        \"sent2\": \"sentence_B\",\n",
    "        \"label\": \"relatedness_score\",\n",
    "    }\n",
    "dataset_name = \"sick\"\n",
    "sick_data = STSData(\n",
    "    dataset_name=dataset_name,\n",
    "    columns_mapping=columns_mapping,\n",
    "    normalize_labels=True,\n",
    "    normalization_const=5.0,\n",
    ")\n",
    "batch_size = 64\n",
    "sick_dataloaders = sick_data.get_data_loader(batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2448c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from utils import similarity_score\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Wrapper class using Pytorch nn.Module to create the architecture for our model\n",
    "Architecture is based on the paper: \n",
    "A STRUCTURED SELF-ATTENTIVE SENTENCE EMBEDDING\n",
    "https://arxiv.org/pdf/1703.03130.pdf\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class SiameseBiLSTMAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        batch_size,\n",
    "        output_size,\n",
    "        hidden_size,\n",
    "        vocab_size,\n",
    "        embedding_size,\n",
    "        embedding_weights,\n",
    "        lstm_layers,\n",
    "        device,\n",
    "        bidirectional,\n",
    "        self_attention_config,\n",
    "        fc_hidden_size,\n",
    "    ):\n",
    "        super(SiameseBiLSTMAttention, self).__init__()\n",
    "        \"\"\"\n",
    "        Initializes model layers and loads pre-trained embeddings from task 1\n",
    "        \"\"\"\n",
    "        ## model hyper parameters\n",
    "        self.batch_size = batch_size\n",
    "        self.output_size = output_size\n",
    "        self.lstm_hidden_size = hidden_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.lstm_layers = lstm_layers\n",
    "        self.device = device\n",
    "        self.bidirectional = bidirectional\n",
    "        self.fc_hidden_size = fc_hidden_size\n",
    "        self.lstm_directions = (\n",
    "            2 if self.bidirectional else 1\n",
    "        )  ## decide directions based on input flag\n",
    "        \n",
    "        \n",
    "        \n",
    "        ## model layers\n",
    "        # TODO initialize the look-up table.\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_size)\n",
    "        # TODO assign the look-up table to the pre-trained fasttext word embeddings.\n",
    "        \n",
    "        self.lookup= embedding_weights   #self.embeddings\n",
    "        \n",
    "        ## TODO initialize lstm layer\n",
    "        self.bi_lstm = torch.nn.LSTM(self.embedding_size, self.lstm_hidden_size, \n",
    "                            lstm_layers, batch_first=True , bias= True,bidirectional=True)\n",
    "        \n",
    "        ## TODO initialize self attention layers\n",
    "        self.SelfAtt= SelfAttention(2*self.lstm_hidden_size, self_attention_config['hidden_size'],\n",
    "                                    self_attention_config['output_size'])\n",
    "        \n",
    "        #Initialize fully connected layer\n",
    "        self.fc = nn.Linear(2*self.lstm_hidden_size * self_attention_config['output_size'], self.fc_hidden_size )\n",
    "        self.tanh = nn.Tanh()\n",
    "        ## incase we are using bi-directional lstm we'd have to take care of bi-directional outputs in\n",
    "        ## subsequent layers\n",
    "        \n",
    "        \n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        \"\"\"\n",
    "        Initializes hidden and context weight matrix before each\n",
    "                forward pass through LSTM\n",
    "        \"\"\"\n",
    "        h0 = torch.randn(self.lstm_directions*self.lstm_layers, batch_size, self.lstm_hidden_size)\n",
    "        c0 = torch.randn(self.lstm_directions*self.lstm_layers, batch_size, self.lstm_hidden_size)\n",
    "        \n",
    "        return h0, c0 \n",
    "\n",
    "    def forward_once(self, batch, lengths):\n",
    "        \"\"\"\n",
    "        Performs the forward pass for each batch\n",
    "        \"\"\"\n",
    "\n",
    "        ## batch shape: (batch_size, seq_len)\n",
    "        batch_size , sequence_len = batch.size()\n",
    "        ## embeddings shape: ( batch_size, seq_len, embedding_size)\n",
    "        \n",
    "        #h_init,c_init = self.init_hidden(batch_size)\n",
    "        input_batch_sequences= self.lookup[batch]\n",
    "        \n",
    "        output, (hn, cn) = self.bi_lstm(input_batch_sequences, (self.h_init, self.c_init))\n",
    "\n",
    "        return output , (hn , cn)\n",
    "\n",
    "    def forward(self, sent1_batch, sent2_batch, sent1_lengths, sent2_lengths):\n",
    "        \"\"\"\n",
    "        Performs the forward pass for each batch\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        ## init context and hidden weights for lstm cell\n",
    "        self.h_init,self.c_init = self.init_hidden(self.batch_size)\n",
    "        output1,_ = self.forward_once(sent1_batch,sent1_lengths)\n",
    "        self.h_init,self.c_init = self.init_hidden(self.batch_size)\n",
    "        output2,_ = self.forward_once(sent2_batch,sent2_lengths)\n",
    "        \n",
    "        ## Self attention Layer\n",
    "        attended_embeddings_sent1, attention_matrix_sent1 = self.SelfAtt.forward(output1)\n",
    "        attended_embeddings_sent2, attention_matrix_sent2 = self.SelfAtt.forward(output2)\n",
    "        \n",
    "        ## Fully connected layer \n",
    "        \n",
    "        final_embeddings_sent1= self.tanh(self.fc(attended_embeddings_sent1.reshape(output1.size(0),-1)))\n",
    "        final_embeddings_sent2= self.tanh(self.fc(attended_embeddings_sent2.reshape(output2.size(0),-1)))\n",
    "        \n",
    "        \n",
    "        #similarity score prediction\n",
    "        predictions = similarity_score(final_embeddings_sent1, final_embeddings_sent2)\n",
    "        \n",
    "        print(torch.cat((attention_matrix_sent1, attention_matrix_sent2), 1).size())\n",
    "        return predictions , \n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Implementation of the attention block\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        # TODO implement\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.ws1 = nn.Linear(input_size, hidden_size, bias=False)\n",
    "        self.ws2 = nn.Linear(hidden_size, output_size, bias=False)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.softmax = nn.Softmax()\n",
    "        \n",
    "        \n",
    "    ## the forward function would receive lstm's all hidden states as input\n",
    "    def forward(self, attention_input):\n",
    "        # TODO implement\n",
    "        #pass\n",
    "        \n",
    "        size = attention_input.size()\n",
    "        inp = attention_input.reshape(size[0]*size[1],size[2])\n",
    "        attention_matrix = self.softmax(self.ws2(self.tanh(self.ws1(inp))))\n",
    "        attention_matrix= attention_matrix.reshape(size[0], self.output_size, -1)\n",
    "        attended_embeddings_sent1= torch.bmm(attention_matrix , attention_input)\n",
    "        \n",
    "        return attended_embeddings_sent1, attention_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9639a56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "output_size = 1\n",
    "hidden_size = 128\n",
    "vocab_size = len(vocab)\n",
    "embedding_size = 300\n",
    "embedding_weights = vocab.vectors\n",
    "lstm_layers = 4\n",
    "learning_rate = 1e-1\n",
    "fc_hidden_size = 64\n",
    "max_epochs = 5\n",
    "bidirectional = True\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "## self attention config\n",
    "self_attention_config = {\n",
    "    \"hidden_size\": 150,  ## refers to variable 'da' in the ICLR paper\n",
    "    \"output_size\": 20,  ## refers to variable 'r' in the ICLR paper\n",
    "    \"penalty\": 0.0,  ## refers to penalty coefficient term in the ICLR paper\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cff170e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## init siamese lstm\n",
    "siamese_lstm_attention = SiameseBiLSTMAttention(\n",
    "    batch_size=batch_size,\n",
    "    output_size=output_size,\n",
    "    hidden_size=hidden_size,\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_size=embedding_size,\n",
    "    embedding_weights=embedding_weights,\n",
    "    lstm_layers=lstm_layers,\n",
    "    self_attention_config=self_attention_config,\n",
    "    fc_hidden_size=fc_hidden_size,\n",
    "    device=device,\n",
    "    bidirectional=bidirectional,\n",
    ")\n",
    "## move model to device\n",
    "optimizer = torch.optim.Adam(params=siamese_lstm_attention.parameters())\n",
    "siamese_lstm_attention.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe08b333",
   "metadata": {},
   "outputs": [],
   "source": [
    "siamese_lstm_attention.forward(batch[0],batch[1],7,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04e6d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "\"\"\"\n",
    "Script for training the neural network and saving the better models \n",
    "while monitoring a metric like accuracy etc\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def train_model(model, optimizer, dataloader, data, max_epochs, config_dict):\n",
    "    device = config_dict[\"device\"]\n",
    "    criterion = nn.MSELoss()\n",
    "    max_accuracy = 5e-1\n",
    "    train_loader , val_loader , test_loader = dataloader\n",
    "    dictionary_info={}\n",
    "    for epoch in tqdm(range(max_epochs)):\n",
    "        \n",
    "        try:\n",
    "            # Samples the batch\n",
    "            sent1_batch, sent2_batch, sent1_lengths, sent2_lengths,targets,raw_sent1,raw_sent2= next(iter(dataloader))\n",
    "\n",
    "        except StopIteration:\n",
    "            # restart the generator if the previous generator is exhausted.\n",
    "            train_generator = iter(train_loader)\n",
    "            sent1_batch, sent2_batch, sent1_lengths, sent2_lengths,targets,raw_sent1,raw_sent2= next(iter(dataloader))\n",
    "        \n",
    "        \n",
    "        predictions , attention_matrix = model.forward(sent1_batch, sent2_batch, sent1_lengths, sent2_lengths)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        try:\n",
    "             train_loss = criterion(predictions,targets) + attention_penalty_loss(attention_matrix, \n",
    "                                                                  config_dict['self_attention_config']['penalty'], device)\n",
    "                       \n",
    "        except RuntimeError:\n",
    "            \n",
    "            raise Exception(\"Model Loss gets nan values on regularization.Either remove regularization or add very small values\")\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        # TODO: computing accuracy using sklearn's function\n",
    "        ## acc = \n",
    "        #accuracy = (torch.argmax(predictions, axis=-1) == targets).float().mean()\n",
    "        \n",
    "        acc=accuracy_score(targets, predictions)\n",
    "        \n",
    "        ## compute model metrics on dev set\n",
    "        val_acc, val_loss = evaluate_dev_set(\n",
    "            model, data, criterion, dataloader, config_dict, device\n",
    "        )\n",
    "\n",
    "        \n",
    "        \n",
    "        if val_acc > max_accuracy:\n",
    "            max_accuracy = val_acc\n",
    "            logging.info(\n",
    "                \"new model saved\")  \n",
    "            \n",
    "            ## save the model if it is better than the prior best\n",
    "            torch.save(model.state_dict(), \"{}.pth\".format(config_dict[\"model_name\"]))\n",
    "\n",
    "        logging.info(\n",
    "            \"Train loss: {} - acc: {} -- Validation loss: {} - acc: {}\".format(\n",
    "                torch.mean(train_loss.data.float()), acc, val_loss, val_acc\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        if epoch % 100 == 0:\n",
    "              print('[%d/%d] loss: %.3f, accuracy: %.3f' %\n",
    "                   (i , max_epochs - 1, loss.item(), acc.item()))\n",
    "        if epoch == max_epochs - 1:\n",
    "               print('Final accuracy: %.3f, expected %.3f' %\n",
    "                         (accuracy.item(), 1.0))\n",
    "        \n",
    "    return model\n",
    "\n",
    "\n",
    "def evaluate_dev_set(model, data, criterion, data_loader, config_dict, device):\n",
    "    \"\"\"\n",
    "    Evaluates the model performance on dev data\n",
    "    \"\"\"\n",
    "    logging.info(\"Evaluating accuracy on dev set\")\n",
    "\n",
    "    # TODO implement\n",
    "    pass\n",
    "\n",
    "def attention_penalty_loss(annotation_weight_matrix, penalty_coef, device):\n",
    "    \"\"\"\n",
    "    This function computes the loss from annotation/attention matrix\n",
    "    to reduce redundancy in annotation matrix and for attention\n",
    "    to focus on different parts of the sequence corresponding to the\n",
    "    penalty term 'P' in the ICLR paper\n",
    "    ----------------------------------\n",
    "    'annotation_weight_matrix' refers to matrix 'A' in the ICLR paper\n",
    "    annotation_weight_matrix shape: (batch_size, attention_out, seq_len)\n",
    "    \"\"\"\n",
    "    \n",
    "    batch_size, attention_out_size = annotation_weight_matrix.size(0), annotation_weight_matrix.size(1)\n",
    "    annotation_weight_matrix_trans = torch.transpose(annotation_weight_matrix, 0, 1)\n",
    "    identity = torch.eye(annotation_weight_matrix.size(0))\n",
    "    annotation_mul_difference=annotation_weight_matrix@annotation_weight_matrix_trans - identity\n",
    "    penalty = frobenius_norm(annotation_mul_difference)\n",
    "    return penalty_coef*penalty\n",
    "\n",
    "\n",
    "def frobenius_norm(annotation_mul_difference):\n",
    "    \"\"\"\n",
    "    Computes the frobenius norm of the annotation_mul_difference input as matrix\n",
    "    \"\"\"\n",
    "   \n",
    "    \"\"\"\n",
    "    \n",
    " \n",
    "    Args:\n",
    "      annotation_mul_difference= ||AAT - I||\n",
    " \n",
    "    Returns:\n",
    "            regularized value\n",
    " \n",
    "       \n",
    "        \"\"\"\n",
    "    \n",
    "#    torch.norm(annotation_mul_difference.float(), p='fro')\n",
    "#     torch.sum(torch.sum(torch.sum(annotation_mul_difference**2,1),1)**0.5).type(torch.DoubleTensor)\n",
    "    return torch.sqrt(torch.sum(annotation_mul_difference**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3ce048",
   "metadata": {},
   "outputs": [],
   "source": [
    "siamese_lstm_attention = train_model(\n",
    "    model=siamese_lstm_attention,\n",
    "    optimizer=optimizer,\n",
    "    dataloader=train_loader,\n",
    "    data=sick_data,\n",
    "    max_epochs=max_epochs,\n",
    "    config_dict={\n",
    "        \"device\": device,\n",
    "        \"model_name\": \"siamese_lstm_attention\",\n",
    "        \"self_attention_config\": self_attention_config,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd8aeda",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset=  STSDataset(train_data_df['sent1_tensor'],\n",
    "                         train_data_df['sent2_tensor'],\n",
    "                         train_data_df['relatedness_score'],\n",
    "                         train_data_df['sents1_length_tensor'],\n",
    "                         train_data_df['sents2_length_tensor'],\n",
    "                         train_data_df['sentence_A'],\n",
    "                         train_data_df['sentence_B']\n",
    "                         )\n",
    "\n",
    "train_loader = DataLoader(dataset = train_dataset, batch_size = 64, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71f46f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main training loop\n",
    "# train_loader , val_loader , test_loader = sick_dataloaders\n",
    "train_generator = iter(train_loader)\n",
    "for i in range(100):\n",
    "    try:\n",
    "        # Samples the batch\n",
    "        sent1_batch, sent2_batch, sent1_lengths, sent2_lengths, targets,raw_sent1,raw_sent2= next(train_generator)\n",
    "        print(Variable(targets))\n",
    "        break\n",
    "    except StopIteration:\n",
    "        # restart the generator if the previous generator is exhausted.\n",
    "        train_generator = iter(train_loader)\n",
    "        sent1_batch, sent2_batch, sent1_lengths, sent2_lengths, targets,raw_sent1,raw_sent2= next(train_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b67d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.LongTensor([[1,2,4,5],[4,3,2,9]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314a39f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab.vectors[input]\n",
    "embedding = nn.Embedding(len(vocab), 300)\n",
    "embedding(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f91d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab.vectors.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6415540",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with torch.no_grad():\n",
    "a=torch.randn([64, 40, 14])\n",
    "at= np.transpose(a, (0, 2, 1)).clone().detach().requires_grad_(True)\n",
    "# frobenius_norm(a)\n",
    "diff=a@at-torch.eye(a.size(1))\n",
    "diff.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adac8040",
   "metadata": {},
   "outputs": [],
   "source": [
    "att=torch.randn([64, 40, 14])\n",
    "attT = att.transpose(1,2)\n",
    "identity = torch.eye(att.size(1))\n",
    "identity = Variable(identity.unsqueeze(0).expand(train_loader.batch_size,att.size(1),att.size(1)))\n",
    "diff=att@attT - identity\n",
    "diff.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a9b1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "identity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87963ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1=torch.tensor([0.3724, 0.4367, 0.3584, 0.4131, 0.4151, 0.3754, 0.4444, 0.3806, 0.4540,\n",
    "        0.3293, 0.4363, 0.3715, 0.4190, 0.4442, 0.4099, 0.4260, 0.3768, 0.4290,\n",
    "        0.4401, 0.4283, 0.3629, 0.3899, 0.3863, 0.4223, 0.4615, 0.4136, 0.3793,\n",
    "        0.4060, 0.4051, 0.4540, 0.4548, 0.4677, 0.4346, 0.3802, 0.3646, 0.4234,\n",
    "        0.4135, 0.3769, 0.4404, 0.3910, 0.4340, 0.4046, 0.3871, 0.3655, 0.4430,\n",
    "        0.4434, 0.3824, 0.4189, 0.4324, 0.3946, 0.3496, 0.4714, 0.4147, 0.4114,\n",
    "        0.4263, 0.4088, 0.3955, 0.3722, 0.4222, 0.3962, 0.3961, 0.4276, 0.3328,\n",
    "        0.3668]) \n",
    "t2=torch.tensor([0.7000, 0.1750, 0.5750, 0.8250, 0.3250, 0.4750, 0.8500, 0.7750, 0.5750,\n",
    "        0.6000, 0.5750, 0.4500, 0.6500, 0.6750, 0.7750, 0.9250, 0.1000, 0.8750,\n",
    "        0.5250, 0.6500, 0.5750, 0.3000, 0.0000, 0.6750, 0.0250, 0.7750, 0.9500,\n",
    "        0.6250, 0.3250, 0.9500, 0.6250, 0.5750, 0.7250, 0.4000, 0.9000, 0.7250,\n",
    "        0.9750, 0.0000, 0.5500, 0.6000, 0.8250, 0.2500, 0.7000, 0.9750, 0.8750,\n",
    "        0.5750, 0.6750, 0.6500, 0.0750, 0.9000, 0.1500, 0.8750, 0.7250, 0.6500,\n",
    "        0.8250, 0.5750, 0.5000, 0.7750, 0.5250, 0.0250, 0.9000, 0.5750, 0.7500,\n",
    "        0.0500])\n",
    "# tensor(0.1044, dtype=torch.float64, grad_fn=<AddBackward0>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbea0269",
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy_score(list(t1.numpy()),list(t2.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a515f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor as rfr\n",
    "from sklearn.metrics import pairwise.cosine_similarity,DistanceMetric , r2_score , max_error, mean_absolute_error, explained_variance_score\n",
    "r2_score(list(t2),list(t1))\n",
    "# DistanceMetric(list(t2),list(t1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7479ec80",
   "metadata": {},
   "outputs": [],
   "source": [
    "explained_variance_score(t1,t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e909aa79",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_absolute_error(t1,t2)\n",
    "-1.7976931348623157e+308"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f740394f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "cosine_similarity(t1, t2, dense_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace2b51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader.batch_sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe9aed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5793bfb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "70*64\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5262269a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "x = np.linspace(0, 5, 10)\n",
    "y = x ** 2\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(8,4), dpi=100)\n",
    "\n",
    "# plot subplot 1\n",
    "axes[0].plot(x, x**2, color=\"green\", label=\"y = x**2\")\n",
    "axes[0].plot(x, x**3, color=\"red\", label=\"y = x**3\")\n",
    "axes[0].legend(loc=2); # upper left corner\n",
    "axes[0].set_xlabel('x')\n",
    "axes[0].set_ylabel('y')\n",
    "axes[0].set_title('Plot of y=x^2 and y=x^3')\n",
    "\n",
    "# plot subplot 2\n",
    "axes[1].plot(x, x**2, color=\"violet\", label=\"y = x**2\")\n",
    "axes[1].plot(x, x**3, color=\"blue\", label=\"y = x**3\")\n",
    "axes[1].legend(loc=2); # upper left corner\n",
    "axes[1].set_xlabel('x')\n",
    "axes[1].set_ylabel('y')\n",
    "axes[1].set_title('Plot of y=x^2 and y=x^3')\n",
    "\n",
    "# `fig.tight_layout()` automatically adjusts the positions of the axes on the figure canvas so that there is no overlapping content\n",
    "# comment this out to see the difference\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f93c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "train=train_pd\n",
    "train_pd['relatedness_score']=(train_pd['relatedness_score']-train_pd['relatedness_score'].min())/(train_pd['relatedness_score'].max()-train_pd['relatedness_score'].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "32adac1e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'batch_train_num' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_9532/116516476.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_train_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                 \u001b[0;31m# Samples the batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                 \u001b[0msent1_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent2_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent1_lengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent2_lengths\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mraw_sent1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mraw_sent2\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'batch_train_num' is not defined"
     ]
    }
   ],
   "source": [
    "for batch in range(batch_train_num):\n",
    "            \n",
    "            try:\n",
    "                # Samples the batch\n",
    "                sent1_batch, sent2_batch, sent1_lengths, sent2_lengths,targets,raw_sent1,raw_sent2= next(train_generator)\n",
    "\n",
    "            except StopIteration:\n",
    "                # restart the generator if the previous generator is exhausted.\n",
    "                train_generator = iter(train_loader)\n",
    "                sent1_batch, sent2_batch, sent1_lengths, sent2_lengths,targets,raw_sent1,raw_sent2= next(train_generator)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "39dc3090",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function datasets.load.load_dataset(path: str, name: Union[str, NoneType] = None, data_dir: Union[str, NoneType] = None, data_files: Union[Dict, List] = None, split: Union[str, datasets.splits.Split, NoneType] = None, cache_dir: Union[str, NoneType] = None, features: Union[datasets.features.Features, NoneType] = None, download_config: Union[datasets.utils.file_utils.DownloadConfig, NoneType] = None, download_mode: Union[datasets.utils.download_manager.GenerateMode, NoneType] = None, ignore_verifications: bool = False, keep_in_memory: Union[bool, NoneType] = None, save_infos: bool = False, script_version: Union[str, datasets.utils.version.Version, NoneType] = None, use_auth_token: Union[str, bool, NoneType] = None, task: Union[str, datasets.tasks.base.TaskTemplate, NoneType] = None, streaming: bool = False, **config_kwargs) -> Union[datasets.dataset_dict.DatasetDict, datasets.arrow_dataset.Dataset, datasets.dataset_dict.IterableDatasetDict, datasets.iterable_dataset.IterableDataset]>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb8a9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.zero_grad(req)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
