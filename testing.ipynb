{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13262990",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab6a4f42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-25 18:08:48.653045: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-02-25 18:08:48.653072: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from test import evaluate_test_set\n",
    "import sts_data\n",
    "from importlib import reload\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b85ac35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from preprocess import Preprocess\n",
    "import logging\n",
    "import torch\n",
    "import re\n",
    "from dataset import STSDataset\n",
    "from datasets import load_dataset,Dataset\n",
    "import spacy\n",
    "import torchtext\n",
    "from torchtext.data import get_tokenizer\n",
    "from spacy.lang.en import English\n",
    "from torchtext.legacy.data import Field\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c1e7f6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values(['sentence_A', 'sentence_B', 'relatedness_score'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataset = load_dataset('text', data_files='SICK.txt')\n",
    "# dataset\n",
    "columns_mapping = {\n",
    "        \"sent1\": \"sentence_A\",\n",
    "        \"sent2\": \"sentence_B\",\n",
    "        \"label\": \"relatedness_score\",\n",
    "    }\n",
    "stopwords_path=\"stopwords-en.txt\"\n",
    "columns_mapping.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e44a157",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Using custom data configuration default\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset sick/default (download: 212.48 KiB, generated: 2.50 MiB, post-processed: Unknown size, total: 2.71 MiB) to /home/ibrahimssd/.cache/huggingface/datasets/sick/default/0.0.0/c6b3b0b44eb84b134851396d6d464e5cb8f026960519d640e087fe33472626db...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset sick downloaded and prepared to /home/ibrahimssd/.cache/huggingface/datasets/sick/default/0.0.0/c6b3b0b44eb84b134851396d6d464e5cb8f026960519d640e087fe33472626db. Subsequent calls will reuse this data.\n"
     ]
    }
   ],
   "source": [
    "sick_dataset = load_dataset('sick',download_mode='reuse_cache_if_exists')\n",
    "sick_dataset=sick_dataset.remove_columns(['label','id','entailment_AB', 'entailment_BA', 'sentence_A_original', 'sentence_B_original', 'sentence_A_dataset', 'sentence_B_dataset'])\n",
    "train_pd=pd.DataFrame.from_dict(sick_dataset['train'])\n",
    "validation_pd=pd.DataFrame.from_dict(sick_dataset['validation'])\n",
    "test_pd=pd.DataFrame.from_dict(sick_dataset['test'])\n",
    "sick_data=[train_pd,validation_pd,test_pd]\n",
    "# sick_df = pd.DataFrame(data=sick_dataset.data, columns=sick_dataset.column_names)\n",
    "# sen_A=columns_mapping['sent1']\n",
    "# sen_B= columns_mapping['sent2']\n",
    "# score=columns_mapping['label']\n",
    "# sick_df=sick_df[[sen_A,sen_B,score]]\n",
    "# pd.read_csv(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be64544e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_data_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_9532/2781948741.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msplits\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msick_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msick_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;36m4439\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_data_df' is not defined"
     ]
    }
   ],
   "source": [
    "# sick_dataset['train']\n",
    "# Dataset.from_pandas(tran_pd)\n",
    "splits= list(sick_dataset.keys())\n",
    "type(sick_dataset)\n",
    "len(train_data_df)\n",
    "4439/64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de534416",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "\"\"\"\n",
    "Performs basic text cleansing on the unstructured field \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class Preprocess:\n",
    "    def __init__(self, stpwds_file_path):\n",
    "        \"\"\"\n",
    "        Initializes regex patterns and loads stopwords\n",
    "        \"\"\"\n",
    "        # TODO implement\n",
    "        with open(stpwds_file_path) as fh:\n",
    "            self.stopwords=list(set(fh.read().split()))\n",
    "        self.noise_re = re.compile('\\\\b(%s)\\\\W'%('|'.join(map(re.escape,self.stopwords))),re.I)\n",
    "    \n",
    "\n",
    "    def perform_preprocessing(self, data, columns_mapping):\n",
    "        sen_A=columns_mapping['sent1']\n",
    "        sen_B= columns_mapping['sent2']\n",
    "        score=columns_mapping['label']\n",
    "        cleaned_data=[]\n",
    "        for data_frame in data:\n",
    "            groupA=list(data_frame[sen_A])\n",
    "            groupB=list(data_frame[sen_B])\n",
    "            ## normalize text to lower case\n",
    "            groupA=[x.lower() for x in groupA]\n",
    "            groupB=[x.lower() for x in groupB]\n",
    "            ## remove punctuations\n",
    "            groupA=[''.join(c for c in x if c not in string.punctuation) for x in groupA]\n",
    "            groupB=[''.join(c for c in x if c not in string.punctuation) for x in groupB]\n",
    "            ## remove stopwords\n",
    "            groupA=[self.noise_re.sub('',p) for p in groupA]\n",
    "            groupB=[self.noise_re.sub('',p) for p in groupB]\n",
    "            # Trim extra whitespace\n",
    "            groupA=[' '.join(x.split()) for x in groupA]\n",
    "            groupB=[' '.join(x.split()) for x in groupB]\n",
    "            # Remove numbers\n",
    "            groupA=[''.join(c for c in x if c not in '0123456789') for x in groupA]\n",
    "            groupB=[''.join(c for c in x if c not in '0123456789') for x in groupB]\n",
    "            ## return data_back to DataFrame\n",
    "            data_frame[sen_A]=groupA\n",
    "            data_frame[sen_B]=groupB\n",
    "            cleaned_data.append(data_frame)\n",
    "        \n",
    "        \n",
    "        sick_dataset={'train':Dataset.from_pandas(cleaned_data[0]),\n",
    "                      'validation':Dataset.from_pandas(cleaned_data[1]),\n",
    "                      'test':Dataset.from_pandas(cleaned_data[2])}\n",
    "        data_frame=pd.concat(cleaned_data, ignore_index=True)\n",
    "        \n",
    "        return sick_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fde4a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "process=Preprocess(stopwords_path)\n",
    "formatted_data=process.perform_preprocessing(sick_data,columns_mapping)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042f6b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data= pd.DataFrame(formatted_data['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f762528",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = list(columns_mapping.values())\n",
    "cols.pop()\n",
    "sen_list=list(train_data[cols].apply(lambda row: ' '.join(row.values.astype(str)), axis=1))\n",
    "sen_str = ' '.join(map(str, sen_list))\n",
    "print(len(sen_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177d693f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word tokenization\n",
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "nlp = English()\n",
    "text = \"\"\"When learning data science, you shouldn't get discouraged!\n",
    "Challenges and setbacks aren't failures, they're just part of the journey. You've got this!\"\"\"\n",
    "\n",
    "#  \"nlp\" Object is used to create documents with linguistic annotations.\n",
    "my_doc = nlp(sen_str)\n",
    "\n",
    "# Create list of word tokens\n",
    "token_list = []\n",
    "for token in my_doc:\n",
    "    token_list.append(token.text)\n",
    "# print(token_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1172f105",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import vocab\n",
    "from collections import Counter, OrderedDict\n",
    "counter = Counter(token_list)\n",
    "sorted_by_freq_tuples = sorted(counter.items(), key=lambda x: x[1], reverse=True)\n",
    "ordered_dict = OrderedDict(sorted_by_freq_tuples)\n",
    "v= vocab(ordered_dict)\n",
    "\n",
    "PAD_token = 0   # Used for padding short sentences\n",
    "SOS_token = 1   # Start-of-sentence token\n",
    "EOS_token = 2   # End-of-sentence token\n",
    "special_words={PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"}\n",
    "default_index = -1\n",
    "v.set_default_index(default_index)\n",
    "for key , value in special_words.items():\n",
    "    if value not in v: v.insert_token(value, key)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7f19ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(stopwords_path) as fh:\n",
    "            stopwords=list(set(fh.read().split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a252b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['sentenceA&B']=train_data[cols].apply(lambda row: ' '.join(row.values.astype(str)), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534639c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_field = Field(\n",
    "#     tokenize='basic_english', \n",
    "    lower=True,\n",
    "    include_lengths=True,\n",
    "    pad_token='PAD',\n",
    "    pad_first='SOS',\n",
    "    stop_words=stopwords,\n",
    ")\n",
    "\n",
    "# label_field = Field(sequential=False, use_vocab=False)\n",
    "preprocessed_text = train_data['sentenceA&B'].apply(lambda x: text_field.preprocess(x))\n",
    "text_field.build_vocab(\n",
    "    preprocessed_text, \n",
    "    vectors='fasttext.simple.300d')\n",
    "\n",
    "# get the vocab instance\n",
    "vocab = text_field.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50a07ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_text\n",
    "len(vocab.itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634009d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def data2tensors(self, data):\n",
    "#         \"\"\"\n",
    "#         Converts raw data sequences into vectorized sequences as tensors\n",
    "#         \"\"\"\n",
    "#         pass\n",
    "#         # TODO implement\n",
    "\n",
    "# def get_data_loader(self, batch_size=8):\n",
    "#         pass\n",
    "#         # TODO implement\n",
    "\n",
    "# def sort_batch(self, batch, targets, lengths):\n",
    "#         \"\"\"\n",
    "#         Sorts the data, lengths and target tensors based on the lengths\n",
    "#         of the sequences from longest to shortest in batch\n",
    "#         \"\"\"\n",
    "#         sents1_lengths, perm_idx = lengths.sort(0, descending=True)\n",
    "#         sequence_tensor = batch[perm_idx]\n",
    "#         target_tensor = targets[perm_idx]\n",
    "#         return sequence_tensor.transpose(0, 1), target_tensor, sents1_lengths\n",
    "\n",
    "# def vectorize_sequence(self, sentence):\n",
    "#         \"\"\"\n",
    "#         Replaces tokens with their indices in vocabulary\n",
    "#         \"\"\"\n",
    "#         pass\n",
    "#         # TODO implement\n",
    "\n",
    "# def pad_sequences(self, vectorized_sents, sents_lengths):\n",
    "#         \"\"\"\n",
    "#         Pads zeros at the end of each sequence in data tensor till max\n",
    "#         length of sequence in that batch\n",
    "#         \"\"\"\n",
    "#         pass\n",
    "#         # TODO implement\n",
    "# def encode(vocab,string):\n",
    "#        return vocab[string]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2f9949",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data['sentenceA&B'][0].apply(lambda x: print(x) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6da72d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_sequence(sentence):\n",
    "        \"\"\"\n",
    "        Replaces tokens with their indices in vocabulary\n",
    "        \"\"\"\n",
    "        splited_sentence=sentence.split()\n",
    "        encodes=[vocab[token] for token in splited_sentence]\n",
    "        return encodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60c0459",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = list(columns_mapping.values())\n",
    "cols.pop()\n",
    "train_data= pd.DataFrame(formatted_data['train'])\n",
    "val_data=pd.DataFrame(formatted_data['validation'])\n",
    "test_data=pd.DataFrame(formatted_data['test'])\n",
    "train_data['sentenceA&B']=train_data[cols].apply(lambda row: ' '.join(row.values.astype(str)), axis=1)\n",
    "val_data['sentenceA&B']=val_data[cols].apply(lambda row: ' '.join(row.values.astype(str)), axis=1)\n",
    "test_data['sentenceA&B']=test_data[cols].apply(lambda row: ' '.join(row.values.astype(str)), axis=1)\n",
    "\n",
    "codes=[]\n",
    "for sentence in val_data['sentenceA&B']:\n",
    "    encodes= vectorize_sequence(sentence)\n",
    "    codes.append(encodes)\n",
    "val_data['sentenceA&B'] = codes\n",
    "val_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90fe4221",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data2tensors(data):\n",
    "        \"\"\"\n",
    "        Converts raw data sequences into vectorized sequences as tensors\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        \n",
    "        data['sent1_tensor']=data['sent1_tensor'].apply(lambda lis : torch.as_tensor(lis))\n",
    "        data['sent2_tensor']=data['sent2_tensor'].apply(lambda lis : torch.as_tensor(lis))\n",
    "           \n",
    "        \n",
    "        \n",
    "        return data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334dd9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame(data2tensors(formatted_data)['train'])[\"sentenceA&B\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e617eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders = {x:DataLoader(formatted_data[x],32, shuffle=True, num_workers=4) for x in ['train','validation','test']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3637178",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb20f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import DataLoader\n",
    "\n",
    "# train_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)\n",
    "# test_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)\n",
    "\n",
    "# train_features, train_labels = next(iter(dataloaders['train']))\n",
    "# from torch.utils import data\n",
    "# train_tensor = data.TensorDataset(train_data, train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74915ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils import data\n",
    "# train_tensor = data.TensorDataset(train_data, train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5702913d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloaders = {'train': torch.utils.data.DataLoader(STSDataset(sts_train_df, batch_size=64)),\n",
    "#                'val': torch.utils.data.DataLoader(STSDataset(sts_dev_df, batch_size=64))}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b415d13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_target = torch.tensor(train['Target'].values.astype(np.float32))\n",
    "# train = torch.tensor(train.drop('Target', axis = 1).values.astype(np.float32)) \n",
    "# train_tensor = data_utils.TensorDataset(train, train_target) \n",
    "# train_loader = data_utils.DataLoader(dataset = train_tensor, batch_size = batch_size, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0612f9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_df= pd.DataFrame(formatted_data['train'])\n",
    "val_data_df=pd.DataFrame(formatted_data['validation'])\n",
    "test_data_df=pd.DataFrame(formatted_data['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a697f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_df['sent1_tensor']=train_data_df['sentence_A'].apply(lambda sen: vectorize_sequence(sen))\n",
    "train_data_df['sent2_tensor']=train_data_df['sentence_B'].apply(lambda sen: vectorize_sequence(sen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f33b74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data_df = torch.tensor(train_data_df['sentence_A'].values.astype(np.float32))\n",
    "train_data_df=data2tensors(train_data_df)\n",
    "train_data_df['sents1_length_tensor']=train_data_df['sent1_tensor'].apply(lambda tensor : torch.tensor(len(tensor)))\n",
    "train_data_df['sents2_length_tensor']=train_data_df['sent2_tensor'].apply(lambda tensor : torch.tensor(len(tensor)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da71456",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f851e252",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pad_sequences\n",
    "# from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "# padded=pad_sequences(train_data_df['sent1_tensor'],padding=\"post\",truncating=”post”,maxlen=8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5b046f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequences(sequences):\n",
    "        \"\"\"\n",
    "        :param sequences: list of tensors\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        num = len(sequences)\n",
    "        max_len = max([s.shape[0] for s in sequences])\n",
    "        out_dims = (num, max_len, *sequences[0].shape[1:])\n",
    "        out_tensor = sequences[0].data.new(*out_dims).fill_(0)\n",
    "        mask = sequences[0].data.new(*out_dims).fill_(0)\n",
    "        for i, tensor in enumerate(sequences):\n",
    "            length = tensor.size(0)\n",
    "            out_tensor[i, :length] = tensor\n",
    "            mask[i, :length] = 1\n",
    "        return list(out_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c09b71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_df['sent1_tensor']=pad_sequences(train_data_df['sent1_tensor'])\n",
    "train_data_df['sent2_tensor']=pad_sequences(train_data_df['sent2_tensor'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff76a379",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_df['relatedness_score']=train_data_df['relatedness_score'].apply(lambda score : torch.tensor(score/sum(train_data_df['relatedness_score'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01dd9d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763635b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Standard Pytorch Dataset class for loading datasets.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class STSDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        sent1_tensor,\n",
    "        sent2_tensor,\n",
    "        target_tensor,\n",
    "        sents1_length_tensor,\n",
    "        sents2_length_tensor,\n",
    "        raw_sents_1,\n",
    "        raw_sents_2,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        initializes  and populates the the length, data and target tensors, and raw texts list\n",
    "        \"\"\"\n",
    "        \n",
    "        assert (\n",
    "            \n",
    "            \n",
    "            len(sent1_tensor)\n",
    "            == torch.tensor(list(target_tensor)).size(0)\n",
    "            == len(sent2_tensor)\n",
    "            == torch.tensor(list(sents1_length_tensor)).size(0)\n",
    "            == torch.tensor(list(sents2_length_tensor)).size(0)\n",
    "        )\n",
    "        self.sent1_tensor = sent1_tensor\n",
    "        self.sent2_tensor = sent2_tensor\n",
    "        self.target_tensor = target_tensor\n",
    "        self.sents1_length_tensor = sents1_length_tensor\n",
    "        self.sents2_length_tensor = sents2_length_tensor\n",
    "        self.raw_sents_1 = raw_sents_1\n",
    "        self.raw_sents_2 = raw_sents_2\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        returns the tuple of data tensor, targets, lengths of sequences tensor and raw texts list\n",
    "        \"\"\"\n",
    "        return (\n",
    "            self.sent1_tensor[index],\n",
    "            self.sent2_tensor[index],\n",
    "            self.sents1_length_tensor[index],\n",
    "            self.sents2_length_tensor[index],\n",
    "            self.target_tensor[index],\n",
    "            self.raw_sents_1[index],\n",
    "            self.raw_sents_2[index],\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        returns the length of the data tensor.\n",
    "        \"\"\"\n",
    "        return torch.tensor(list(self.target_tensor)).size(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd72037",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_df                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243b82ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset=  STSDataset(train_data_df['sent1_tensor'],\n",
    "                         train_data_df['sent2_tensor'],\n",
    "                         train_data_df['relatedness_score'],\n",
    "                         train_data_df['sents1_length_tensor'],\n",
    "                         train_data_df['sents2_length_tensor'],\n",
    "                         train_data_df['sentence_A'],\n",
    "                         train_data_df['sentence_B']\n",
    "                         )\n",
    "\n",
    "train_loader = DataLoader(dataset = train_dataset, batch_size = 64, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2858ea5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(train_data_df['sent1_tensor'])\n",
    "# train_data_df['sents1_length_tensor'].size()\n",
    "# torch.tensor(list(target_tensor)).size(0)\n",
    "# train_data_df['sent1_tensor'][0]\n",
    "# train_data_df['relatedness_score'][0]\n",
    "# train_data_df['sents1_length_tensor'][0]\n",
    "# train_data_df['sentence_A'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c023118a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e699fbb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_df['relatedness_score'][0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94cb6e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448f4add",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(train_loader)\n",
    "len(dataiter.next())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01def1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_loader:\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6d99c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642c240c",
   "metadata": {},
   "outputs": [],
   "source": [
    "n, d, m = 3, 5, 7\n",
    "embedding = torch.nn.Embedding(n, d, max_norm=True)\n",
    "W = torch.randn((m, d), requires_grad=True)\n",
    "idx = torch.tensor([1, 2])\n",
    "# a = embedding.weight.clone() @ W.t()  # weight must be cloned for this to be differentiable\n",
    "# b = embedding(idx) @ W.t()  # modifies weight in-place\n",
    "# out = (a.unsqueeze(0) + b.unsqueeze(1))\n",
    "# loss = out.sigmoid().prod()\n",
    "# loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47872b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b642cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size= len(vocab)\n",
    "embedding_size= 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b5e9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = torch.nn.Embedding(vocab_size, embedding_size, max_norm=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7caadc40",
   "metadata": {},
   "outputs": [],
   "source": [
    "sen=torch.tensor([[121,  82,   5,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
    "        [ 36,   4, 404,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d74e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding(sen).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb1a9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab.vectors[sen]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181e478d",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_weights= vocab.vectors\n",
    "net = torch.nn.LSTM(10, 20, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef22ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_score(input1, input2):\n",
    "    # Get similarity predictions:\n",
    "    dif = input1.squeeze() - input2.squeeze()\n",
    "\n",
    "    norm = torch.norm(dif, p=1, dim=dif.dim() - 1)\n",
    "    y_hat = torch.exp(-norm)\n",
    "    y_hat = torch.clamp(y_hat, min=1e-7, max=1.0 - 1e-7)\n",
    "    return y_hat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b65dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sts_data import STSData\n",
    "\n",
    "columns_mapping = {\n",
    "        \"sent1\": \"sentence_A\",\n",
    "        \"sent2\": \"sentence_B\",\n",
    "        \"label\": \"relatedness_score\",\n",
    "    }\n",
    "dataset_name = \"sick\"\n",
    "sick_data = STSData(\n",
    "    dataset_name=dataset_name,\n",
    "    columns_mapping=columns_mapping,\n",
    "    normalize_labels=True,\n",
    "    normalization_const=5.0,\n",
    ")\n",
    "batch_size = 64\n",
    "sick_dataloaders = sick_data.get_data_loader(batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2448c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from utils import similarity_score\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Wrapper class using Pytorch nn.Module to create the architecture for our model\n",
    "Architecture is based on the paper: \n",
    "A STRUCTURED SELF-ATTENTIVE SENTENCE EMBEDDING\n",
    "https://arxiv.org/pdf/1703.03130.pdf\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class SiameseBiLSTMAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        batch_size,\n",
    "        output_size,\n",
    "        hidden_size,\n",
    "        vocab_size,\n",
    "        embedding_size,\n",
    "        embedding_weights,\n",
    "        lstm_layers,\n",
    "        device,\n",
    "        bidirectional,\n",
    "        self_attention_config,\n",
    "        fc_hidden_size,\n",
    "    ):\n",
    "        super(SiameseBiLSTMAttention, self).__init__()\n",
    "        \"\"\"\n",
    "        Initializes model layers and loads pre-trained embeddings from task 1\n",
    "        \"\"\"\n",
    "        ## model hyper parameters\n",
    "        self.batch_size = batch_size\n",
    "        self.output_size = output_size\n",
    "        self.lstm_hidden_size = hidden_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.lstm_layers = lstm_layers\n",
    "        self.device = device\n",
    "        self.bidirectional = bidirectional\n",
    "        self.fc_hidden_size = fc_hidden_size\n",
    "        self.lstm_directions = (\n",
    "            2 if self.bidirectional else 1\n",
    "        )  ## decide directions based on input flag\n",
    "        \n",
    "        \n",
    "        \n",
    "        ## model layers\n",
    "        # TODO initialize the look-up table.\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_size)\n",
    "        # TODO assign the look-up table to the pre-trained fasttext word embeddings.\n",
    "        \n",
    "        self.lookup= embedding_weights   #self.embeddings\n",
    "        \n",
    "        ## TODO initialize lstm layer\n",
    "        self.bi_lstm = torch.nn.LSTM(self.embedding_size, self.lstm_hidden_size, \n",
    "                            lstm_layers, batch_first=True , bias= True,bidirectional=True)\n",
    "        \n",
    "        ## TODO initialize self attention layers\n",
    "        self.SelfAtt= SelfAttention(2*self.lstm_hidden_size, self_attention_config['hidden_size'],\n",
    "                                    self_attention_config['output_size'])\n",
    "        \n",
    "        #Initialize fully connected layer\n",
    "        self.fc = nn.Linear(2*self.lstm_hidden_size * self_attention_config['output_size'], self.fc_hidden_size )\n",
    "        self.tanh = nn.Tanh()\n",
    "        ## incase we are using bi-directional lstm we'd have to take care of bi-directional outputs in\n",
    "        ## subsequent layers\n",
    "        \n",
    "        \n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        \"\"\"\n",
    "        Initializes hidden and context weight matrix before each\n",
    "                forward pass through LSTM\n",
    "        \"\"\"\n",
    "        h0 = torch.randn(self.lstm_directions*self.lstm_layers, batch_size, self.lstm_hidden_size)\n",
    "        c0 = torch.randn(self.lstm_directions*self.lstm_layers, batch_size, self.lstm_hidden_size)\n",
    "        \n",
    "        return h0, c0 \n",
    "\n",
    "    def forward_once(self, batch, lengths):\n",
    "        \"\"\"\n",
    "        Performs the forward pass for each batch\n",
    "        \"\"\"\n",
    "\n",
    "        ## batch shape: (batch_size, seq_len)\n",
    "        batch_size , sequence_len = batch.size()\n",
    "        ## embeddings shape: ( batch_size, seq_len, embedding_size)\n",
    "        \n",
    "        #h_init,c_init = self.init_hidden(batch_size)\n",
    "        input_batch_sequences= self.lookup[batch]\n",
    "        \n",
    "        output, (hn, cn) = self.bi_lstm(input_batch_sequences, (self.h_init, self.c_init))\n",
    "\n",
    "        return output , (hn , cn)\n",
    "\n",
    "    def forward(self, sent1_batch, sent2_batch, sent1_lengths, sent2_lengths):\n",
    "        \"\"\"\n",
    "        Performs the forward pass for each batch\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        ## init context and hidden weights for lstm cell\n",
    "        self.h_init,self.c_init = self.init_hidden(self.batch_size)\n",
    "        output1,_ = self.forward_once(sent1_batch,sent1_lengths)\n",
    "        self.h_init,self.c_init = self.init_hidden(self.batch_size)\n",
    "        output2,_ = self.forward_once(sent2_batch,sent2_lengths)\n",
    "        \n",
    "        ## Self attention Layer\n",
    "        attended_embeddings_sent1, attention_matrix_sent1 = self.SelfAtt.forward(output1)\n",
    "        attended_embeddings_sent2, attention_matrix_sent2 = self.SelfAtt.forward(output2)\n",
    "        \n",
    "        ## Fully connected layer \n",
    "        \n",
    "        final_embeddings_sent1= self.tanh(self.fc(attended_embeddings_sent1.reshape(output1.size(0),-1)))\n",
    "        final_embeddings_sent2= self.tanh(self.fc(attended_embeddings_sent2.reshape(output2.size(0),-1)))\n",
    "        \n",
    "        \n",
    "        #similarity score prediction\n",
    "        predictions = similarity_score(final_embeddings_sent1, final_embeddings_sent2)\n",
    "        \n",
    "        print(torch.cat((attention_matrix_sent1, attention_matrix_sent2), 1).size())\n",
    "        return predictions , \n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Implementation of the attention block\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        # TODO implement\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.ws1 = nn.Linear(input_size, hidden_size, bias=False)\n",
    "        self.ws2 = nn.Linear(hidden_size, output_size, bias=False)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.softmax = nn.Softmax()\n",
    "        \n",
    "        \n",
    "    ## the forward function would receive lstm's all hidden states as input\n",
    "    def forward(self, attention_input):\n",
    "        # TODO implement\n",
    "        #pass\n",
    "        \n",
    "        size = attention_input.size()\n",
    "        inp = attention_input.reshape(size[0]*size[1],size[2])\n",
    "        attention_matrix = self.softmax(self.ws2(self.tanh(self.ws1(inp))))\n",
    "        attention_matrix= attention_matrix.reshape(size[0], self.output_size, -1)\n",
    "        attended_embeddings_sent1= torch.bmm(attention_matrix , attention_input)\n",
    "        \n",
    "        return attended_embeddings_sent1, attention_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9639a56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "output_size = 1\n",
    "hidden_size = 128\n",
    "vocab_size = len(vocab)\n",
    "embedding_size = 300\n",
    "embedding_weights = vocab.vectors\n",
    "lstm_layers = 4\n",
    "learning_rate = 1e-1\n",
    "fc_hidden_size = 64\n",
    "max_epochs = 5\n",
    "bidirectional = True\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "## self attention config\n",
    "self_attention_config = {\n",
    "    \"hidden_size\": 150,  ## refers to variable 'da' in the ICLR paper\n",
    "    \"output_size\": 20,  ## refers to variable 'r' in the ICLR paper\n",
    "    \"penalty\": 0.0,  ## refers to penalty coefficient term in the ICLR paper\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cff170e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## init siamese lstm\n",
    "siamese_lstm_attention = SiameseBiLSTMAttention(\n",
    "    batch_size=batch_size,\n",
    "    output_size=output_size,\n",
    "    hidden_size=hidden_size,\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_size=embedding_size,\n",
    "    embedding_weights=embedding_weights,\n",
    "    lstm_layers=lstm_layers,\n",
    "    self_attention_config=self_attention_config,\n",
    "    fc_hidden_size=fc_hidden_size,\n",
    "    device=device,\n",
    "    bidirectional=bidirectional,\n",
    ")\n",
    "## move model to device\n",
    "optimizer = torch.optim.Adam(params=siamese_lstm_attention.parameters())\n",
    "siamese_lstm_attention.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe08b333",
   "metadata": {},
   "outputs": [],
   "source": [
    "siamese_lstm_attention.forward(batch[0],batch[1],7,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04e6d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "\"\"\"\n",
    "Script for training the neural network and saving the better models \n",
    "while monitoring a metric like accuracy etc\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def train_model(model, optimizer, dataloader, data, max_epochs, config_dict):\n",
    "    device = config_dict[\"device\"]\n",
    "    criterion = nn.MSELoss()\n",
    "    max_accuracy = 5e-1\n",
    "    train_loader , val_loader , test_loader = dataloader\n",
    "    dictionary_info={}\n",
    "    for epoch in tqdm(range(max_epochs)):\n",
    "        \n",
    "        try:\n",
    "            # Samples the batch\n",
    "            sent1_batch, sent2_batch, sent1_lengths, sent2_lengths,targets,raw_sent1,raw_sent2= next(iter(dataloader))\n",
    "\n",
    "        except StopIteration:\n",
    "            # restart the generator if the previous generator is exhausted.\n",
    "            train_generator = iter(train_loader)\n",
    "            sent1_batch, sent2_batch, sent1_lengths, sent2_lengths,targets,raw_sent1,raw_sent2= next(iter(dataloader))\n",
    "        \n",
    "        \n",
    "        predictions , attention_matrix = model.forward(sent1_batch, sent2_batch, sent1_lengths, sent2_lengths)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        try:\n",
    "             train_loss = criterion(predictions,targets) + attention_penalty_loss(attention_matrix, \n",
    "                                                                  config_dict['self_attention_config']['penalty'], device)\n",
    "                       \n",
    "        except RuntimeError:\n",
    "            \n",
    "            raise Exception(\"Model Loss gets nan values on regularization.Either remove regularization or add very small values\")\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        # TODO: computing accuracy using sklearn's function\n",
    "        ## acc = \n",
    "        #accuracy = (torch.argmax(predictions, axis=-1) == targets).float().mean()\n",
    "        \n",
    "        acc=accuracy_score(targets, predictions)\n",
    "        \n",
    "        ## compute model metrics on dev set\n",
    "        val_acc, val_loss = evaluate_dev_set(\n",
    "            model, data, criterion, dataloader, config_dict, device\n",
    "        )\n",
    "\n",
    "        \n",
    "        \n",
    "        if val_acc > max_accuracy:\n",
    "            max_accuracy = val_acc\n",
    "            logging.info(\n",
    "                \"new model saved\")  \n",
    "            \n",
    "            ## save the model if it is better than the prior best\n",
    "            torch.save(model.state_dict(), \"{}.pth\".format(config_dict[\"model_name\"]))\n",
    "\n",
    "        logging.info(\n",
    "            \"Train loss: {} - acc: {} -- Validation loss: {} - acc: {}\".format(\n",
    "                torch.mean(train_loss.data.float()), acc, val_loss, val_acc\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        if epoch % 100 == 0:\n",
    "              print('[%d/%d] loss: %.3f, accuracy: %.3f' %\n",
    "                   (i , max_epochs - 1, loss.item(), acc.item()))\n",
    "        if epoch == max_epochs - 1:\n",
    "               print('Final accuracy: %.3f, expected %.3f' %\n",
    "                         (accuracy.item(), 1.0))\n",
    "        \n",
    "    return model\n",
    "\n",
    "\n",
    "def evaluate_dev_set(model, data, criterion, data_loader, config_dict, device):\n",
    "    \"\"\"\n",
    "    Evaluates the model performance on dev data\n",
    "    \"\"\"\n",
    "    logging.info(\"Evaluating accuracy on dev set\")\n",
    "\n",
    "    # TODO implement\n",
    "    pass\n",
    "\n",
    "def attention_penalty_loss(annotation_weight_matrix, penalty_coef, device):\n",
    "    \"\"\"\n",
    "    This function computes the loss from annotation/attention matrix\n",
    "    to reduce redundancy in annotation matrix and for attention\n",
    "    to focus on different parts of the sequence corresponding to the\n",
    "    penalty term 'P' in the ICLR paper\n",
    "    ----------------------------------\n",
    "    'annotation_weight_matrix' refers to matrix 'A' in the ICLR paper\n",
    "    annotation_weight_matrix shape: (batch_size, attention_out, seq_len)\n",
    "    \"\"\"\n",
    "    \n",
    "    batch_size, attention_out_size = annotation_weight_matrix.size(0), annotation_weight_matrix.size(1)\n",
    "    annotation_weight_matrix_trans = torch.transpose(annotation_weight_matrix, 0, 1)\n",
    "    identity = torch.eye(annotation_weight_matrix.size(0))\n",
    "    annotation_mul_difference=annotation_weight_matrix@annotation_weight_matrix_trans - identity\n",
    "    penalty = frobenius_norm(annotation_mul_difference)\n",
    "    return penalty_coef*penalty\n",
    "\n",
    "\n",
    "def frobenius_norm(annotation_mul_difference):\n",
    "    \"\"\"\n",
    "    Computes the frobenius norm of the annotation_mul_difference input as matrix\n",
    "    \"\"\"\n",
    "   \n",
    "    \"\"\"\n",
    "    \n",
    " \n",
    "    Args:\n",
    "      annotation_mul_difference= ||AAT - I||\n",
    " \n",
    "    Returns:\n",
    "            regularized value\n",
    " \n",
    "       \n",
    "        \"\"\"\n",
    "    \n",
    "#    torch.norm(annotation_mul_difference.float(), p='fro')\n",
    "#     torch.sum(torch.sum(torch.sum(annotation_mul_difference**2,1),1)**0.5).type(torch.DoubleTensor)\n",
    "    return torch.sqrt(torch.sum(annotation_mul_difference**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3ce048",
   "metadata": {},
   "outputs": [],
   "source": [
    "siamese_lstm_attention = train_model(\n",
    "    model=siamese_lstm_attention,\n",
    "    optimizer=optimizer,\n",
    "    dataloader=train_loader,\n",
    "    data=sick_data,\n",
    "    max_epochs=max_epochs,\n",
    "    config_dict={\n",
    "        \"device\": device,\n",
    "        \"model_name\": \"siamese_lstm_attention\",\n",
    "        \"self_attention_config\": self_attention_config,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd8aeda",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset=  STSDataset(train_data_df['sent1_tensor'],\n",
    "                         train_data_df['sent2_tensor'],\n",
    "                         train_data_df['relatedness_score'],\n",
    "                         train_data_df['sents1_length_tensor'],\n",
    "                         train_data_df['sents2_length_tensor'],\n",
    "                         train_data_df['sentence_A'],\n",
    "                         train_data_df['sentence_B']\n",
    "                         )\n",
    "\n",
    "train_loader = DataLoader(dataset = train_dataset, batch_size = 64, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71f46f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main training loop\n",
    "# train_loader , val_loader , test_loader = sick_dataloaders\n",
    "train_generator = iter(train_loader)\n",
    "for i in range(100):\n",
    "    try:\n",
    "        # Samples the batch\n",
    "        sent1_batch, sent2_batch, sent1_lengths, sent2_lengths, targets,raw_sent1,raw_sent2= next(train_generator)\n",
    "        print(Variable(targets))\n",
    "        break\n",
    "    except StopIteration:\n",
    "        # restart the generator if the previous generator is exhausted.\n",
    "        train_generator = iter(train_loader)\n",
    "        sent1_batch, sent2_batch, sent1_lengths, sent2_lengths, targets,raw_sent1,raw_sent2= next(train_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b67d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.LongTensor([[1,2,4,5],[4,3,2,9]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314a39f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab.vectors[input]\n",
    "embedding = nn.Embedding(len(vocab), 300)\n",
    "embedding(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f91d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab.vectors.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6415540",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with torch.no_grad():\n",
    "a=torch.randn([64, 40, 14])\n",
    "at= np.transpose(a, (0, 2, 1)).clone().detach().requires_grad_(True)\n",
    "# frobenius_norm(a)\n",
    "diff=a@at-torch.eye(a.size(1))\n",
    "diff.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adac8040",
   "metadata": {},
   "outputs": [],
   "source": [
    "att=torch.randn([64, 40, 14])\n",
    "attT = att.transpose(1,2)\n",
    "identity = torch.eye(att.size(1))\n",
    "identity = Variable(identity.unsqueeze(0).expand(train_loader.batch_size,att.size(1),att.size(1)))\n",
    "diff=att@attT - identity\n",
    "diff.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a9b1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "identity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "87963ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1=torch.tensor([0.3724, 0.4367, 0.3584, 0.4131, 0.4151, 0.3754, 0.4444, 0.3806, 0.4540,\n",
    "        0.3293, 0.4363, 0.3715, 0.4190, 0.4442, 0.4099, 0.4260, 0.3768, 0.4290,\n",
    "        0.4401, 0.4283, 0.3629, 0.3899, 0.3863, 0.4223, 0.4615, 0.4136, 0.3793,\n",
    "        0.4060, 0.4051, 0.4540, 0.4548, 0.4677, 0.4346, 0.3802, 0.3646, 0.4234,\n",
    "        0.4135, 0.3769, 0.4404, 0.3910, 0.4340, 0.4046, 0.3871, 0.3655, 0.4430,\n",
    "        0.4434, 0.3824, 0.4189, 0.4324, 0.3946, 0.3496, 0.4714, 0.4147, 0.4114,\n",
    "        0.4263, 0.4088, 0.3955, 0.3722, 0.4222, 0.3962, 0.3961, 0.4276, 0.3328,\n",
    "        0.3668]) \n",
    "t2=torch.tensor([0.7000, 0.1750, 0.5750, 0.8250, 0.3250, 0.4750, 0.8500, 0.7750, 0.5750,\n",
    "        0.6000, 0.5750, 0.4500, 0.6500, 0.6750, 0.7750, 0.9250, 0.1000, 0.8750,\n",
    "        0.5250, 0.6500, 0.5750, 0.3000, 0.0000, 0.6750, 0.0250, 0.7750, 0.9500,\n",
    "        0.6250, 0.3250, 0.9500, 0.6250, 0.5750, 0.7250, 0.4000, 0.9000, 0.7250,\n",
    "        0.9750, 0.0000, 0.5500, 0.6000, 0.8250, 0.2500, 0.7000, 0.9750, 0.8750,\n",
    "        0.5750, 0.6750, 0.6500, 0.0750, 0.9000, 0.1500, 0.8750, 0.7250, 0.6500,\n",
    "        0.8250, 0.5750, 0.5000, 0.7750, 0.5250, 0.0250, 0.9000, 0.5750, 0.7500,\n",
    "        0.0500])\n",
    "# tensor(0.1044, dtype=torch.float64, grad_fn=<AddBackward0>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "cbea0269",
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy_score(list(t1.numpy()),list(t2.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "4a515f11",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2325236808.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_9532/2325236808.py\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    from sklearn.metrics import pairwise.cosine_similarity,DistanceMetric , r2_score , max_error, mean_absolute_error, explained_variance_score\u001b[0m\n\u001b[0m                                        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor as rfr\n",
    "from sklearn.metrics import pairwise.cosine_similarity,DistanceMetric , r2_score , max_error, mean_absolute_error, explained_variance_score\n",
    "r2_score(list(t2),list(t1))\n",
    "# DistanceMetric(list(t2),list(t1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "7479ec80",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'explained_variance_score' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_9532/834185789.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mexplained_variance_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'explained_variance_score' is not defined"
     ]
    }
   ],
   "source": [
    "explained_variance_score(t1,t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "e909aa79",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mean_absolute_error' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_9532/3913528676.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmean_absolute_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1.7976931348623157e+308\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'mean_absolute_error' is not defined"
     ]
    }
   ],
   "source": [
    "mean_absolute_error(t1,t2)\n",
    "-1.7976931348623157e+308"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "f740394f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=[0.3724 0.4367 0.3584 0.4131 0.4151 0.3754 0.4444 0.3806 0.454  0.3293\n 0.4363 0.3715 0.419  0.4442 0.4099 0.426  0.3768 0.429  0.4401 0.4283\n 0.3629 0.3899 0.3863 0.4223 0.4615 0.4136 0.3793 0.406  0.4051 0.454\n 0.4548 0.4677 0.4346 0.3802 0.3646 0.4234 0.4135 0.3769 0.4404 0.391\n 0.434  0.4046 0.3871 0.3655 0.443  0.4434 0.3824 0.4189 0.4324 0.3946\n 0.3496 0.4714 0.4147 0.4114 0.4263 0.4088 0.3955 0.3722 0.4222 0.3962\n 0.3961 0.4276 0.3328 0.3668].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_9532/3108703608.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpairwise\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcosine_similarity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcosine_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdense_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/sklearn/metrics/pairwise.py\u001b[0m in \u001b[0;36mcosine_similarity\u001b[0;34m(X, Y, dense_output)\u001b[0m\n\u001b[1;32m   1249\u001b[0m     \u001b[0;31m# to avoid recursive import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1251\u001b[0;31m     \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_pairwise_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1253\u001b[0m     \u001b[0mX_normalized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/sklearn/metrics/pairwise.py\u001b[0m in \u001b[0;36mcheck_pairwise_arrays\u001b[0;34m(X, Y, precomputed, dtype, accept_sparse, force_all_finite, copy)\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m             \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_all_finite\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m         )\n\u001b[1;32m    164\u001b[0m         Y = check_array(\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[1;32m    771\u001b[0m                     \u001b[0;34m\"Reshape your data either using array.reshape(-1, 1) if \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    772\u001b[0m                     \u001b[0;34m\"your data has a single feature or array.reshape(1, -1) \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 773\u001b[0;31m                     \u001b[0;34m\"if it contains a single sample.\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    774\u001b[0m                 )\n\u001b[1;32m    775\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=[0.3724 0.4367 0.3584 0.4131 0.4151 0.3754 0.4444 0.3806 0.454  0.3293\n 0.4363 0.3715 0.419  0.4442 0.4099 0.426  0.3768 0.429  0.4401 0.4283\n 0.3629 0.3899 0.3863 0.4223 0.4615 0.4136 0.3793 0.406  0.4051 0.454\n 0.4548 0.4677 0.4346 0.3802 0.3646 0.4234 0.4135 0.3769 0.4404 0.391\n 0.434  0.4046 0.3871 0.3655 0.443  0.4434 0.3824 0.4189 0.4324 0.3946\n 0.3496 0.4714 0.4147 0.4114 0.4263 0.4088 0.3955 0.3722 0.4222 0.3962\n 0.3961 0.4276 0.3328 0.3668].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "cosine_similarity(t1, t2, dense_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "ace2b51c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_9532/3354266869.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_sampler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'train_loader' is not defined"
     ]
    }
   ],
   "source": [
    "train_loader.batch_sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "5fe9aed7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_9532/4006441096.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'train_loader' is not defined"
     ]
    }
   ],
   "source": [
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "5793bfb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4480"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "70*64\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "5262269a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x400 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "x = np.linspace(0, 50, 10)\n",
    "y = x ** 2\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(8,4), dpi=100)\n",
    "\n",
    "# plot subplot 1\n",
    "axes[0].plot(x, x**2, color=\"green\", label=\"y = x**2\")\n",
    "axes[0].plot(x, x**3, color=\"red\", label=\"y = x**3\")\n",
    "axes[0].legend(loc=2); # upper left corner\n",
    "axes[0].set_xlabel('x')\n",
    "axes[0].set_ylabel('y')\n",
    "axes[0].set_title('Plot of y=x^2 and y=x^3')\n",
    "fig.savefig(\"plots.pdf\")\n",
    "# plot subplot 2\n",
    "axes[1].plot(x, x**2, color=\"violet\", label=\"y = x**2\")\n",
    "axes[1].plot(x, x**3, color=\"blue\", label=\"y = x**3\")\n",
    "axes[1].legend(loc=2); # upper left corner\n",
    "axes[1].set_xlabel('x')\n",
    "axes[1].set_ylabel('y')\n",
    "axes[1].set_title('Plot of y=x^2 and y=x^3')\n",
    "\n",
    "# `fig.tight_layout()` automatically adjusts the positions of the axes on the figure canvas so that there is no overlapping content\n",
    "# comment this out to see the difference\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "80f93c28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'whiskers': [<matplotlib.lines.Line2D at 0x7fe0e7a06b90>,\n",
       "  <matplotlib.lines.Line2D at 0x7fe0e77b7d90>],\n",
       " 'caps': [<matplotlib.lines.Line2D at 0x7fe0e77bf210>,\n",
       "  <matplotlib.lines.Line2D at 0x7fe0e77bf650>],\n",
       " 'boxes': [<matplotlib.lines.Line2D at 0x7fe0e77b76d0>],\n",
       " 'medians': [<matplotlib.lines.Line2D at 0x7fe0e77bfad0>],\n",
       " 'fliers': [<matplotlib.lines.Line2D at 0x7fe0e77bff10>],\n",
       " 'means': []}"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAD4CAYAAAD1jb0+AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8rg+JYAAAACXBIWXMAAAsTAAALEwEAmpwYAAAKcklEQVR4nO3dX4hmh1nH8d9jtqGixm7MuIRs4wYaKrlpCkOs1BsTK/EPJhcltIjsxcLeKFQUNHrTFrxob6xeeLOY0L3QNqFaEkpRQ0wpQomd2KpJoySGBhOS7NRsaL1RUh8v9o0uu7OZd3bmncmT/XxgeM857zl7nqvvHs68Z97q7gAwzw8c9AAAXB4BBxhKwAGGEnCAoQQcYKhD+3my6667ro8dO7afpwQY74knnvhOd69duH1fA37s2LFsbGzs5ykBxquq57fa7hYKwFACDjCUgAMMJeAAQwk4wFBLfQqlqr6d5HtJvp/k9e5er6prkzyQ5FiSbye5p7vPrmZMAC60kyvwn+3uW7t7fbF+b5JHu/vmJI8u1gHYJ7u5hXJXktOL5dNJ7t71NAAsbdmAd5K/qaonqurkYtuR7n5psfxykiNbHVhVJ6tqo6o2Njc3dzkuLKeq9uUHDtKyT2L+THe/WFU/nuSRqvqX89/s7q6qLb8ZortPJTmVJOvr6749gn2x0y8qqaodHwMHbakr8O5+cfF6JskXk9yW5JWquj5JFq9nVjUkABfbNuBV9UNV9SNvLCf5+SRPJnk4yfHFbseTPLSqIQG42DK3UI4k+eLift+hJH/e3X9VVV9P8mBVnUjyfJJ7VjcmABfaNuDd/VyS922x/T+S3LGKoQDYnicxAYYScIChBBxgKAEHGErAAYYScIChBBxgKAEHGErAAYYScIChBBxgKAEHGErAAYYScIChBBxgKAEHGErAAYYScIChBBxgKAEHGErAAYYScIChBBxgKAEHGErAAYYScIChBBxgKAEHGErAAYYScIChBBxgqKUDXlVXVdU3qupLi/Wbqurxqnq2qh6oqqtXNyYAF9rJFfjHkjx93vqnk3ymu9+T5GySE3s5GABvbqmAV9XRJL+U5E8X65Xk9iRfWOxyOsndK5gPgEtY9gr8j5L8TpL/Waz/WJLXuvv1xfoLSW7Y6sCqOllVG1W1sbm5uZtZATjPtgGvql9Ocqa7n7icE3T3qe5e7+71tbW1y/knANjCoSX2+WCSX6mqX0zyziTXJPnjJO+qqkOLq/CjSV5c3ZgAXGjbK/Du/r3uPtrdx5J8JMnfdvevJnksyYcXux1P8tDKpgTgIrv5HPjvJvmtqno25+6J37c3IwGwjGVuofyf7v5Kkq8slp9LctvejwTAMjyJCTCUgAMMJeAAQwk4wFA7+iUmHIRrr702Z8+eXfl5zv2FiNU5fPhwXn311ZWegyuLgPOWd/bs2XT3QY+xa6v+D4Irj1soAEMJOMBQAg4wlIADDCXgAEMJOMBQAg4wlIADDCXgAEMJOMBQAg4wlIADDCXgAEMJOMBQAg4wlIADDCXgAEMJOMBQAg4wlIADDCXgAEMJOMBQAg4wlIADDLVtwKvqnVX191X1j1X1VFV9crH9pqp6vKqeraoHqurq1Y8LwBuWuQL/ryS3d/f7ktya5M6q+kCSTyf5THe/J8nZJCdWNiUAF9k24H3Ofy5W37H46SS3J/nCYvvpJHevYkAAtrbUPfCquqqqvpnkTJJHkvxbkte6+/XFLi8kueESx56sqo2q2tjc3NyDkQFIlgx4d3+/u29NcjTJbUl+ctkTdPep7l7v7vW1tbXLmxKAi+zoUyjd/VqSx5L8dJJ3VdWhxVtHk7y4t6MB8GaW+RTKWlW9a7H8g0k+lOTpnAv5hxe7HU/y0IpmBGALh7bfJdcnOV1VV+Vc8B/s7i9V1beSfL6q/iDJN5Lct8I5AbjAtgHv7n9K8v4ttj+Xc/fDATgAnsQEGErAAYYScIChBBxgKAEHGErAAYYScIChBBxgKAEHGErAAYYScIChBBxgKAEHGErAAYZa5u+Bw4Hqj1+TfOJHD3qMXeuPX3PQI/A2I+C85dUnv5vuPugxdq2q0p846Cl4O3ELBWAoAQcYSsABhhJwgKEEHGAoAQcYSsABhhJwgKEEHGAoAQcYSsABhhJwgKEEHGAoAQcYSsABhto24FX17qp6rKq+VVVPVdXHFtuvrapHquqZxevh1Y8LwBuWuQJ/Pclvd/ctST6Q5Ner6pYk9yZ5tLtvTvLoYh2AfbJtwLv7pe7+h8Xy95I8neSGJHclOb3Y7XSSu1c0IwBb2NE98Ko6luT9SR5PcqS7X1q89XKSI5c45mRVbVTVxubm5m5mBeA8Swe8qn44yV8k+c3u/u757/W5Lyzc8ksLu/tUd6939/ra2tquhgXg/y0V8Kp6R87F+8+6+y8Xm1+pqusX71+f5MxqRgRgK8t8CqWS3Jfk6e7+w/PeejjJ8cXy8SQP7f14AFzKoSX2+WCSX0vyz1X1zcW230/yqSQPVtWJJM8nuWclEwKwpW0D3t1/l6Qu8fYdezsOAMvyJCbAUAIOMJSAAwwl4ABDCTjAUAIOMJSAAwwl4ABDCTjAUAIOMJSAAwwl4ABDCTjAUAIOMJSAAwwl4ABDCTjAUAIOMJSAAwwl4ABDCTjAUAIOMJSAAwx16KAHgGVU1UGPsGuHDx8+6BF4mxFw3vK6e+XnqKp9OQ/sJbdQAIYScIChBBxgKAEHGErAAYbaNuBVdX9VnamqJ8/bdm1VPVJVzyxefT4KYJ8tcwX+2SR3XrDt3iSPdvfNSR5drAOwj7YNeHd/NcmrF2y+K8npxfLpJHfv7VgAbOdy74Ef6e6XFssvJzlyqR2r6mRVbVTVxubm5mWeDoAL7fqXmH3u8bVLPsLW3ae6e72719fW1nZ7OgAWLjfgr1TV9UmyeD2zdyMBsIzLDfjDSY4vlo8neWhvxgFgWct8jPBzSb6W5L1V9UJVnUjyqSQfqqpnkvzcYh2AfbTtXyPs7o9e4q079ngWAHbAk5gAQwk4wFACDjCUgAMMJeAAQwk4wFACDjCUgAMMJeAAQwk4wFACDjCUgAMMJeAAQwk4wFACDjCUgAMMJeAAQwk4wFACDjCUgAMMJeAAQwk4wFACDjCUgAMMJeAAQwk4wFACDjCUgAMMJeAAQwk4wFACDjDUrgJeVXdW1b9W1bNVde9eDQXA9i474FV1VZI/SfILSW5J8tGqumWvBgPgze3mCvy2JM9293Pd/d9JPp/krr0ZC4DtHNrFsTck+ffz1l9I8lMX7lRVJ5OcTJIbb7xxF6eD5VXVvhzT3Ts+BvbKyn+J2d2nunu9u9fX1tZWfTpIci6s+/EDB2k3AX8xybvPWz+62AbAPthNwL+e5Oaquqmqrk7ykSQP781YAGznsu+Bd/frVfUbSf46yVVJ7u/up/ZsMgDe1G5+iZnu/nKSL+/RLADsgCcxAYYScIChBBxgKAEHGKr282GEqtpM8vy+nRCWd12S7xz0EHAJP9HdFz0Jua8Bh7eqqtro7vWDngN2wi0UgKEEHGAoAYdzTh30ALBT7oEDDOUKHGAoAQcYSsC5olXV/VV1pqqePOhZYKcEnCvdZ5PcedBDwOUQcK5o3f3VJK8e9BxwOQQcYCgBBxhKwAGGEnCAoQScK1pVfS7J15K8t6peqKoTBz0TLMuj9ABDuQIHGErAAYYScIChBBxgKAEHGErAAYYScICh/heB/1fZPhzRKwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.boxplot(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "0ee93c5e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'batch_train_num' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_9532/116516476.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_train_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                 \u001b[0;31m# Samples the batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                 \u001b[0msent1_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent2_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent1_lengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent2_lengths\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mraw_sent1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mraw_sent2\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'batch_train_num' is not defined"
     ]
    }
   ],
   "source": [
    "for batch in range(batch_train_num):\n",
    "            \n",
    "            try:\n",
    "                # Samples the batch\n",
    "                sent1_batch, sent2_batch, sent1_lengths, sent2_lengths,targets,raw_sent1,raw_sent2= next(train_generator)\n",
    "\n",
    "            except StopIteration:\n",
    "                # restart the generator if the previous generator is exhausted.\n",
    "                train_generator = iter(train_loader)\n",
    "                sent1_batch, sent2_batch, sent1_lengths, sent2_lengths,targets,raw_sent1,raw_sent2= next(train_generator)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "a70051a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function datasets.load.load_dataset(path: str, name: Union[str, NoneType] = None, data_dir: Union[str, NoneType] = None, data_files: Union[Dict, List] = None, split: Union[str, datasets.splits.Split, NoneType] = None, cache_dir: Union[str, NoneType] = None, features: Union[datasets.features.Features, NoneType] = None, download_config: Union[datasets.utils.file_utils.DownloadConfig, NoneType] = None, download_mode: Union[datasets.utils.download_manager.GenerateMode, NoneType] = None, ignore_verifications: bool = False, keep_in_memory: Union[bool, NoneType] = None, save_infos: bool = False, script_version: Union[str, datasets.utils.version.Version, NoneType] = None, use_auth_token: Union[str, bool, NoneType] = None, task: Union[str, datasets.tasks.base.TaskTemplate, NoneType] = None, streaming: bool = False, **config_kwargs) -> Union[datasets.dataset_dict.DatasetDict, datasets.arrow_dataset.Dataset, datasets.dataset_dict.IterableDatasetDict, datasets.iterable_dataset.IterableDataset]>"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "fe295e9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now =23:52:03.358994\n",
      "type(now) = <class 'str'>\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "now = datetime.now().time() # time object\n",
    "\n",
    "print(\"now =\"+ str(now))\n",
    "print(\"type(now) =\", type(str(now)))\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "9e682770",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import spatial\n",
    "def cosine_score(input1, input2):\n",
    "    # Get similarity predictions:\n",
    "    inp1=input1.detach().numpy()\n",
    "    inp2=input2.detach().numpy()\n",
    "    relatedness_list=[]\n",
    "    for sen in range(len(input1)):        \n",
    "        relatedness = 1 - spatial.distance.cosine(inp1[sen],inp2[sen])\n",
    "        relatedness_list.append(relatedness)\n",
    "    return relatedness_list\n",
    "\n",
    "a=[[29.032497240149457, 27.402251754981883, 26.907307942708332, 26.211151565330617], \n",
    "   [26.448624320652176, 25.580964461616848, 22.726847330729168, 22.599697831748188]]\n",
    "   \n",
    "b=[[26.448624320652176, 25.580964461616848, 22.726847330729168, 22.599697831748188],\n",
    "  [29.032497240149457, 27.402251754981883, 26.907307942708332, 26.211151565330617]] \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "49223181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[29.032497 27.402252 26.907309 26.211151] [26.448624 25.580965 22.726847 22.599697]\n",
      "[26.448624 25.580965 22.726847 22.599697] [29.032497 27.402252 26.907309 26.211151]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.9991987347602844, 0.9991987347602844]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=torch.tensor(a)\n",
    "b=torch.tensor(b)\n",
    "cosine_score(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "d4193373",
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_score(input1, input2):\n",
    "    # Get similarity predictions:\n",
    "    dif = input1.squeeze() - input2.squeeze()\n",
    "\n",
    "    norm = torch.norm(dif, p=1, dim=dif.dim() - 1)\n",
    "    y_hat = torch.exp(-norm)\n",
    "    y_hat = torch.clamp(y_hat, min=1e-7, max=1.0 - 1e-7)\n",
    "    return y_hat\n",
    "#Cosine Distance\n",
    "def cosine_dist(input1, input2):\n",
    "    # Get similarity predictions:\n",
    "    inp1=input1.detach().numpy()\n",
    "    inp2=input2.detach().numpy()\n",
    "    relatedness_list=[]\n",
    "    for sen in range(len(input1)):        \n",
    "        relatedness = 1 - spatial.distance.cosine(inp1[sen],inp2[sen])\n",
    "        relatedness_list.append(relatedness)\n",
    "    return relatedness_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "c63f0add",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "similarity_score() missing 1 required positional argument: 'input2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_9532/2253679071.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msimilarity_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: similarity_score() missing 1 required positional argument: 'input2'"
     ]
    }
   ],
   "source": [
    "similarity_score(a,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "630586f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrue=[0.5, 0.8250000476837158, 0.8250000476837158, 0.44999998807907104, 0.9500000476837158, 0.15000000596046448, 0.7250000238418579, 0.875, 0.574999988079071, 0.8500000238418579, 0.375, 0.17500001192092896, 0.75, 0.625, 0.8999999761581421, 0.42500001192092896, 1.0, 0.9249999523162842, 0.9500000476837158, 0.675000011920929, 0.675000011920929, 0.875, 0.5537499785423279, 0.42500001192092896, 0.550000011920929, 0.574999988079071, 0.75, 0.5, 0.699999988079071, 0.550000011920929, 0.8250000476837158, 0.050000011920928955, 0.30000001192092896, 0.550000011920929, 0.9500000476837158, 0.9750000238418579, 0.550000011920929, 0.5, 0.7250000238418579, 0.550000011920929, 0.8500000238418579, 0.625, 0.8500000238418579, 0.8999999761581421, 0.8999999761581421, 0.574999988079071, 0.0, 0.699999988079071, 0.42500001192092896, 0.574999988079071, 0.6000000238418579, 0.8250000476837158, 0.8250000476837158, 0.5249999761581421, 1.0, 0.7749999761581421, 0.9249999523162842, 0.5249999761581421, 1.0, 0.7250000238418579, 0.9249999523162842, 1.0, 0.6000000238418579, 0.625, 0.0, 0.8250000476837158, 0.699999988079071, 0.3837500214576721, 0.0, 0.4750000238418579, 0.0, 0.625, 0.3999999761581421, 0.42500001192092896, 0.7712500095367432, 0.9750000238418579, 0.550000011920929, 0.19999998807907104, 0.9750000238418579, 0.8999999761581421, 0.9750000238418579, 0.5, 0.9249999523162842, 0.6000000238418579, 0.5249999761581421, 0.025000005960464478, 0.4750000238418579, 0.699999988079071, 1.0, 0.6000000238418579, 0.5, 0.875, 0.8999999761581421, 0.44999998807907104, 0.8500000238418579, 1.0, 0.7749999761581421, 0.7250000238418579, 0.875, 0.7999999523162842, 0.125, 0.699999988079071, 0.5, 0.7250000238418579, 0.699999988079071, 0.32499998807907104, 0.0, 0.4750000238418579, 0.7250000238418579, 0.6000000238418579, 0.625, 0.9249999523162842, 0.0, 0.7999999523162842, 0.42500001192092896, 0.07499998807907104, 0.6000000238418579, 0.9750000238418579, 0.625, 0.625, 0.0, 0.75, 0.32499998807907104, 0.7250000238418579, 0.4750000238418579, 0.7749999761581421, 0.6000000238418579, 0.5, 0.8500000238418579, 0.550000011920929, 0.75, 0.025000005960464478, 0.699999988079071, 0.8500000238418579, 0.675000011920929, 0.7250000238418579, 0.09999999403953552, 0.699999988079071, 0.7749999761581421, 0.15000000596046448, 0.574999988079071, 0.75, 0.8250000476837158, 0.025000005960464478, 0.8999999761581421, 0.5, 0.9500000476837158, 0.675000011920929, 0.6000000238418579, 0.42500001192092896, 0.9750000238418579, 0.07499998807907104, 0.7999999523162842, 0.7749999761581421, 0.42500001192092896, 0.550000011920929, 0.875, 0.699999988079071, 0.7999999523162842, 0.6000000238418579, 0.675000011920929, 0.9750000238418579, 0.8250000476837158, 0.8250000476837158, 0.8500000238418579, 0.7250000238418579, 0.6499999761581421, 0.44999998807907104, 0.675000011920929, 0.09999999403953552, 0.0, 0.699999988079071, 0.574999988079071, 0.875, 0.025000005960464478, 0.675000011920929, 0.699999988079071, 0.875, 0.699999988079071, 0.5, 0.6000000238418579, 0.9249999523162842, 0.550000011920929, 0.574999988079071, 0.675000011920929, 0.574999988079071, 0.7250000238418579, 0.8250000476837158, 0.6499999761581421, 0.9249999523162842, 0.550000011920929, 0.699999988079071, 0.9750000238418579, 0.22499999403953552, 0.8500000238418579, 0.875, 0.9249999523162842, 0.32499998807907104, 0.625, 1.0, 0.5, 0.025000005960464478, 0.375, 0.6000000238418579, 0.75, 0.6000000238418579, 0.550000011920929, 0.5, 0.625, 0.675000011920929, 0.0, 0.7999999523162842, 1.0, 0.875, 0.9750000238418579, 0.7999999523162842, 0.7999999523162842, 0.32499998807907104, 0.3999999761581421, 0.9249999523162842, 0.625, 0.550000011920929, 0.4750000238418579, 0.5, 0.8999999761581421, 0.5249999761581421, 0.050000011920928955, 0.699999988079071, 0.8999999761581421, 0.44999998807907104, 1.0, 0.6000000238418579, 0.5249999761581421, 0.8500000238418579, 0.75, 0.7250000238418579, 0.6000000238418579, 0.5249999761581421, 0.625, 0.050000011920928955, 0.8999999761581421, 0.5662500262260437, 0.7250000238418579, 0.0, 0.8999999761581421, 0.9500000476837158, 0.5, 0.625, 0.6499999761581421, 0.5249999761581421, 0.574999988079071, 0.9750000238418579, 0.550000011920929, 0.9750000238418579, 0.9750000238418579, 0.5, 0.9500000476837158, 1.0, 0.6499999761581421, 0.8500000238418579, 0.8250000476837158, 0.5, 0.6000000238418579, 0.699999988079071, 0.9500000476837158, 0.9500000476837158, 0.875, 0.7250000238418579, 0.44999998807907104, 0.5, 0.7749999761581421, 0.574999988079071, 0.5249999761581421, 0.19999998807907104, 0.675000011920929, 0.3999999761581421, 0.375, 0.8500000238418579, 0.8250000476837158, 0.8250000476837158, 0.25, 0.375, 0.75, 0.8500000238418579, 0.8250000476837158, 0.6000000238418579, 0.9500000476837158, 1.0, 0.9750000238418579, 0.6000000238418579, 0.6499999761581421, 0.8250000476837158, 0.44999998807907104, 0.09999999403953552, 0.8250000476837158, 0.0, 0.44999998807907104, 0.7250000238418579, 0.2787500023841858, 0.7749999761581421, 0.625, 0.574999988079071, 0.7999999523162842, 0.3500000238418579, 0.875, 0.6000000238418579, 0.75, 0.550000011920929, 0.637499988079071, 0.07499998807907104, 0.9750000238418579, 1.0, 0.574999988079071, 0.675000011920929, 0.875, 0.75, 0.9750000238418579, 0.875, 0.75, 0.6499999761581421, 0.6499999761581421, 0.6000000238418579, 0.699999988079071, 0.3500000238418579, 0.15000000596046448, 0.8500000238418579, 0.550000011920929, 0.7250000238418579, 0.8500000238418579, 0.625, 0.574999988079071, 0.6499999761581421, 0.675000011920929, 0.0, 0.6499999761581421, 0.574999988079071, 0.9750000238418579, 0.3999999761581421, 0.7999999523162842, 1.0, 0.875, 0.550000011920929, 0.9249999523162842, 0.8500000238418579, 0.699999988079071, 0.9249999523162842, 0.9500000476837158, 0.3999999761581421, 0.675000011920929, 0.625, 0.9750000238418579, 0.7250000238418579, 0.8999999761581421, 0.699999988079071, 0.5, 1.0, 0.2749999761581421, 0.8250000476837158, 0.6000000238418579, 0.699999988079071, 0.7712500095367432, 0.550000011920929, 0.6499999761581421, 0.4750000238418579, 0.8250000476837158, 0.675000011920929, 0.7999999523162842, 0.699999988079071, 0.5249999761581421, 0.625, 0.7749999761581421, 0.699999988079071, 0.25, 0.7999999523162842, 0.2749999761581421, 0.15000000596046448, 0.42500001192092896, 0.025000005960464478, 0.550000011920929, 0.3500000238418579, 0.3999999761581421, 0.4750000238418579, 0.6000000238418579, 0.2749999761581421, 0.8500000238418579, 0.8250000476837158, 0.75, 0.7749999761581421, 0.0, 0.75, 0.25, 0.625, 0.7037500143051147, 0.0, 0.4750000238418579, 0.0, 0.75, 0.44999998807907104, 0.8250000476837158, 0.8250000476837158, 0.7999999523162842, 0.75, 0.574999988079071, 0.75, 0.625, 0.09999999403953552, 0.7250000238418579, 0.3500000238418579, 0.675000011920929, 0.75, 0.699999988079071, 0.17500001192092896, 0.25, 0.0, 0.675000011920929, 0.675000011920929, 0.3500000238418579, 0.6499999761581421, 0.8999999761581421, 0.9500000476837158, 0.9500000476837158, 0.17500001192092896, 0.675000011920929, 0.9750000238418579, 0.699999988079071, 0.7250000238418579, 0.7749999761581421, 0.574999988079071, 0.125, 0.9249999523162842, 0.6499999761581421, 0.050000011920928955, 0.625, 0.8250000476837158, 0.32499998807907104, 0.09999999403953552, 0.7749999761581421, 0.9750000238418579, 0.9750000238418579, 0.574999988079071, 1.0, 0.4750000238418579, 0.625, 0.574999988079071, 0.574999988079071, 0.9249999523162842, 0.6499999761581421, 0.574999988079071, 0.025000005960464478, 0.9500000476837158, 0.7999999523162842, 0.7250000238418579, 0.19999998807907104, 0.625, 0.8250000476837158, 0.875, 1.0, 0.9750000238418579, 0.9750000238418579, 0.699999988079071, 0.8250000476837158, 0.550000011920929, 0.699999988079071, 0.7999999523162842, 0.7999999523162842, 0.675000011920929, 0.5, 0.675000011920929, 0.9750000238418579, 0.5249999761581421, 0.625, 0.7749999761581421, 0.9500000476837158, 0.5, 0.5249999761581421, 0.675000011920929, 0.7999999523162842, 0.6499999761581421, 0.574999988079071, 0.8999999761581421, 0.6000000238418579, 0.32499998807907104, 0.625, 0.75, 0.45875000953674316, 0.699999988079071, 0.846250057220459, 0.625, 0.9249999523162842, 0.75, 0.550000011920929, 0.7749999761581421, 0.8250000476837158, 0.8250000476837158, 0.625, 0.125, 0.75, 0.875, 0.050000011920928955, 0.6000000238418579, 0.44999998807907104, 0.75, 0.07499998807907104, 0.6499999761581421, 0.4750000238418579, 0.9750000238418579, 0.6000000238418579, 0.5, 0.3500000238418579, 0.025000005960464478, 0.22499999403953552, 0.625, 0.875, 0.4750000238418579, 0.6000000238418579, 1.0, 0.7999999523162842, 0.6000000238418579, 0.699999988079071, 0.3500000238418579, 0.550000011920929, 0.8999999761581421, 0.5912500023841858, 0.8999999761581421, 0.574999988079071, 0.625, 0.5, 0.15000000596046448, 0.574999988079071, 0.675000011920929, 0.6000000238418579, 1.0, 0.3087499737739563, 0.7999999523162842, 0.42500001192092896, 0.375, 1.0, 0.6037499904632568, 0.5249999761581421, 0.125, 0.9750000238418579, 0.574999988079071, 0.17500001192092896, 0.7999999523162842, 0.8999999761581421, 0.9500000476837158, 0.5249999761581421, 0.6499999761581421, 0.6000000238418579, 0.6000000238418579, 0.625, 0.8999999761581421, 0.9750000238418579, 0.7999999523162842, 0.7999999523162842, 0.25, 0.8999999761581421, 0.550000011920929, 0.7999999523162842, 0.09999999403953552, 0.625, 0.9750000238418579, 0.9750000238418579, 0.9537500143051147, 0.9249999523162842, 0.8500000238418579, 0.7250000238418579, 0.32499998807907104, 0.574999988079071, 0.7250000238418579, 0.8250000476837158, 0.75, 0.699999988079071, 0.7250000238418579, 0.675000011920929, 1.0, 0.6000000238418579, 1.0, 0.9750000238418579, 0.125, 0.675000011920929, 0.574999988079071, 0.574999988079071, 0.574999988079071, 0.625, 0.4750000238418579, 0.699999988079071, 0.550000011920929, 0.675000011920929, 0.9500000476837158, 0.699999988079071, 0.9500000476837158, 0.125, 0.0, 0.375, 0.7250000238418579, 0.9500000476837158, 0.699999988079071, 0.07499998807907104, 0.5249999761581421, 0.675000011920929, 0.0, 0.3999999761581421, 0.3999999761581421, 0.44999998807907104, 1.0, 0.6499999761581421, 0.5, 0.675000011920929, 0.4750000238418579, 0.5, 0.574999988079071, 0.875, 0.550000011920929, 0.7250000238418579, 0.375, 0.8500000238418579, 0.9500000476837158, 0.0, 0.699999988079071, 0.4750000238418579, 0.375, 0.3999999761581421, 0.7250000238418579, 0.5249999761581421, 0.675000011920929, 0.0, 0.8500000238418579, 0.6000000238418579, 0.550000011920929, 0.5, 0.3500000238418579, 0.6499999761581421, 0.550000011920929, 0.4750000238418579, 0.6000000238418579, 0.75, 0.44999998807907104, 0.574999988079071, 0.9750000238418579, 0.6499999761581421, 0.7999999523162842, 0.3500000238418579, 0.0, 0.7999999523162842, 0.75, 0.8500000238418579, 0.9750000238418579, 0.3500000238418579, 0.9750000238418579, 0.125, 0.875, 0.675000011920929, 0.050000011920928955, 0.7749999761581421, 0.5, 0.875, 0.875, 0.5, 0.25, 0.4750000238418579, 0.5249999761581421, 0.550000011920929, 0.6000000238418579, 0.699999988079071, 0.9750000238418579, 0.625, 0.8250000476837158, 0.675000011920929, 0.025000005960464478, 0.6499999761581421, 0.675000011920929, 0.7999999523162842, 0.5, 0.9750000238418579, 1.0, 0.6499999761581421, 0.574999988079071, 0.5, 0.125, 0.8250000476837158, 0.9750000238418579, 0.625, 0.574999988079071, 0.6412500143051147, 0.875, 0.699999988079071, 0.7250000238418579, 0.574999988079071, 0.625, 0.7999999523162842, 0.9750000238418579, 0.875, 1.0, 0.699999988079071, 1.0, 0.30000001192092896, 0.574999988079071, 0.9537500143051147, 0.9750000238418579, 0.8999999761581421, 0.4750000238418579, 0.8999999761581421, 0.7250000238418579, 1.0, 0.8999999761581421, 0.875, 0.625, 0.375, 0.75, 0.7250000238418579, 0.8500000238418579, 0.025000005960464478, 0.7250000238418579, 0.7999999523162842, 0.25, 0.7749999761581421, 0.6499999761581421, 0.7749999761581421, 0.675000011920929, 0.8250000476837158, 0.9750000238418579, 0.7250000238418579, 0.550000011920929, 0.9750000238418579, 0.574999988079071, 0.75, 0.025000005960464478, 0.6000000238418579, 0.5249999761581421, 0.7749999761581421, 0.0, 0.2749999761581421, 0.75, 0.9500000476837158, 0.574999988079071, 0.550000011920929, 0.8999999761581421, 0.6837499737739563, 0.7999999523162842, 0.5, 0.75, 0.7749999761581421, 0.8250000476837158, 0.125, 0.9750000238418579, 0.25, 0.07499998807907104, 0.7250000238418579, 0.22499999403953552, 0.6499999761581421, 0.6499999761581421, 0.8999999761581421, 0.5, 0.7749999761581421, 0.6499999761581421, 0.44999998807907104, 0.4750000238418579, 0.5, 1.0, 0.5249999761581421, 0.75, 0.0, 1.0, 0.675000011920929, 0.30000001192092896, 0.5, 0.0, 0.6499999761581421, 0.8500000238418579, 0.9750000238418579, 0.5249999761581421, 0.6000000238418579, 0.8999999761581421, 0.9500000476837158, 0.574999988079071, 0.9750000238418579, 0.3999999761581421, 0.4750000238418579, 0.75, 0.7250000238418579, 0.5249999761581421, 0.625, 0.7999999523162842, 1.0, 0.9500000476837158, 0.675000011920929, 0.7250000238418579, 0.44999998807907104, 0.0, 0.574999988079071, 0.2749999761581421, 0.125, 0.699999988079071, 1.0, 0.32499998807907104, 0.9750000238418579, 0.8500000238418579, 0.7999999523162842, 0.699999988079071, 0.875, 0.574999988079071, 0.625, 0.375, 0.699999988079071, 0.44999998807907104, 0.574999988079071, 0.550000011920929, 0.9750000238418579, 0.9750000238418579, 0.15000000596046448, 0.7250000238418579, 0.7749999761581421, 0.550000011920929, 0.7250000238418579, 0.625, 0.875, 0.7749999761581421, 0.875, 0.625, 0.675000011920929, 0.875, 0.875, 0.75, 0.8250000476837158, 0.44999998807907104, 0.9249999523162842, 0.574999988079071, 0.778749942779541, 0.7749999761581421, 0.7749999761581421, 0.7749999761581421, 0.8500000238418579, 0.5249999761581421, 0.8250000476837158, 0.6499999761581421, 0.675000011920929, 0.9249999523162842, 0.875, 0.574999988079071, 0.9500000476837158, 0.9500000476837158, 0.8374999761581421, 0.675000011920929, 0.050000011920928955, 0.046249985694885254, 0.7749999761581421, 0.699999988079071, 0.8999999761581421, 0.30000001192092896, 0.375, 0.675000011920929, 0.875, 0.875, 0.8250000476837158, 0.4750000238418579, 0.0, 0.8500000238418579, 0.375, 0.699999988079071, 0.5, 0.9750000238418579, 0.8500000238418579, 0.75, 0.9750000238418579, 0.5, 0.9750000238418579, 0.44999998807907104, 0.7999999523162842, 1.0, 0.574999988079071, 0.7250000238418579, 0.550000011920929, 0.75, 1.0, 0.550000011920929, 0.3500000238418579, 0.7999999523162842, 0.8500000238418579, 0.6499999761581421, 0.550000011920929, 0.550000011920929, 0.8500000238418579, 0.8999999761581421, 0.4750000238418579, 0.025000005960464478, 0.7999999523162842, 0.6499999761581421, 0.0, 0.7999999523162842, 0.9500000476837158, 0.9249999523162842, 0.875, 0.9249999523162842, 0.7749999761581421, 0.7250000238418579, 0.42500001192092896, 0.8500000238418579, 0.6587499976158142, 0.7999999523162842, 0.9500000476837158, 0.875, 0.9750000238418579, 0.44999998807907104, 0.625, 0.3500000238418579, 0.9750000238418579, 1.0, 0.6499999761581421, 0.5, 0.9750000238418579, 0.8500000238418579, 0.675000011920929, 0.875, 0.9500000476837158, 0.6499999761581421, 0.8250000476837158, 0.3999999761581421, 0.4750000238418579, 0.875, 0.9125000238418579, 0.44999998807907104, 0.6499999761581421, 0.6000000238418579, 0.9500000476837158, 0.625, 0.550000011920929, 0.8500000238418579, 0.625, 0.7250000238418579, 0.8250000476837158, 0.0, 0.625, 0.75, 0.6499999761581421, 0.7749999761581421, 0.4750000238418579, 0.6000000238418579, 0.6499999761581421, 0.574999988079071, 0.625, 0.550000011920929, 0.550000011920929, 0.3500000238418579, 0.6499999761581421, 0.9500000476837158, 0.9750000238418579, 0.625, 0.8250000476837158, 0.7999999523162842, 0.625, 0.7999999523162842, 0.4750000238418579, 0.0, 0.8250000476837158, 0.8250000476837158, 1.0, 0.6000000238418579, 0.6000000238418579, 0.6000000238418579, 0.699999988079071, 0.6499999761581421, 0.574999988079071, 0.8250000476837158, 0.574999988079071, 0.9750000238418579, 0.42500001192092896, 0.3500000238418579, 0.875, 0.9750000238418579, 0.699999988079071, 0.9750000238418579, 0.44999998807907104, 0.3999999761581421, 0.7250000238418579, 0.675000011920929, 0.9249999523162842, 0.550000011920929, 0.8500000238418579, 0.4750000238418579, 0.8500000238418579, 0.7749999761581421, 0.625, 0.8250000476837158, 0.625, 0.8500000238418579, 0.30000001192092896, 0.0, 0.875, 0.6499999761581421, 0.6000000238418579, 0.6499999761581421, 0.3999999761581421, 0.550000011920929, 0.75, 0.375, 0.9750000238418579, 0.550000011920929, 0.4750000238418579, 0.3500000238418579, 0.9500000476837158, 0.9249999523162842, 0.025000005960464478, 1.0, 1.0, 0.675000011920929, 0.875, 0.9750000238418579, 0.6000000238418579, 0.6000000238418579, 0.09999999403953552, 0.6000000238418579, 0.574999988079071, 0.5249999761581421, 0.550000011920929, 0.3500000238418579, 0.875, 0.550000011920929, 0.19999998807907104, 0.025000005960464478, 0.30000001192092896, 0.8250000476837158, 0.550000011920929, 0.6499999761581421, 0.699999988079071, 0.42500001192092896, 0.625, 0.8999999761581421, 0.699999988079071, 0.75, 0.625, 0.9500000476837158, 0.4125000238418579, 0.30000001192092896, 0.09999999403953552, 0.75, 0.22499999403953552, 0.9249999523162842, 0.6000000238418579, 0.675000011920929, 0.75, 0.7250000238418579, 0.9750000238418579, 0.5249999761581421, 0.6000000238418579, 0.7412499785423279, 0.32499998807907104, 0.7999999523162842, 0.3999999761581421, 0.2749999761581421, 0.7749999761581421, 0.9750000238418579, 1.0, 0.574999988079071, 0.9500000476837158, 0.6499999761581421, 0.550000011920929, 1.0, 0.6000000238418579, 0.22499999403953552, 0.7749999761581421, 0.7999999523162842, 0.0, 0.6499999761581421, 0.8500000238418579, 0.7749999761581421, 0.4750000238418579, 0.42500001192092896, 0.7999999523162842, 0.550000011920929, 0.875, 0.7250000238418579, 0.4624999761581421, 0.7749999761581421, 0.8500000238418579, 0.4750000238418579, 0.8250000476837158, 0.8999999761581421, 0.6499999761581421, 0.625, 0.5, 0.2749999761581421, 0.75, 0.6000000238418579, 0.550000011920929, 1.0, 0.7250000238418579, 0.6000000238418579, 0.39125001430511475, 0.675000011920929, 0.75, 0.9287500381469727, 0.6000000238418579, 0.44999998807907104, 1.0, 0.0, 0.4750000238418579, 0.675000011920929, 0.8500000238418579, 0.7250000238418579, 0.699999988079071, 0.699999988079071, 0.44999998807907104, 0.9750000238418579, 0.7875000238418579, 0.8999999761581421, 0.8250000476837158, 0.5, 0.699999988079071, 0.3500000238418579, 0.625, 0.9249999523162842, 0.6499999761581421, 0.8500000238418579, 0.7749999761581421, 0.75, 0.8250000476837158, 1.0, 0.9249999523162842, 0.15000000596046448, 0.699999988079071, 0.9249999523162842, 0.875, 0.5874999761581421, 0.6499999761581421, 0.5249999761581421, 0.7250000238418579, 0.574999988079071, 0.9249999523162842, 0.699999988079071, 0.7250000238418579, 0.6000000238418579, 0.8500000238418579, 0.6499999761581421, 0.050000011920928955, 0.5712500214576721, 0.050000011920928955, 0.5, 0.675000011920929, 0.875, 0.375, 0.699999988079071, 0.75, 0.6499999761581421, 0.75, 0.6000000238418579, 0.3500000238418579, 0.32499998807907104, 0.9750000238418579, 0.5249999761581421, 0.675000011920929, 0.8500000238418579, 0.0, 0.625, 0.625, 0.6499999761581421, 0.625, 0.7999999523162842, 0.6499999761581421, 0.6000000238418579, 0.675000011920929, 0.3999999761581421, 0.9249999523162842, 0.44999998807907104, 0.550000011920929, 0.9500000476837158, 0.550000011920929, 0.7250000238418579, 0.4750000238418579, 0.3500000238418579, 0.7250000238418579, 0.6000000238418579, 0.9750000238418579, 0.7250000238418579, 0.3999999761581421, 0.19999998807907104, 0.375, 0.550000011920929, 0.4125000238418579, 0.6499999761581421, 0.6499999761581421, 0.9750000238418579, 0.9500000476837158, 0.875, 0.6499999761581421, 0.625, 0.0, 0.550000011920929, 0.699999988079071, 0.5249999761581421, 0.7749999761581421, 0.9249999523162842, 0.7250000238418579, 0.42500001192092896, 0.22499999403953552, 0.574999988079071, 0.75, 0.0, 0.625, 0.5249999761581421, 0.699999988079071, 0.550000011920929, 0.22499999403953552, 0.44999998807907104, 0.5249999761581421, 0.7999999523162842, 0.8500000238418579, 0.7250000238418579, 0.4750000238418579, 0.3999999761581421, 0.75, 0.675000011920929, 0.9249999523162842, 0.6499999761581421, 0.675000011920929, 0.7999999523162842, 0.8999999761581421, 0.9500000476837158, 0.699999988079071, 0.42500001192092896, 0.7999999523162842, 0.625, 0.6000000238418579, 1.0, 0.5, 0.9500000476837158, 0.3999999761581421, 0.7250000238418579, 0.44999998807907104, 0.9500000476837158, 0.625, 0.7999999523162842, 1.0, 0.574999988079071, 0.30000001192092896, 0.675000011920929, 0.6499999761581421, 0.574999988079071, 0.550000011920929, 1.0, 0.574999988079071, 0.17500001192092896, 0.07499998807907104, 0.6499999761581421, 0.675000011920929, 0.8500000238418579, 0.5, 0.675000011920929, 0.5249999761581421, 0.42500001192092896, 0.6000000238418579, 0.0, 0.8875000476837158, 0.8250000476837158, 1.0, 0.9750000238418579, 0.7749999761581421, 0.625, 0.550000011920929, 0.9750000238418579, 0.625, 0.8999999761581421, 0.550000011920929, 0.675000011920929, 0.8250000476837158, 0.7250000238418579, 0.3500000238418579, 0.7749999761581421, 0.375, 0.5, 0.75, 0.5, 0.5249999761581421, 0.0, 0.44999998807907104, 0.7999999523162842, 0.30000001192092896, 0.6499999761581421, 0.8999999761581421, 0.75, 0.550000011920929, 1.0, 0.5249999761581421, 0.9249999523162842, 0.550000011920929, 0.025000005960464478, 0.875, 0.32499998807907104, 0.75, 0.7999999523162842, 0.7250000238418579, 0.22499999403953552, 0.0, 0.8250000476837158, 0.2749999761581421, 0.550000011920929, 0.699999988079071, 0.25, 0.42500001192092896, 0.6499999761581421, 0.75, 0.0, 0.5, 0.025000005960464478, 0.5249999761581421, 0.5, 0.875, 0.6499999761581421, 0.9249999523162842, 0.6499999761581421, 0.9500000476837158, 0.9500000476837158, 0.875, 0.9249999523162842, 0.7250000238418579, 0.75, 1.0, 0.6499999761581421, 0.4750000238418579, 0.3999999761581421, 0.0, 0.0, 0.3999999761581421, 0.550000011920929, 0.75, 0.675000011920929, 0.8500000238418579, 0.875, 0.7250000238418579, 0.125, 0.8250000476837158, 0.6000000238418579, 0.8250000476837158, 0.625, 0.699999988079071, 0.9750000238418579, 0.046249985694885254, 0.9750000238418579, 0.9750000238418579, 0.8999999761581421, 0.75, 0.0, 0.8250000476837158, 0.699999988079071, 1.0, 0.699999988079071, 0.48000001907348633, 0.625, 0.8250000476837158, 0.8999999761581421, 0.0, 0.625, 0.8250000476837158, 0.875, 0.675000011920929, 0.8500000238418579, 0.8250000476837158, 0.9750000238418579, 0.0, 0.44999998807907104, 0.699999988079071, 0.675000011920929, 0.8500000238418579, 0.050000011920928955, 0.8500000238418579, 0.6499999761581421, 0.550000011920929, 0.875, 0.7250000238418579, 0.0, 0.7999999523162842, 0.9249999523162842, 0.7250000238418579, 0.6000000238418579, 0.699999988079071, 0.6499999761581421, 0.875, 0.025000005960464478, 0.6000000238418579, 0.22499999403953552, 0.7999999523162842, 0.8250000476837158, 0.4750000238418579, 0.125, 0.3500000238418579, 0.3999999761581421, 0.7749999761581421, 0.3999999761581421, 0.699999988079071, 1.0, 0.574999988079071, 0.75, 1.0, 0.0, 0.9500000476837158, 0.9500000476837158, 1.0, 0.75, 0.3999999761581421, 0.625, 0.42500001192092896, 0.9750000238418579, 0.875, 0.7250000238418579, 0.5249999761581421, 0.574999988079071, 0.6000000238418579, 0.7999999523162842, 0.8500000238418579, 0.0, 0.8624999523162842, 0.675000011920929, 0.30000001192092896, 0.75, 0.6000000238418579, 0.9750000238418579, 0.574999988079071, 0.625, 0.699999988079071, 0.7250000238418579, 0.8250000476837158, 0.8500000238418579, 0.4750000238418579, 0.375, 0.5249999761581421, 0.9249999523162842, 0.32499998807907104, 0.699999988079071, 0.47874999046325684, 1.0, 0.5, 0.75, 0.32499998807907104, 0.0, 0.7999999523162842, 0.3999999761581421, 0.42500001192092896, 0.9249999523162842, 0.625, 0.44999998807907104, 0.625, 0.699999988079071, 0.9750000238418579, 0.4750000238418579, 0.375, 0.6000000238418579, 0.9500000476837158, 0.574999988079071, 0.3412500023841858, 0.7250000238418579, 0.5249999761581421, 0.8250000476837158, 0.625, 0.7999999523162842, 0.75, 0.3500000238418579, 0.8500000238418579, 0.9750000238418579, 0.5375000238418579, 0.5249999761581421, 0.6000000238418579, 0.6000000238418579, 0.8250000476837158, 0.699999988079071, 0.5249999761581421, 0.0, 0.9750000238418579, 0.9500000476837158, 0.025000005960464478, 0.574999988079071, 0.3999999761581421, 0.22499999403953552, 0.875, 1.0, 0.0, 0.574999988079071, 0.7749999761581421, 0.4662500023841858, 0.699999988079071, 0.32499998807907104, 0.375, 0.908750057220459, 0.675000011920929, 0.4750000238418579, 0.75, 0.7999999523162842, 0.675000011920929, 0.5249999761581421, 0.625, 0.7999999523162842, 0.6000000238418579, 0.7749999761581421, 0.7250000238418579, 0.32499998807907104, 0.75, 0.125, 0.625, 0.9750000238418579, 0.8500000238418579, 0.4750000238418579, 0.675000011920929, 0.574999988079071, 0.75, 0.09999999403953552, 0.9500000476837158, 0.6499999761581421, 0.9500000476837158, 0.42500001192092896, 0.625, 0.8500000238418579, 0.875, 0.675000011920929, 0.699999988079071, 0.42500001192092896, 0.7999999523162842, 0.8500000238418579, 0.9249999523162842, 0.4750000238418579, 0.9249999523162842, 0.6000000238418579, 0.875, 0.75, 0.7749999761581421, 0.699999988079071, 0.675000011920929, 0.9750000238418579, 0.09999999403953552, 0.5, 0.7250000238418579, 0.0, 0.75, 0.42500001192092896, 0.5249999761581421, 0.32499998807907104, 0.42500001192092896, 0.4750000238418579, 0.4750000238418579, 0.7749999761581421, 0.5, 0.625, 0.75, 0.6499999761581421, 0.9500000476837158, 0.6499999761581421, 0.6499999761581421, 0.9249999523162842, 0.2749999761581421, 0.550000011920929, 0.7250000238418579, 0.550000011920929, 0.25, 0.75, 0.7999999523162842, 0.75, 0.675000011920929, 0.4750000238418579, 0.75, 0.875, 0.9249999523162842, 0.625, 0.32499998807907104, 0.6000000238418579, 0.675000011920929, 0.9750000238418579, 0.6000000238418579, 0.0, 0.625, 0.699999988079071, 0.7749999761581421, 0.625, 0.75, 0.6000000238418579, 0.9750000238418579, 0.75, 0.42500001192092896, 0.5249999761581421, 0.5874999761581421, 0.8250000476837158, 0.6499999761581421, 0.4750000238418579, 0.6000000238418579, 0.025000005960464478, 0.550000011920929, 0.574999988079071, 0.8999999761581421, 0.15000000596046448, 0.675000011920929, 0.0, 0.8250000476837158, 0.7749999761581421, 0.699999988079071, 0.9750000238418579, 0.6000000238418579, 0.5249999761581421, 0.7749999761581421, 0.7250000238418579, 0.550000011920929, 0.9500000476837158, 0.07499998807907104, 0.875, 0.9750000238418579, 0.0, 0.9750000238418579, 0.75, 0.7749999761581421, 0.8250000476837158, 0.6000000238418579, 0.675000011920929, 0.6000000238418579, 0.44999998807907104, 0.2749999761581421, 0.8999999761581421, 0.75, 0.5, 0.6499999761581421, 0.75, 0.550000011920929, 0.8500000238418579, 0.8250000476837158, 0.7749999761581421, 0.42500001192092896, 0.3999999761581421, 0.8250000476837158, 0.7749999761581421, 0.675000011920929, 0.8250000476837158, 0.875, 0.9750000238418579, 0.699999988079071, 0.375, 0.574999988079071, 0.7999999523162842, 0.17500001192092896, 0.675000011920929, 0.574999988079071, 0.7999999523162842, 0.75, 0.8250000476837158, 0.3999999761581421, 0.6499999761581421, 0.9750000238418579, 0.0, 0.5249999761581421, 0.42500001192092896, 0.0, 0.574999988079071, 0.6499999761581421, 0.625, 0.5, 0.8999999761581421, 0.675000011920929, 0.4750000238418579, 0.9750000238418579, 0.8999999761581421, 0.8250000476837158, 0.44999998807907104, 0.42500001192092896, 0.8250000476837158, 0.675000011920929, 0.7749999761581421, 0.8250000476837158, 0.6000000238418579, 0.8999999761581421, 0.0, 0.6499999761581421, 0.875, 0.9500000476837158, 0.6000000238418579, 0.5, 0.7749999761581421, 0.9500000476837158, 0.025000005960464478, 0.8999999761581421, 0.5249999761581421, 0.699999988079071, 0.0, 0.42500001192092896, 0.17500001192092896, 0.3999999761581421, 0.32499998807907104, 0.625, 0.6499999761581421, 0.699999988079071, 0.550000011920929, 0.9750000238418579, 0.8999999761581421, 0.9750000238418579, 0.550000011920929, 0.625, 0.8250000476837158, 0.9500000476837158, 0.75, 0.5, 0.8500000238418579, 0.625, 0.7749999761581421, 0.675000011920929, 0.574999988079071, 0.9750000238418579, 0.550000011920929, 0.4750000238418579, 0.8500000238418579, 0.0, 0.42500001192092896, 0.7749999761581421, 0.9249999523162842, 0.375, 0.699999988079071, 0.875, 0.375, 0.0, 0.9249999523162842, 0.0, 0.8999999761581421, 0.6499999761581421, 0.7999999523162842, 0.625, 0.7749999761581421, 0.0, 0.9249999523162842, 0.675000011920929, 0.699999988079071, 0.9500000476837158, 0.3500000238418579, 0.0, 0.7250000238418579, 0.6499999761581421, 0.6000000238418579, 0.09999999403953552, 0.5249999761581421, 0.7250000238418579, 0.4750000238418579, 0.0, 0.3500000238418579, 0.875, 0.699999988079071, 0.5249999761581421, 0.75, 0.8250000476837158, 0.6499999761581421, 0.9500000476837158, 0.625, 0.699999988079071, 0.3999999761581421, 0.6000000238418579, 0.2749999761581421, 0.9500000476837158, 0.0, 0.7250000238418579, 0.9249999523162842, 0.625, 0.675000011920929, 0.75, 0.8500000238418579, 0.574999988079071, 0.75, 0.574999988079071, 0.17500001192092896, 0.375, 0.699999988079071, 0.675000011920929, 0.8250000476837158, 0.875, 0.42500001192092896, 0.625, 0.8250000476837158, 0.7749999761581421, 0.625, 0.574999988079071, 0.675000011920929, 0.9500000476837158, 0.75, 0.9500000476837158, 0.625, 0.44999998807907104, 0.0, 0.3999999761581421, 0.8500000238418579, 0.7250000238418579, 0.675000011920929, 0.19999998807907104, 0.7749999761581421, 0.0, 0.875, 0.9500000476837158, 0.675000011920929, 0.7999999523162842, 0.5, 0.7250000238418579, 0.7250000238418579, 0.574999988079071, 0.675000011920929, 0.4750000238418579, 0.09999999403953552, 0.6499999761581421, 0.0, 0.875, 0.42500001192092896, 0.875, 0.8999999761581421, 1.0, 0.9500000476837158, 0.5249999761581421, 0.6499999761581421, 0.9500000476837158, 0.4662500023841858, 0.6000000238418579, 0.699999988079071, 0.9750000238418579, 0.574999988079071, 0.30000001192092896, 0.574999988079071, 0.675000011920929, 0.9249999523162842, 0.9537500143051147, 0.7250000238418579, 1.0, 0.5, 0.75, 0.07499998807907104, 0.9750000238418579, 0.9750000238418579, 0.25, 0.574999988079071, 0.5249999761581421, 0.025000005960464478, 0.675000011920929, 0.375, 0.42500001192092896, 0.550000011920929, 0.9500000476837158, 0.8250000476837158, 0.75, 0.8250000476837158, 0.050000011920928955, 0.625, 0.5249999761581421, 0.4750000238418579, 0.0, 0.8500000238418579, 0.7749999761581421, 0.6000000238418579, 0.574999988079071, 0.9750000238418579, 0.8250000476837158, 0.675000011920929, 0.5249999761581421, 0.9750000238418579, 0.875, 0.875, 0.0, 0.6000000238418579, 0.32499998807907104, 0.375, 0.9750000238418579, 0.5, 0.6499999761581421, 1.0, 0.675000011920929, 0.4750000238418579, 0.025000005960464478, 0.7749999761581421, 0.32499998807907104, 0.9500000476837158, 0.75, 0.4750000238418579, 0.7749999761581421, 0.875, 0.7124999761581421, 0.75, 0.8999999761581421, 0.9750000238418579, 0.7749999761581421, 0.375, 0.8500000238418579, 0.6000000238418579, 0.8250000476837158, 0.625, 0.7749999761581421, 1.0, 0.6000000238418579, 0.42500001192092896, 0.42500001192092896, 0.42500001192092896, 0.9500000476837158, 0.025000005960464478, 0.7749999761581421, 0.375, 0.25, 0.9500000476837158, 0.4750000238418579, 0.699999988079071, 0.3999999761581421, 0.8999999761581421, 0.025000005960464478, 0.4750000238418579, 0.375, 0.4750000238418579, 0.875, 0.7749999761581421, 0.6000000238418579, 0.8250000476837158, 0.9249999523162842, 0.5, 0.6499999761581421, 0.19999998807907104, 0.675000011920929, 0.7999999523162842, 0.42500001192092896, 0.9750000238418579, 0.7250000238418579, 0.7749999761581421, 1.0, 0.7749999761581421, 0.3999999761581421, 0.9500000476837158, 0.699999988079071, 0.625, 0.574999988079071, 0.2749999761581421, 0.8999999761581421, 0.7250000238418579, 0.5249999761581421, 0.8500000238418579, 0.75, 0.5249999761581421, 0.4750000238418579, 0.7250000238418579, 0.44999998807907104, 0.625, 0.6499999761581421, 0.3999999761581421, 0.19999998807907104, 0.9249999523162842, 0.625, 0.6499999761581421, 0.6000000238418579, 0.9750000238418579, 0.8500000238418579, 0.025000005960464478, 0.5, 0.574999988079071, 0.625, 0.9500000476837158, 0.550000011920929, 0.8999999761581421, 0.574999988079071, 0.574999988079071, 0.75, 0.6499999761581421, 0.7749999761581421, 0.5, 0.6499999761581421, 0.15000000596046448, 0.9750000238418579, 0.22499999403953552, 0.5249999761581421, 0.8999999761581421, 1.0, 0.9750000238418579, 0.7749999761581421, 0.9249999523162842, 0.75, 0.8250000476837158, 0.13374999165534973, 0.0, 0.32499998807907104, 0.9500000476837158, 0.2749999761581421, 0.9500000476837158, 0.7749999761581421, 0.5, 0.550000011920929, 0.8500000238418579, 0.8250000476837158, 0.6000000238418579, 0.32499998807907104, 0.9500000476837158, 0.3500000238418579, 0.6499999761581421, 0.8250000476837158, 0.6499999761581421, 0.6499999761581421, 0.4750000238418579, 0.9249999523162842, 0.625, 0.75, 0.9500000476837158, 0.875, 0.550000011920929, 0.8999999761581421, 0.5249999761581421, 0.9750000238418579, 0.75, 0.550000011920929, 0.09999999403953552, 0.8500000238418579, 0.75, 0.6499999761581421, 0.050000011920928955, 0.44999998807907104, 0.5, 0.5249999761581421, 0.5, 0.3500000238418579, 0.5, 0.3999999761581421, 0.574999988079071, 0.8250000476837158, 0.025000005960464478, 0.8500000238418579, 0.0, 1.0, 0.7999999523162842, 0.6000000238418579, 0.22499999403953552, 0.550000011920929, 0.9500000476837158, 0.8999999761581421, 0.75, 0.07499998807907104, 0.75, 0.9750000238418579, 0.375, 0.875, 0.699999988079071, 0.3500000238418579, 0.4750000238418579, 0.3500000238418579, 0.9750000238418579, 0.6499999761581421, 0.4750000238418579, 0.625, 0.5249999761581421, 0.5037500262260437, 0.5249999761581421, 0.9249999523162842, 0.09999999403953552, 0.9500000476837158, 0.5912500023841858, 0.574999988079071, 0.699999988079071, 0.6499999761581421, 0.6287500262260437, 0.550000011920929, 0.7749999761581421, 0.25, 0.8999999761581421, 0.7999999523162842, 0.9500000476837158, 0.9750000238418579, 0.6499999761581421, 0.8999999761581421, 0.550000011920929, 0.07499998807907104, 0.0, 0.44999998807907104, 0.8500000238418579, 0.6000000238418579, 0.6499999761581421, 0.625, 0.7749999761581421, 0.15000000596046448, 0.44999998807907104, 0.9750000238418579, 0.7250000238418579, 0.8500000238418579, 0.675000011920929, 0.675000011920929, 0.7749999761581421, 1.0, 0.8500000238418579, 0.875, 0.7999999523162842, 0.17500001192092896, 0.75, 0.125, 0.625, 0.32499998807907104, 0.2749999761581421, 0.375, 0.9750000238418579, 0.025000005960464478, 0.699999988079071, 0.47874999046325684, 0.8500000238418579, 0.4750000238418579, 0.375, 0.9249999523162842, 0.7999999523162842, 0.7999999523162842, 0.375, 0.8250000476837158, 0.7962499856948853, 1.0, 0.75, 0.625, 0.875, 0.625, 0.846250057220459, 0.5249999761581421, 0.375, 0.3999999761581421, 0.2749999761581421, 1.0, 0.675000011920929, 0.4750000238418579, 0.7749999761581421, 0.25, 0.574999988079071, 0.0, 0.675000011920929, 0.7999999523162842, 0.75, 0.3500000238418579, 0.75, 0.050000011920928955, 0.42500001192092896, 0.6000000238418579, 0.75, 0.675000011920929, 0.25, 0.07499998807907104, 0.699999988079071, 0.875, 0.875, 0.6499999761581421, 0.44999998807907104, 0.75, 0.9249999523162842, 0.8999999761581421, 0.574999988079071, 0.5, 0.75, 0.0, 0.8250000476837158, 0.9500000476837158, 0.6499999761581421, 0.44999998807907104, 0.36250001192092896, 0.8999999761581421, 0.0, 0.3999999761581421, 0.6000000238418579, 0.9537500143051147, 0.6037499904632568, 0.675000011920929, 0.6000000238418579, 0.574999988079071, 0.3500000238418579, 0.0, 0.5, 0.625, 0.574999988079071, 0.75, 0.375, 0.675000011920929, 0.675000011920929, 1.0, 0.9500000476837158, 0.875, 0.574999988079071, 0.6499999761581421, 0.9500000476837158, 0.2749999761581421, 0.44999998807907104, 0.625, 0.625, 0.32499998807907104, 0.7749999761581421, 0.675000011920929, 0.7250000238418579, 0.7749999761581421, 0.44999998807907104, 0.9500000476837158, 0.574999988079071, 0.8500000238418579, 0.8999999761581421, 1.0, 0.699999988079071, 0.6499999761581421, 0.7250000238418579, 0.9500000476837158, 0.0, 0.9750000238418579, 0.9500000476837158, 0.025000005960464478, 0.875, 0.7250000238418579, 0.875, 0.44999998807907104, 0.5249999761581421, 0.09999999403953552, 0.7749999761581421, 0.75, 0.3999999761581421, 0.6499999761581421, 0.375, 0.9500000476837158, 1.0, 0.875, 0.9500000476837158, 0.8999999761581421, 0.699999988079071, 0.6087499856948853, 0.7999999523162842, 0.3999999761581421, 0.050000011920928955, 0.5, 0.7250000238418579, 0.5249999761581421, 0.699999988079071, 0.7999999523162842, 0.8500000238418579, 0.8250000476837158, 0.574999988079071, 0.30000001192092896, 0.9750000238418579, 0.0, 0.44999998807907104, 1.0, 0.875, 0.8500000238418579, 0.8999999761581421, 0.8250000476837158, 0.22499999403953552, 0.5249999761581421, 0.44999998807907104, 0.75, 0.6000000238418579, 0.42500001192092896, 1.0, 0.8999999761581421, 0.574999988079071, 0.675000011920929, 0.4750000238418579, 0.9249999523162842, 0.6499999761581421, 0.675000011920929, 0.625, 0.8999999761581421, 0.875, 0.15000000596046448, 0.6000000238418579, 0.875, 0.675000011920929, 0.9750000238418579, 0.22499999403953552, 1.0, 0.7250000238418579, 0.9750000238418579, 0.6499999761581421, 0.675000011920929, 0.9750000238418579, 0.7999999523162842, 0.09999999403953552, 0.44999998807907104, 0.44999998807907104, 0.8999999761581421, 0.699999988079071, 0.875, 0.025000005960464478, 1.0, 0.875, 0.7250000238418579, 0.7999999523162842, 0.8500000238418579, 0.4750000238418579, 0.7749999761581421, 0.699999988079071, 0.6000000238418579, 0.09999999403953552, 0.9249999523162842, 0.375, 0.699999988079071, 0.30000001192092896, 0.6000000238418579, 0.25, 0.44999998807907104, 1.0, 0.6499999761581421, 0.5249999761581421, 0.17500001192092896, 0.6000000238418579, 0.6162499785423279, 0.22499999403953552, 0.550000011920929, 0.7749999761581421, 0.8500000238418579, 0.625, 0.8999999761581421, 0.30000001192092896, 0.699999988079071, 1.0, 0.699999988079071, 0.9750000238418579, 0.699999988079071, 0.675000011920929, 0.050000011920928955, 0.5, 0.7250000238418579, 0.675000011920929, 0.5249999761581421, 0.625, 0.550000011920929, 0.8250000476837158, 0.6499999761581421, 0.30000001192092896, 0.675000011920929, 0.025000005960464478, 0.7999999523162842, 0.550000011920929, 0.8500000238418579, 0.8500000238418579, 0.5249999761581421, 0.9500000476837158, 0.550000011920929, 0.42500001192092896, 0.8500000238418579, 0.8500000238418579, 0.7999999523162842, 0.4750000238418579, 0.09999999403953552, 0.17500001192092896, 0.9750000238418579, 0.3500000238418579, 0.7250000238418579, 0.574999988079071, 0.6000000238418579, 0.6499999761581421, 0.550000011920929, 0.9500000476837158, 0.9500000476837158, 0.44999998807907104, 0.550000011920929, 0.675000011920929, 0.699999988079071, 0.3999999761581421, 0.30000001192092896, 0.42500001192092896, 1.0, 0.7999999523162842, 0.44999998807907104, 0.675000011920929, 0.7250000238418579, 0.2749999761581421, 0.8712500333786011, 0.5249999761581421, 0.550000011920929, 0.6000000238418579, 0.32499998807907104, 0.025000005960464478, 0.6499999761581421, 0.7250000238418579, 0.7999999523162842, 0.75, 0.574999988079071, 0.6499999761581421, 0.6499999761581421, 0.5, 0.699999988079071, 0.3500000238418579, 0.8999999761581421, 0.550000011920929, 0.44999998807907104, 0.9249999523162842, 0.0, 0.574999988079071, 0.42500001192092896, 0.7749999761581421, 0.675000011920929, 0.8999999761581421, 0.8500000238418579, 0.5, 0.550000011920929, 0.22499999403953552, 0.5249999761581421, 0.9500000476837158, 1.0, 0.8500000238418579, 0.699999988079071, 1.0, 0.3500000238418579, 0.050000011920928955, 0.8250000476837158, 0.7337499856948853, 0.3500000238418579, 0.574999988079071, 0.6499999761581421, 0.875, 0.675000011920929, 0.75, 0.4750000238418579, 0.050000011920928955, 0.025000005960464478, 0.44999998807907104, 0.6462500095367432, 0.050000011920928955, 0.3500000238418579, 0.9750000238418579, 0.8250000476837158, 0.8250000476837158, 0.675000011920929, 0.5, 0.7250000238418579, 0.7999999523162842, 0.875, 0.8500000238418579, 0.7749999761581421, 0.5, 0.3999999761581421, 0.7250000238418579, 0.3999999761581421, 0.6000000238418579, 0.875, 0.3500000238418579, 0.9249999523162842, 0.574999988079071, 0.9500000476837158, 0.7749999761581421, 0.0, 0.3500000238418579, 1.0, 0.8250000476837158, 0.675000011920929, 0.7250000238418579, 0.875, 0.0, 0.699999988079071, 0.44999998807907104, 0.550000011920929, 0.550000011920929, 0.699999988079071, 0.44999998807907104, 0.8250000476837158, 0.7749999761581421, 0.19999998807907104, 0.7999999523162842, 1.0, 0.0, 0.8500000238418579, 0.699999988079071, 0.5249999761581421, 0.675000011920929, 0.75, 0.42500001192092896, 0.6499999761581421, 0.625, 0.9249999523162842, 0.050000011920928955, 0.699999988079071, 0.44999998807907104, 0.7999999523162842, 0.9500000476837158, 0.875, 0.8250000476837158, 0.42500001192092896, 0.9249999523162842, 0.375, 0.550000011920929, 0.6000000238418579, 0.8999999761581421, 0.9249999523162842, 0.574999988079071, 0.9750000238418579, 0.699999988079071, 0.6499999761581421, 0.4750000238418579, 0.699999988079071, 0.7250000238418579, 0.7999999523162842, 0.5037500262260437, 0.6499999761581421, 0.07499998807907104, 0.625, 0.8250000476837158, 0.875, 0.7250000238418579, 0.8500000238418579, 0.675000011920929, 0.8999999761581421, 0.699999988079071, 0.44999998807907104, 0.3162500262260437, 0.9249999523162842, 0.75, 0.574999988079071, 0.699999988079071, 0.699999988079071, 0.75, 0.3500000238418579, 0.44999998807907104, 0.9750000238418579, 0.7250000238418579, 0.699999988079071, 0.8999999761581421, 0.9750000238418579, 0.699999988079071, 0.8624999523162842, 0.7250000238418579, 0.550000011920929, 0.6000000238418579, 0.0, 0.699999988079071, 0.675000011920929, 0.375, 0.675000011920929, 0.8999999761581421, 0.574999988079071, 0.9500000476837158, 0.5249999761581421, 0.699999988079071, 0.3999999761581421, 0.6499999761581421, 0.6000000238418579, 0.375, 0.699999988079071, 0.8999999761581421, 0.5, 0.8500000238418579, 0.44999998807907104, 0.9249999523162842, 0.0, 0.9249999523162842, 0.2749999761581421, 0.15000000596046448, 0.4750000238418579, 0.6000000238418579, 0.5249999761581421, 0.9500000476837158, 0.42500001192092896, 0.5, 0.7749999761581421, 0.675000011920929, 0.625, 0.699999988079071, 0.9750000238418579, 0.7999999523162842, 0.6000000238418579, 0.8500000238418579, 0.6000000238418579, 0.8250000476837158, 1.0, 0.6499999761581421, 0.5249999761581421, 0.675000011920929, 0.9249999523162842, 0.625, 0.675000011920929, 0.7250000238418579, 0.3999999761581421, 0.9249999523162842, 0.699999988079071, 0.19999998807907104, 0.6499999761581421, 0.5837500095367432, 0.19999998807907104, 0.625, 0.550000011920929, 0.5249999761581421, 0.550000011920929, 0.22499999403953552, 0.75, 0.6499999761581421, 0.8500000238418579, 0.550000011920929, 0.8250000476837158, 0.625, 0.9500000476837158, 0.8250000476837158, 0.375, 0.8999999761581421, 0.7250000238418579, 0.625, 0.15000000596046448, 0.625, 0.2749999761581421, 0.6499999761581421, 0.574999988079071, 0.9750000238418579, 0.6499999761581421, 0.9750000238418579, 0.3999999761581421, 0.7749999761581421, 0.6000000238418579, 0.32499998807907104, 0.8999999761581421, 0.6000000238418579, 0.7749999761581421, 0.7749999761581421, 0.7250000238418579, 0.0, 0.7999999523162842, 0.699999988079071, 0.6499999761581421, 0.07499998807907104, 0.7999999523162842, 0.6499999761581421, 0.44999998807907104, 0.8500000238418579, 0.875, 0.5249999761581421, 0.8250000476837158, 0.6449999809265137, 0.550000011920929, 1.0, 0.7999999523162842, 0.5, 0.75, 0.3999999761581421, 0.675000011920929, 0.875, 0.7250000238418579, 0.15000000596046448, 0.7250000238418579, 0.625, 0.7250000238418579, 0.3999999761581421, 0.9750000238418579, 0.42500001192092896, 0.9750000238418579, 0.908750057220459, 0.8500000238418579, 0.44999998807907104, 0.8500000238418579, 0.30000001192092896, 0.75, 0.7749999761581421, 0.8500000238418579, 0.6499999761581421, 0.7999999523162842, 0.9750000238418579, 0.5249999761581421, 0.8999999761581421, 0.625, 0.9500000476837158, 0.8500000238418579, 0.6000000238418579, 0.9500000476837158, 0.8999999761581421, 0.8250000476837158, 0.19999998807907104, 0.6000000238418579, 0.4750000238418579, 0.7250000238418579, 0.550000011920929, 0.45875000953674316, 1.0, 0.7250000238418579, 0.625, 0.875, 0.7749999761581421, 0.9500000476837158, 0.7250000238418579, 0.846250057220459, 0.4750000238418579, 0.8250000476837158, 0.0, 0.7749999761581421, 0.3999999761581421, 0.75, 0.6499999761581421, 0.0, 0.625, 0.675000011920929, 0.675000011920929, 1.0, 0.22499999403953552, 0.4750000238418579, 0.6499999761581421, 0.6499999761581421, 0.3999999761581421, 0.9249999523162842, 0.675000011920929, 0.8250000476837158, 0.19999998807907104, 0.550000011920929, 0.8500000238418579, 0.7250000238418579, 0.5, 0.9249999523162842, 0.6499999761581421, 0.3500000238418579, 0.19999998807907104, 0.8999999761581421, 0.6000000238418579, 0.09999999403953552, 0.050000011920928955, 0.6000000238418579, 0.550000011920929, 0.9750000238418579, 0.675000011920929, 0.375, 0.8250000476837158, 0.9249999523162842, 0.6499999761581421, 0.0, 0.07499998807907104, 0.050000011920928955, 0.15000000596046448, 0.8250000476837158, 0.8500000238418579, 0.9249999523162842, 0.9750000238418579, 0.9750000238418579, 0.5, 0.07499998807907104, 0.4750000238418579, 0.574999988079071, 0.6499999761581421, 0.8500000238418579, 0.0, 0.09999999403953552, 0.42500001192092896, 0.5249999761581421, 0.8250000476837158, 0.8999999761581421, 0.625, 0.6000000238418579, 0.675000011920929, 0.3999999761581421, 0.574999988079071, 0.7749999761581421, 0.7250000238418579, 0.574999988079071, 0.875, 0.5, 0.42500001192092896, 0.625, 0.675000011920929, 0.8500000238418579, 0.699999988079071, 0.7749999761581421, 0.0, 0.8250000476837158, 0.9500000476837158, 0.375, 0.17500001192092896, 0.8500000238418579, 0.699999988079071, 0.8250000476837158, 0.9750000238418579, 0.8250000476837158, 0.7749999761581421, 0.75, 0.9750000238418579, 0.5, 0.22499999403953552, 0.9750000238418579, 0.574999988079071, 0.6499999761581421, 0.9750000238418579, 0.15000000596046448, 0.75, 0.42500001192092896, 0.675000011920929, 0.9249999523162842, 0.7999999523162842, 0.4750000238418579, 0.75, 0.7749999761581421, 0.75, 0.19999998807907104, 0.625, 0.7999999523162842, 0.675000011920929, 0.550000011920929, 0.6000000238418579, 0.8500000238418579, 0.9750000238418579, 1.0, 0.625, 0.5, 0.025000005960464478, 0.675000011920929, 0.5249999761581421, 0.6499999761581421, 0.5, 0.22499999403953552, 0.675000011920929, 0.7749999761581421, 0.8500000238418579, 0.574999988079071, 0.6499999761581421, 0.699999988079071, 0.7999999523162842, 0.7712500095367432, 0.699999988079071, 0.0, 0.6499999761581421, 0.5249999761581421, 0.675000011920929, 0.6000000238418579, 0.8500000238418579, 1.0, 0.699999988079071, 0.025000005960464478, 0.7250000238418579, 0.875, 0.75, 0.5, 1.0, 0.574999988079071, 0.6000000238418579, 0.7250000238418579, 0.4750000238418579, 0.9750000238418579, 0.3500000238418579, 1.0, 0.6499999761581421, 0.8250000476837158, 0.8999999761581421, 0.5, 0.7749999761581421, 1.0, 0.375, 0.8500000238418579, 0.9249999523162842, 0.6499999761581421, 0.7999999523162842, 0.44999998807907104, 0.44999998807907104, 0.9500000476837158, 0.4750000238418579, 0.6499999761581421, 0.5249999761581421, 0.699999988079071, 0.44999998807907104, 0.675000011920929, 0.675000011920929, 0.699999988079071, 0.675000011920929, 0.4750000238418579, 0.574999988079071, 1.0, 0.550000011920929, 0.7749999761581421, 0.375, 0.22499999403953552, 0.5249999761581421, 0.550000011920929, 0.050000011920928955, 0.9750000238418579, 0.9249999523162842, 1.0, 0.699999988079071, 0.17500001192092896, 0.0, 0.0, 0.8500000238418579, 0.6037499904632568, 0.550000011920929, 0.0, 0.875, 0.6000000238418579, 0.7999999523162842, 0.7999999523162842, 0.675000011920929, 0.875, 0.5249999761581421, 0.9750000238418579, 0.3999999761581421, 0.550000011920929, 0.574999988079071, 0.699999988079071, 0.0, 0.42500001192092896, 0.47874999046325684, 0.8999999761581421, 0.675000011920929, 0.9500000476837158, 0.6000000238418579, 0.19999998807907104, 0.675000011920929, 0.6000000238418579, 0.7250000238418579, 0.9750000238418579, 0.7250000238418579, 0.6499999761581421, 0.17500001192092896, 0.6000000238418579, 0.6499999761581421, 0.44999998807907104, 0.5, 0.625, 0.4750000238418579, 0.699999988079071, 0.6000000238418579, 0.9750000238418579, 0.6000000238418579, 0.6499999761581421, 0.6625000238418579, 1.0, 0.625, 0.5249999761581421, 0.0, 0.25, 0.44999998807907104, 0.7250000238418579, 0.5249999761581421, 0.8500000238418579, 0.699999988079071, 0.8999999761581421, 0.625, 0.4750000238418579, 0.3999999761581421, 0.7999999523162842, 0.8500000238418579, 0.42500001192092896, 0.8999999761581421, 0.6499999761581421, 0.3999999761581421, 0.15000000596046448, 0.7749999761581421, 0.8250000476837158, 0.8999999761581421, 0.8250000476837158, 0.44999998807907104, 0.7749999761581421, 0.19999998807907104, 0.44999998807907104, 0.8500000238418579, 0.6000000238418579, 0.675000011920929, 0.6499999761581421, 0.4750000238418579, 0.75, 0.22499999403953552, 0.6499999761581421, 1.0, 0.125, 0.8500000238418579, 0.22499999403953552, 0.8500000238418579, 0.6499999761581421, 0.5, 0.7999999523162842, 0.7749999761581421, 0.625, 0.7749999761581421, 0.625, 0.8999999761581421, 0.15000000596046448, 0.2212499976158142, 0.7250000238418579, 0.5412499904632568, 0.050000011920928955, 0.675000011920929, 0.4750000238418579, 0.5249999761581421, 0.675000011920929, 0.75, 0.4750000238418579, 0.675000011920929, 0.8250000476837158, 0.5287500023841858, 0.125, 0.675000011920929, 0.8250000476837158, 0.375, 0.3999999761581421, 0.025000005960464478, 0.8250000476837158, 0.15000000596046448, 0.8999999761581421, 0.050000011920928955, 0.699999988079071, 0.050000011920928955, 0.550000011920929, 0.4750000238418579, 0.8500000238418579, 0.7749999761581421, 0.625, 0.550000011920929, 0.32499998807907104, 0.6499999761581421, 0.675000011920929, 0.9750000238418579, 0.699999988079071, 0.25, 0.6000000238418579, 0.6287500262260437, 0.675000011920929, 0.9500000476837158, 0.7250000238418579, 0.8250000476837158, 0.6499999761581421, 0.3500000238418579, 0.875, 0.8250000476837158, 0.8500000238418579, 0.9500000476837158, 0.42500001192092896, 0.5, 0.875, 0.625, 0.9750000238418579, 0.6000000238418579, 0.9500000476837158, 0.375, 0.550000011920929, 0.8250000476837158, 0.8999999761581421, 0.6499999761581421, 0.7712500095367432, 0.7749999761581421, 0.44999998807907104, 0.375, 0.0, 0.9249999523162842, 0.9249999523162842, 0.699999988079071, 0.699999988079071, 0.625, 0.2749999761581421, 0.625, 0.574999988079071, 0.550000011920929, 1.0, 0.7250000238418579, 0.625, 0.9750000238418579, 0.3500000238418579, 0.9500000476837158, 0.6000000238418579, 0.9500000476837158, 0.75, 0.8500000238418579, 0.6962500214576721, 0.9750000238418579, 0.7999999523162842, 0.6000000238418579, 0.625, 0.3500000238418579, 0.7999999523162842, 0.7999999523162842, 0.4750000238418579, 0.6499999761581421, 0.42500001192092896, 1.0, 0.625, 0.8250000476837158, 0.6000000238418579, 0.625, 0.9500000476837158, 0.625, 0.4750000238418579, 1.0, 0.3999999761581421, 0.7087500095367432, 0.9249999523162842, 0.8999999761581421, 0.574999988079071, 0.6000000238418579, 0.5, 0.9500000476837158, 0.07499998807907104, 0.625, 0.5249999761581421, 0.8250000476837158, 0.5, 0.44999998807907104, 0.6499999761581421, 0.38749998807907104, 0.574999988079071, 0.44999998807907104, 0.32499998807907104, 0.125, 0.875, 0.7999999523162842, 0.625, 0.675000011920929, 0.5249999761581421, 0.8250000476837158, 0.5, 0.75, 0.7999999523162842, 0.75, 0.5249999761581421, 0.3500000238418579, 0.375, 0.625, 0.625, 0.8500000238418579, 0.5249999761581421, 0.675000011920929, 0.75, 0.9500000476837158, 0.675000011920929, 0.625, 0.44999998807907104, 0.675000011920929, 0.574999988079071, 0.8250000476837158, 0.8500000238418579, 0.125, 0.2749999761581421, 0.699999988079071, 0.550000011920929, 0.675000011920929, 0.8999999761581421, 0.42500001192092896, 0.8500000238418579, 0.025000005960464478, 0.0, 0.44999998807907104, 0.6000000238418579, 0.8999999761581421, 0.6000000238418579, 0.6675000190734863, 0.875, 0.17500001192092896, 0.7749999761581421, 0.4750000238418579, 0.75, 0.025000005960464478, 0.574999988079071, 0.5249999761581421, 0.6000000238418579, 1.0, 0.7749999761581421, 0.8500000238418579, 0.625, 0.6499999761581421, 0.6499999761581421, 0.9500000476837158, 1.0, 0.9249999523162842, 0.9750000238418579, 0.8250000476837158, 0.5, 0.699999988079071, 0.5, 0.9500000476837158, 0.0, 0.7250000238418579, 0.574999988079071, 0.75, 0.8500000238418579, 0.125, 0.44999998807907104, 0.0, 0.0, 0.0, 0.6000000238418579, 0.7999999523162842, 1.0, 0.7999999523162842, 0.6000000238418579, 0.07499998807907104, 0.699999988079071, 0.9750000238418579, 0.5249999761581421, 0.4750000238418579, 1.0, 0.25, 0.6000000238418579, 0.8250000476837158, 0.5, 0.8500000238418579, 0.9500000476837158, 0.675000011920929, 0.875, 0.9500000476837158, 0.0, 0.5, 0.699999988079071, 0.625, 0.6000000238418579, 0.5, 0.0, 0.6000000238418579, 0.625, 0.675000011920929, 1.0, 0.7250000238418579, 0.3999999761581421, 0.6912500262260437, 0.5, 0.2749999761581421, 0.7749999761581421, 0.0, 0.4750000238418579, 0.574999988079071, 1.0, 0.875, 0.550000011920929, 0.7250000238418579, 0.675000011920929, 0.9249999523162842, 0.6499999761581421, 0.44999998807907104, 0.875, 0.8500000238418579, 0.8500000238418579, 0.550000011920929, 0.699999988079071, 0.7999999523162842, 0.025000005960464478, 0.7250000238418579, 0.6000000238418579, 0.75, 0.875, 0.7749999761581421, 0.625, 0.30000001192092896, 0.8250000476837158, 0.5249999761581421, 0.6499999761581421, 0.8250000476837158, 0.9500000476837158, 0.8287500143051147, 0.6499999761581421, 0.8250000476837158, 0.5, 0.6499999761581421, 0.5, 0.625, 0.9249999523162842, 0.875, 0.574999988079071, 0.5, 0.6000000238418579, 0.32499998807907104, 0.0, 0.15000000596046448, 0.17500001192092896, 0.75, 0.625, 0.675000011920929, 0.699999988079071, 0.0, 0.8999999761581421, 0.44999998807907104, 0.44999998807907104, 0.9125000238418579, 0.6499999761581421, 0.625, 0.8999999761581421, 0.550000011920929, 0.7250000238418579, 0.0, 0.8999999761581421, 0.7875000238418579, 0.625, 0.550000011920929, 0.7749999761581421, 0.7250000238418579, 0.625, 0.9500000476837158, 0.675000011920929, 0.5, 0.44999998807907104, 0.5249999761581421, 0.699999988079071, 0.75, 0.625, 0.699999988079071, 0.7999999523162842, 0.30000001192092896, 0.050000011920928955, 0.9249999523162842, 0.6499999761581421, 0.8500000238418579, 0.9249999523162842, 0.75, 0.675000011920929, 0.675000011920929, 0.875, 0.8999999761581421, 0.5, 0.9249999523162842, 0.9750000238418579, 0.0, 0.8999999761581421, 0.5249999761581421, 0.0, 0.19999998807907104, 0.42500001192092896, 0.8500000238418579, 0.625, 0.699999988079071, 0.675000011920929, 0.7250000238418579, 0.0, 0.5, 0.025000005960464478, 0.5412499904632568, 0.3500000238418579, 0.30000001192092896, 0.625, 0.0, 0.9249999523162842, 0.5537499785423279, 0.5249999761581421, 0.19999998807907104, 0.4750000238418579, 0.025000005960464478, 0.6499999761581421, 0.5249999761581421, 0.8500000238418579, 0.625, 0.875, 0.675000011920929, 0.9587500095367432, 0.7999999523162842, 0.42500001192092896, 0.44999998807907104, 0.7250000238418579, 0.6499999761581421, 0.75, 0.4750000238418579, 1.0, 0.30000001192092896, 0.375, 0.3500000238418579, 0.025000005960464478, 0.44999998807907104, 0.8250000476837158, 0.9750000238418579, 0.3999999761581421, 0.8500000238418579, 0.9750000238418579, 1.0, 0.8500000238418579, 0.875, 1.0, 0.8500000238418579, 0.7250000238418579, 0.625, 0.5249999761581421, 0.625, 0.7250000238418579, 0.7250000238418579, 0.675000011920929, 0.8500000238418579, 0.25, 0.9500000476837158, 0.8500000238418579, 0.8162499666213989, 0.9750000238418579, 0.9500000476837158, 0.09999999403953552, 0.19999998807907104, 0.9750000238418579, 0.75, 0.7999999523162842, 0.5, 0.6499999761581421, 0.9249999523162842, 0.4750000238418579, 0.125, 0.875, 0.625, 0.7999999523162842, 0.675000011920929, 0.75, 0.625, 0.9750000238418579, 0.32499998807907104, 0.875, 0.4337499737739563, 0.3999999761581421, 0.6499999761581421, 0.5249999761581421, 0.6499999761581421, 0.6499999761581421, 0.9249999523162842, 0.4750000238418579, 0.44999998807907104, 0.4750000238418579, 0.6000000238418579, 0.625, 0.5249999761581421, 0.5249999761581421, 0.42500001192092896, 0.8500000238418579, 0.8250000476837158, 0.5962499976158142, 0.5, 0.5, 0.7250000238418579, 0.4750000238418579, 0.7250000238418579, 0.75, 0.625, 0.9750000238418579, 0.6499999761581421, 0.574999988079071, 0.75, 0.45875000953674316, 0.8250000476837158, 0.5, 0.550000011920929, 0.7749999761581421, 0.4750000238418579, 0.75, 0.675000011920929, 0.8500000238418579, 0.7749999761581421, 0.8250000476837158, 0.7999999523162842, 0.7662500143051147, 0.7999999523162842, 0.6000000238418579, 0.7250000238418579, 0.42500001192092896, 0.5249999761581421, 0.45875000953674316, 0.75, 0.9249999523162842, 0.75, 0.908750057220459, 0.6000000238418579, 0.9500000476837158, 0.4750000238418579, 0.5, 0.75, 0.550000011920929, 0.699999988079071, 0.75, 0.75, 0.9249999523162842, 0.675000011920929, 0.574999988079071, 0.09999999403953552, 0.550000011920929, 0.42500001192092896, 0.9500000476837158, 0.675000011920929, 0.875, 0.9500000476837158, 0.4750000238418579, 0.574999988079071, 0.8250000476837158, 0.7999999523162842, 0.550000011920929, 0.574999988079071, 0.5249999761581421, 0.32499998807907104, 0.42500001192092896, 0.375, 0.625, 0.8250000476837158, 0.4750000238418579, 0.5249999761581421, 0.7749999761581421, 0.7999999523162842, 0.44999998807907104, 0.9500000476837158, 0.09999999403953552, 0.9500000476837158, 0.7250000238418579, 0.3500000238418579, 0.7250000238418579, 0.42500001192092896, 0.0, 0.5249999761581421, 0.2749999761581421, 0.625, 0.15000000596046448, 0.025000005960464478, 0.699999988079071, 0.0, 0.07499998807907104, 0.6000000238418579, 0.7999999523162842, 0.9750000238418579, 0.25, 0.625, 0.6499999761581421, 0.7250000238418579, 0.6499999761581421, 0.875, 0.0, 0.025000005960464478, 0.7250000238418579, 0.9750000238418579, 0.574999988079071, 0.8999999761581421, 0.025000005960464478, 0.9750000238418579, 0.5787500143051147, 0.25, 0.675000011920929, 0.574999988079071, 0.699999988079071, 0.550000011920929, 0.8500000238418579, 0.9750000238418579, 0.6499999761581421, 0.625, 0.5249999761581421, 0.875, 0.0, 0.675000011920929, 0.25, 0.3500000238418579, 0.32499998807907104, 0.9500000476837158, 0.375, 0.625, 0.3500000238418579, 0.5, 0.7250000238418579, 0.7250000238418579, 0.875, 0.574999988079071, 0.7749999761581421, 0.8500000238418579, 0.7749999761581421, 0.6499999761581421, 0.574999988079071, 0.32499998807907104, 0.8500000238418579, 0.6462500095367432, 1.0, 0.8999999761581421, 0.6499999761581421, 0.5249999761581421, 0.875, 0.7250000238418579, 0.7962499856948853, 0.8500000238418579, 0.5249999761581421, 1.0, 0.675000011920929, 0.6499999761581421, 0.5412499904632568, 0.875, 0.7749999761581421, 0.3999999761581421, 0.6000000238418579, 0.30000001192092896, 0.9750000238418579, 0.625, 0.4750000238418579, 0.42500001192092896, 0.9500000476837158, 0.025000005960464478, 0.75, 0.7749999761581421, 0.675000011920929, 0.5, 0.8712500333786011, 0.4750000238418579, 0.025000005960464478, 0.7749999761581421, 0.9750000238418579, 0.375, 0.8500000238418579, 0.9249999523162842, 0.875, 0.7749999761581421, 0.3500000238418579, 0.6000000238418579, 0.675000011920929, 0.0, 0.550000011920929, 0.17500001192092896, 0.875, 0.675000011920929, 0.7250000238418579, 0.550000011920929, 0.0, 0.8999999761581421, 0.2749999761581421, 0.4750000238418579, 0.5249999761581421, 0.699999988079071, 0.22499999403953552, 0.6499999761581421, 0.574999988079071, 0.9750000238418579, 0.30000001192092896, 0.9500000476837158, 0.574999988079071, 0.44999998807907104, 0.025000005960464478, 1.0, 0.0, 0.75, 0.44999998807907104, 0.75, 0.7250000238418579, 0.550000011920929, 0.0, 0.5249999761581421, 0.0, 0.4750000238418579, 0.6499999761581421, 0.625, 0.9249999523162842, 0.75, 0.699999988079071, 0.875, 0.9500000476837158, 0.32499998807907104, 0.5787500143051147, 0.699999988079071, 0.9750000238418579, 0.6499999761581421, 0.7250000238418579, 0.5, 0.574999988079071, 0.625, 1.0, 0.025000005960464478, 0.025000005960464478, 0.675000011920929, 0.8250000476837158, 0.09999999403953552, 0.7999999523162842, 0.8500000238418579, 0.574999988079071, 0.125, 0.7999999523162842, 0.875, 0.09999999403953552, 0.32499998807907104, 0.7250000238418579, 0.7749999761581421, 0.8500000238418579, 0.9750000238418579, 0.6000000238418579, 0.9500000476837158, 0.7250000238418579, 0.625, 0.5, 0.9750000238418579, 0.8500000238418579, 0.75, 0.875, 0.6499999761581421, 0.75, 0.737500011920929, 0.125, 0.675000011920929, 0.8250000476837158, 0.7999999523162842, 0.8250000476837158, 0.025000005960464478, 0.7250000238418579, 0.625, 0.7250000238418579, 0.675000011920929, 0.4750000238418579, 0.875, 0.44999998807907104, 1.0, 0.75, 0.7749999761581421, 0.0, 0.8999999761581421, 0.75, 0.6962500214576721, 0.0, 0.9249999523162842, 0.8999999761581421, 0.625, 0.875, 0.4750000238418579, 0.75, 0.19999998807907104, 0.625, 0.7999999523162842, 0.9750000238418579, 0.9750000238418579, 0.7749999761581421, 0.7999999523162842, 0.19999998807907104, 0.875, 0.75, 0.675000011920929, 0.0, 0.550000011920929, 0.7250000238418579, 0.574999988079071, 0.75, 0.8624999523162842, 0.9750000238418579, 0.375, 0.6000000238418579, 0.025000005960464478, 0.8999999761581421, 0.9500000476837158, 0.7999999523162842, 0.675000011920929, 0.8500000238418579, 0.550000011920929, 0.625, 0.625, 0.0, 0.19999998807907104, 0.75, 0.3500000238418579, 0.22499999403953552, 0.025000005960464478, 0.7999999523162842, 0.25, 0.0, 0.7999999523162842, 0.025000005960464478, 0.17500001192092896, 0.6625000238418579, 0.7250000238418579, 0.6000000238418579, 0.375, 0.699999988079071, 0.4750000238418579, 0.8999999761581421, 0.675000011920929, 0.75, 0.7749999761581421, 0.07499998807907104, 0.7749999761581421, 0.625, 0.7250000238418579, 0.4750000238418579, 0.5, 0.7999999523162842, 0.875, 0.9249999523162842, 0.30000001192092896, 0.375, 0.6000000238418579, 0.0, 0.9500000476837158, 0.8999999761581421, 0.42500001192092896, 0.125, 0.7749999761581421, 0.17500001192092896, 0.9750000238418579, 0.375, 0.875, 0.42500001192092896, 0.7250000238418579, 0.7749999761581421, 0.8250000476837158, 0.42500001192092896, 0.8999999761581421, 0.11250001192092896, 0.574999988079071, 0.5, 0.25, 0.8500000238418579, 0.125, 0.625, 0.7749999761581421, 0.8587499856948853, 0.7999999523162842, 0.4750000238418579, 0.0, 0.8500000238418579, 0.025000005960464478, 0.550000011920929, 0.7749999761581421, 0.44999998807907104, 0.875, 0.75, 0.699999988079071, 1.0, 0.9249999523162842, 0.30000001192092896, 0.9249999523162842, 0.6499999761581421, 0.550000011920929, 0.0, 0.9500000476837158, 0.574999988079071, 0.9750000238418579, 0.9500000476837158, 0.42500001192092896, 0.675000011920929, 1.0, 0.9500000476837158, 0.9750000238418579, 0.8250000476837158, 0.25, 0.9500000476837158, 0.625, 0.7749999761581421, 0.875, 0.7999999523162842, 0.6000000238418579, 0.0, 0.30000001192092896, 0.375, 0.8999999761581421, 0.875, 0.75, 0.44999998807907104, 0.574999988079071, 0.9249999523162842, 0.7250000238418579, 0.5212500095367432, 0.7999999523162842, 0.625, 0.675000011920929, 0.8250000476837158, 0.9500000476837158, 0.9750000238418579, 0.9750000238418579, 0.30000001192092896, 0.0, 0.0, 0.6499999761581421, 0.699999988079071, 0.8250000476837158, 0.9500000476837158, 0.25, 0.7749999761581421, 0.675000011920929, 0.6000000238418579, 0.875, 0.6000000238418579, 0.6499999761581421, 0.125, 0.3999999761581421, 0.625, 0.625, 0.7250000238418579, 0.8250000476837158, 0.3500000238418579, 0.6000000238418579, 0.550000011920929, 1.0, 0.6000000238418579, 0.7749999761581421, 0.699999988079071, 0.30000001192092896, 0.75, 0.22499999403953552, 0.625, 0.9750000238418579, 0.7250000238418579, 0.8500000238418579, 0.550000011920929, 0.5249999761581421, 0.8500000238418579, 0.7999999523162842, 0.9500000476837158, 0.675000011920929, 0.9500000476837158, 0.3999999761581421, 0.574999988079071, 0.44999998807907104, 1.0, 0.8999999761581421, 0.3500000238418579, 0.7749999761581421, 0.4750000238418579, 0.375, 0.9500000476837158, 0.5249999761581421, 0.75, 1.0, 0.3999999761581421, 0.875, 0.3999999761581421, 0.625, 0.550000011920929, 0.9750000238418579, 0.5, 0.8500000238418579, 0.699999988079071, 0.625, 0.3500000238418579, 0.6000000238418579, 0.9500000476837158, 0.6000000238418579, 0.875, 0.3999999761581421, 0.574999988079071, 0.675000011920929, 0.0, 0.025000005960464478, 0.675000011920929, 0.699999988079071, 0.699999988079071, 0.17500001192092896, 0.9249999523162842, 0.699999988079071, 0.5249999761581421, 0.699999988079071, 0.550000011920929, 0.574999988079071, 0.574999988079071, 0.9750000238418579, 0.675000011920929, 0.9249999523162842, 0.44999998807907104, 0.6000000238418579, 0.025000005960464478, 0.699999988079071, 0.550000011920929, 0.4750000238418579, 0.9750000238418579, 0.625, 0.675000011920929, 0.699999988079071, 0.8500000238418579, 0.675000011920929, 0.3999999761581421, 0.7999999523162842, 0.699999988079071, 0.5, 0.699999988079071, 0.737500011920929, 0.6499999761581421, 0.7250000238418579, 0.574999988079071, 0.7250000238418579, 1.0, 0.675000011920929, 0.4712499976158142, 0.3500000238418579, 0.8250000476837158, 0.7749999761581421, 0.5249999761581421, 1.0, 0.7999999523162842, 0.5249999761581421, 0.5, 0.875, 0.6499999761581421, 0.6000000238418579, 0.625, 0.5249999761581421, 0.699999988079071, 0.8500000238418579, 0.7250000238418579, 0.8500000238418579, 0.22499999403953552, 0.6337500214576721, 0.574999988079071, 0.7250000238418579, 0.75, 0.7250000238418579, 0.0, 0.9750000238418579, 0.375, 0.5162500143051147, 0.6000000238418579, 0.625, 0.6499999761581421, 0.699999988079071, 0.8250000476837158, 0.675000011920929, 0.5, 0.4750000238418579, 0.574999988079071, 0.7749999761581421, 0.15000000596046448, 0.75, 0.025000005960464478, 0.8250000476837158, 0.5, 0.875, 0.2749999761581421, 0.3999999761581421, 0.675000011920929, 1.0, 0.32499998807907104, 0.699999988079071, 0.050000011920928955, 0.6000000238418579, 0.42500001192092896, 0.0, 0.5, 0.07499998807907104, 0.7999999523162842, 0.44999998807907104, 0.7912499904632568, 0.7037500143051147, 0.8999999761581421, 0.5, 0.025000005960464478, 1.0, 0.0, 0.9500000476837158, 0.625, 0.675000011920929, 0.0, 0.9249999523162842, 0.0, 0.6499999761581421, 0.4750000238418579, 1.0, 0.0, 0.9750000238418579, 1.0, 0.8999999761581421, 0.7087500095367432, 0.3999999761581421, 0.6000000238418579, 0.574999988079071, 0.3500000238418579, 0.7999999523162842, 0.625, 0.7250000238418579, 0.32499998807907104, 0.7999999523162842, 0.6499999761581421, 0.875, 0.050000011920928955, 0.30000001192092896, 0.15000000596046448, 0.699999988079071, 0.7250000238418579, 0.7749999761581421, 0.9750000238418579, 0.025000005960464478, 0.42500001192092896, 0.6499999761581421, 0.9750000238418579, 0.25, 0.6000000238418579, 0.0, 0.6000000238418579, 0.5, 0.32499998807907104, 0.5, 0.6499999761581421, 0.9249999523162842, 0.7749999761581421, 0.5249999761581421, 0.550000011920929, 0.625, 0.875, 0.6000000238418579, 0.42500001192092896, 0.0, 0.75, 0.3999999761581421, 0.8999999761581421, 0.875, 0.7749999761581421, 0.8999999761581421, 0.7999999523162842, 0.875, 0.9750000238418579, 0.574999988079071, 0.550000011920929, 0.25, 0.7250000238418579, 0.7287499904632568, 0.7250000238418579, 0.17500001192092896, 0.574999988079071, 0.9500000476837158, 0.25, 0.0, 0.574999988079071, 0.8250000476837158, 0.6499999761581421, 0.4750000238418579, 0.5, 0.6000000238418579, 0.9249999523162842, 0.0, 0.7250000238418579, 0.9500000476837158, 0.6000000238418579, 0.9500000476837158, 0.625, 0.675000011920929, 0.675000011920929, 0.2749999761581421, 0.8250000476837158, 0.8999999761581421, 0.9500000476837158, 0.5249999761581421, 0.8250000476837158, 0.550000011920929, 1.0, 0.6000000238418579, 0.3500000238418579, 0.675000011920929, 0.025000005960464478, 0.8500000238418579, 0.550000011920929, 0.9500000476837158, 0.75, 0.7999999523162842, 0.8500000238418579, 0.22499999403953552, 0.7749999761581421, 0.875, 0.025000005960464478, 0.5249999761581421, 0.625, 0.9750000238418579, 0.6499999761581421, 0.5, 0.17500001192092896, 0.7749999761581421, 0.9249999523162842, 0.9500000476837158, 0.7250000238418579, 0.5, 1.0, 0.7749999761581421, 0.8500000238418579, 0.8999999761581421, 0.8500000238418579, 0.5, 1.0, 0.699999988079071, 0.9500000476837158, 0.3500000238418579, 0.675000011920929, 0.9750000238418579, 0.09999999403953552, 0.6499999761581421, 0.550000011920929, 0.6000000238418579, 0.9500000476837158, 0.6000000238418579, 0.44999998807907104, 0.09999999403953552, 0.8250000476837158, 0.75, 0.4750000238418579, 0.875, 0.7999999523162842, 0.675000011920929, 0.15000000596046448, 0.625, 0.8500000238418579, 0.625, 0.8999999761581421, 0.8999999761581421, 0.7250000238418579, 0.550000011920929, 0.42500001192092896, 0.8500000238418579, 0.574999988079071, 0.0, 0.625, 0.0, 0.7999999523162842, 0.550000011920929, 0.32499998807907104, 0.5, 0.42500001192092896, 0.9750000238418579, 0.574999988079071, 0.875, 0.3999999761581421, 0.6000000238418579, 0.75, 0.7999999523162842, 0.625, 1.0, 0.3999999761581421, 0.44999998807907104, 0.675000011920929, 0.550000011920929, 0.699999988079071, 0.32499998807907104, 0.8250000476837158, 0.9750000238418579, 0.675000011920929, 0.6499999761581421, 0.574999988079071, 0.6499999761581421, 0.6000000238418579, 0.9500000476837158, 0.6499999761581421, 0.0, 0.050000011920928955, 0.875, 0.875, 0.550000011920929, 0.574999988079071, 0.5249999761581421, 0.42500001192092896, 0.8250000476837158, 0.7250000238418579, 0.9500000476837158, 0.32499998807907104, 0.9750000238418579, 0.7250000238418579, 0.44999998807907104, 0.0, 0.9249999523162842, 1.0, 0.9500000476837158, 0.8250000476837158, 0.8999999761581421, 0.6499999761581421, 0.7749999761581421, 0.9750000238418579, 0.625, 0.30000001192092896, 0.7749999761581421, 0.7462499737739563, 0.8500000238418579, 0.4750000238418579, 0.6000000238418579, 0.699999988079071, 0.625, 0.6000000238418579, 0.675000011920929, 0.0, 0.7749999761581421, 0.7250000238418579, 0.9249999523162842, 1.0, 0.875, 0.9750000238418579, 0.7999999523162842, 0.8250000476837158, 0.675000011920929, 0.9500000476837158, 0.0, 0.15000000596046448, 0.574999988079071, 0.5249999761581421, 0.7749999761581421, 0.09999999403953552, 0.375, 1.0, 0.8250000476837158, 0.7749999761581421, 0.8374999761581421, 0.699999988079071, 0.8999999761581421, 0.625, 0.9500000476837158, 0.5249999761581421, 0.375, 0.574999988079071, 0.8250000476837158, 0.25, 0.6499999761581421, 0.6499999761581421, 0.75, 0.17500001192092896, 0.699999988079071, 0.9249999523162842, 0.574999988079071, 0.6499999761581421, 0.675000011920929, 0.6000000238418579, 1.0, 0.6712499856948853, 0.699999988079071, 0.875, 0.5, 0.675000011920929, 0.675000011920929, 0.7250000238418579, 0.7250000238418579, 0.75, 0.9500000476837158, 0.875, 0.3999999761581421, 0.42124998569488525, 0.32499998807907104, 0.7999999523162842, 0.875, 0.75, 0.7749999761581421, 0.9249999523162842, 0.675000011920929, 0.6000000238418579, 0.8500000238418579, 0.9249999523162842, 0.050000011920928955, 0.5249999761581421, 1.0, 0.6000000238418579, 0.7749999761581421, 0.9249999523162842, 0.09999999403953552, 1.0, 0.0, 0.875, 0.7250000238418579, 0.675000011920929, 0.699999988079071, 0.19999998807907104, 0.574999988079071, 0.7999999523162842, 0.42500001192092896, 0.75, 0.574999988079071, 0.5, 0.8999999761581421, 0.8999999761581421, 0.6499999761581421, 0.675000011920929, 0.7124999761581421, 0.7999999523162842, 0.9249999523162842, 0.4750000238418579, 0.9500000476837158, 0.32499998807907104, 0.6499999761581421, 0.625, 0.9750000238418579, 1.0, 0.8500000238418579, 0.9750000238418579, 0.7999999523162842, 0.675000011920929, 0.0, 0.4750000238418579, 0.625, 0.9750000238418579, 0.75, 0.6499999761581421, 0.6499999761581421, 0.8500000238418579, 0.5, 0.675000011920929, 0.6000000238418579, 0.8500000238418579, 0.5249999761581421, 0.75, 0.5249999761581421, 0.7999999523162842, 0.32499998807907104, 0.32499998807907104, 0.675000011920929, 0.625, 0.8250000476837158, 0.8999999761581421, 0.9500000476837158, 0.550000011920929, 0.875, 0.050000011920928955, 0.8250000476837158, 0.375, 0.875, 0.574999988079071, 0.574999988079071, 0.6000000238418579, 0.07499998807907104, 0.625, 0.5249999761581421, 0.8250000476837158, 0.8500000238418579, 0.6000000238418579, 0.550000011920929, 0.875, 0.550000011920929, 0.550000011920929, 0.875, 0.7749999761581421, 0.15000000596046448, 0.550000011920929, 0.9249999523162842, 0.699999988079071, 0.7250000238418579, 0.5249999761581421, 0.3999999761581421, 0.7999999523162842, 0.8500000238418579, 0.9500000476837158, 0.9750000238418579, 0.4750000238418579, 0.4750000238418579, 0.4750000238418579, 0.8500000238418579, 0.550000011920929, 0.25, 0.3787500262260437, 0.9750000238418579, 0.8250000476837158, 0.4750000238418579, 0.7749999761581421, 0.574999988079071, 0.025000005960464478, 0.7999999523162842, 0.42500001192092896, 0.6000000238418579, 0.7250000238418579, 0.025000005960464478, 0.32499998807907104, 0.9750000238418579, 0.550000011920929, 0.6000000238418579, 0.7250000238418579, 0.699999988079071, 0.9500000476837158, 1.0, 0.7250000238418579, 0.625, 0.75, 0.9500000476837158, 0.550000011920929, 0.675000011920929, 0.75, 0.4037500023841858, 0.6587499976158142, 0.625, 0.6000000238418579, 0.025000005960464478, 0.8250000476837158, 0.625, 0.17500001192092896, 0.9750000238418579, 0.7250000238418579, 0.6000000238418579, 1.0, 0.9500000476837158, 0.07499998807907104, 0.675000011920929, 0.699999988079071, 0.5249999761581421, 0.699999988079071, 0.903749942779541]\n",
    "ypred=[0.9999988, 0.9999837, 0.9999974, 0.99999166, 0.99999475, 0.9999974, 0.9999961, 0.9999685, 0.9999968, 0.99999815, 0.9999996, 0.9999901, 0.9999945, 0.9999963, 0.9999972, 0.99999714, 0.99999595, 0.99999577, 0.9999936, 0.9999924, 0.99999654, 0.9999925, 0.99998695, 0.99999815, 0.9999993, 0.99997586, 0.99998844, 0.9999964, 0.9999906, 0.9999899, 0.9999985, 0.9999966, 0.9999891, 0.9999941, 0.9999893, 0.9999989, 0.9999985, 0.99999475, 0.99998903, 0.999964, 0.9999953, 0.99999887, 0.9999962, 0.99997735, 0.9999757, 0.9999972, 0.9999874, 0.99999917, 0.99999756, 0.9999972, 0.9999648, 0.9999926, 0.9999925, 0.9999871, 0.9999964, 0.9999977, 0.9999977, 0.9999952, 0.9999985, 0.9999988, 0.9999935, 0.9999977, 0.9999945, 0.99999297, 0.9999822, 0.9999983, 0.99999744, 0.9999983, 0.99999374, 0.9999941, 0.9999984, 0.99999726, 0.99999607, 0.99999803, 0.9999949, 0.9999939, 0.9999979, 0.9999925, 0.9999859, 0.99999803, 0.9999968, 0.9999968, 0.9999959, 0.9999978, 0.9999949, 0.9999955, 0.99998283, 0.9999987, 0.9999993, 0.9999988, 0.9999887, 0.99999565, 0.9999998, 0.9999873, 0.999998, 0.9999984, 0.99998933, 0.99999976, 0.9999816, 0.999998, 0.99999046, 0.9999977, 0.9999899, 0.9999992, 0.99999845, 0.999978, 0.9999988, 0.999998, 0.999999, 0.9999912, 0.9999988, 0.999994, 0.9999981, 0.9999814, 0.9999962, 0.9999985, 0.9999973, 0.9999983, 0.9999987, 0.99999565, 0.999975, 0.9999838, 0.9999938, 0.9999932, 0.99999595, 0.99999845, 0.99999934, 0.99999577, 0.9999953, 0.99999845, 0.99997044, 0.9999968, 0.9999814, 0.99997944, 0.99999577, 0.99999577, 0.9999946, 0.99998945, 0.9999945, 0.99999195, 0.9999949, 0.9999977, 0.9999982, 0.9999985, 0.999991, 0.9999986, 0.9999966, 0.999994, 0.9999967, 0.99999386, 0.99998564, 0.9999981, 0.9999983, 0.99998885, 0.9991465, 0.9999982, 0.9999939, 0.99999833, 0.99998206, 0.9999936, 0.99999815, 0.99999577, 0.9999807, 0.99999684, 0.99999803, 0.99999136, 0.99999017, 0.99999803, 0.99999946, 0.99999845, 0.9999968, 0.99998486, 0.9999939, 0.9999979, 0.9999889, 0.9999967, 0.99999905, 0.9999967, 0.9999944, 0.99999475, 0.9999972, 0.9999852, 0.9999939, 0.9999958, 0.9999963, 0.99999326, 0.9999956, 0.99999887, 0.9999916, 0.99999774, 0.99998045, 0.9999982, 0.99999726, 0.9999952, 0.9999964, 0.9999867, 0.99997246, 0.99999875, 0.9999554, 0.99999666, 0.9999625, 0.999995, 0.99999964, 0.99999875, 0.99999624, 0.99999195, 0.99998945, 0.99998784, 0.9999992, 0.99999845, 0.9999816, 0.99998766, 0.9999937, 0.99999547, 0.99996465, 0.9999909, 0.9999642, 0.99999887, 0.999997, 0.9999994, 0.99999666, 0.99999726, 0.9999963, 0.9999976, 0.9999891, 0.99998474, 0.99999243, 0.9999976, 0.99999654, 0.99994534, 0.999999, 0.9999949, 0.9999932, 0.99999493, 0.9999872, 0.9999997, 0.9999933, 0.9999955, 0.99999636, 0.99998873, 0.9999992, 0.99999565, 0.9999955, 0.9999907, 0.99999535, 0.9999964, 0.9999806, 0.9999968, 0.99999535, 0.99999297, 0.9999995, 0.99999094, 0.999982, 0.999998, 0.9999945, 0.999999, 0.99999934, 0.9999668, 0.9999852, 0.99999756, 0.9999527, 0.99999344, 0.99998856, 0.9999851, 0.9999932, 0.9999928, 0.9999928, 0.99999064, 0.99999607, 0.9999891, 0.9999969, 0.9999889, 0.9999968, 0.99999243, 0.9999985, 0.99999726, 0.99999225, 0.9999972, 0.99992925, 0.9999949, 0.9999943, 0.9999849, 0.99999905, 0.9999974, 0.9999909, 0.99999344, 0.9999951, 0.99999505, 0.99999106, 0.99999684, 0.9999958, 0.9999869, 0.9999763, 0.9999851, 0.9999836, 0.99998224, 0.99998176, 0.9999922, 0.99999017, 0.999991, 0.9999107, 0.9999943, 0.99998176, 0.9999966, 0.9999938, 0.9999954, 0.99999106, 0.9999825, 0.9999944, 0.99999785, 0.9999846, 0.99999523, 0.9999987, 0.9999992, 0.9999989, 0.99999917, 0.99999857, 0.9999813, 0.99996936, 0.99999785, 0.9999971, 0.9999668, 0.9999917, 0.99999607, 0.99998605, 0.9999807, 0.9999912, 0.99999857, 0.9999951, 0.9999859, 0.9999982, 0.9999989, 0.99999446, 0.99998087, 0.9999962, 0.999998, 0.99999875, 0.9999949, 0.99995863, 0.99984497, 0.9999963, 0.99999386, 0.99997824, 0.9999983, 0.9999915, 0.99999064, 0.99999535, 0.9978352, 0.99998486, 0.9997177, 0.99999714, 0.99999356, 0.9999842, 0.9999556, 0.99999934, 0.99999636, 0.9999937, 0.999998, 0.9999882, 0.9999937, 0.99999297, 0.9999361, 0.999998, 0.99997157, 0.99999654, 0.9998682, 0.99999464, 0.99999213, 0.99996203, 0.999994, 0.9999939, 0.9999966, 0.99999, 0.9999341, 0.9999975, 0.99999654, 0.9997805, 0.99998206, 0.99997526, 0.9999916, 0.99998474, 0.9999878, 0.9999908, 0.9999866, 0.9996382, 0.999236, 0.99894744, 0.9998952, 0.99998873, 0.99948823, 0.99996674, 0.9992447, 0.9999298, 0.9945143, 0.99999774, 0.9998923, 0.9985976, 0.99999344, 0.99996746, 0.9992723, 0.99745214, 0.9998011, 0.9999772, 0.999689, 0.99993575, 0.99974424, 0.9999936, 0.99998677, 0.99998295, 0.99995357, 0.9999969, 0.99999696, 0.9999718, 0.99993616, 0.99996257, 0.9999872, 0.9999952, 0.9989417, 0.9999865, 0.9999601, 0.9955116, 0.9993906, 0.9999967, 0.99962187, 0.9999842, 0.99989843, 0.999994, 0.9997903, 0.9999743, 0.99998736, 0.9997399, 0.99990493, 0.9999844, 0.99998504, 0.9999586, 0.99999547, 0.9983244, 0.9961666, 0.9999938, 0.9999258, 0.99982494, 0.9997226, 0.9999949, 0.9999799, 0.99990684, 0.9981809, 0.9996892, 0.9999936, 0.7926711, 0.9127214, 0.80123675, 0.791966, 0.89183307, 0.9957058, 0.17615764, 0.94820553, 0.9980536, 0.9972411, 0.9965645, 0.7440102, 0.9106376, 0.6130082, 0.90410376, 0.9932097, 0.82065505, 0.9921753, 0.9652073, 0.48738047, 0.26238927, 0.71213394, 0.9949823, 0.9437686, 0.9999768, 0.99962103, 0.36033788, 0.784912, 0.99807364, 0.99312294, 0.9437963, 0.87896335, 0.99830645, 0.784768, 0.92095405, 0.97334564, 0.8657241, 0.4923862, 0.9256557, 0.9992386, 0.5447594, 0.9988599, 0.9967043, 0.8960933, 0.9976293, 0.97400016, 0.1544312, 0.3114926, 0.6239829, 0.9981858, 0.62776434, 0.9410082, 0.1971162, 0.93526417, 0.9980768, 0.9683735, 0.99606127, 0.99900365, 0.9905434, 0.9987325, 0.9999772, 0.99388635, 0.9846345, 0.7578895, 0.9773015, 0.9992299, 0.9248372, 0.99926746, 0.778149, 0.99720865, 0.9874858, 0.95100826, 0.90131605, 0.93062437, 0.9807899, 0.99678797, 0.9998378, 0.88994515, 0.9858517, 0.9529261, 0.9779117, 0.9998998, 0.41352183, 0.9895791, 0.9639318, 0.9980044, 0.9988342, 0.94649273, 0.64898986, 0.9240145, 0.99976975, 0.9851047, 0.86232233, 0.8572263, 0.9969015, 0.9808555, 0.96430635, 0.9999547, 0.99957615, 0.897326, 0.4924323, 0.749964, 0.95588404, 0.89653563, 0.97445714, 0.99885875, 0.9761912, 0.42292035, 0.9985901, 0.9055683, 0.33172455, 0.9938372, 0.999399, 0.71229964, 0.9217478, 0.9853827, 0.79679984, 0.99992436, 0.2043743, 0.99981797, 0.99897397, 0.8075388, 0.9999341, 0.8651693, 0.87878865, 0.78916013, 0.99425906, 0.9521053, 0.9999873, 0.9999855, 0.9919369, 0.99998087, 0.9998898, 0.9999438, 0.9888268, 0.99997914, 0.9987441, 0.9999791, 0.99997, 0.99998426, 0.99825597, 0.9999938, 0.99999744, 0.99948794, 0.99976534, 0.9998475, 0.9999257, 0.999949, 0.9999888, 0.9999143, 0.99999356, 0.99617726, 0.96362686, 0.99998593, 0.9999888, 0.99983317, 0.9953561, 0.9998382, 0.99997675, 0.9990783, 0.99984795, 0.9996075, 0.9998062, 0.99999714, 0.9999154, 0.999299, 0.99982727, 0.96489877, 0.9999932, 0.98590374, 0.94371563, 0.99981916, 0.9999943, 0.99302965, 0.99448955, 0.999995, 0.99998677, 0.99851555, 0.9998801, 0.9999465, 0.87810636, 0.9979252, 0.9998831, 0.90145016, 0.9986178, 0.90426123, 0.98506045, 0.99998367, 0.99999547, 0.9996731, 0.99943435, 0.9999236, 0.99997133, 0.99818385, 0.999443, 0.9989162, 0.96579045, 0.99507976, 0.98785603, 0.99955916, 0.9998133, 0.99993473, 0.99976546, 0.99997056, 0.99677, 0.91540647, 0.999998, 0.9999988, 0.9999929, 0.999956, 0.99992234, 0.99572736, 0.99983567, 0.997325, 0.9999991, 0.9995936, 0.9989693, 0.9997611, 0.98736984, 0.9322447, 0.9876125, 0.9636743, 0.99995416, 0.99997467, 0.9993743, 0.99982315, 0.9985, 0.99925953, 0.99844766, 0.9974287, 0.9977028, 0.9997938, 0.998932, 0.9570502, 0.99410003, 0.9998934, 0.99911535, 0.9985042, 0.99985427, 0.8997781, 0.9999886, 0.9889254, 0.99950445, 0.99969757, 0.99984837, 0.9994519, 0.99985904, 0.999946, 0.9987871, 0.9998881, 0.99957174, 0.9984227, 0.9959712, 0.9999926, 0.9997166, 0.99270207, 0.99313277, 0.3046288, 0.98599184, 0.14546162, 0.4323948, 0.99561596, 0.28864154, 0.5894424, 0.759013, 0.85641974, 0.9946926, 0.23064923, 0.9988425, 0.977821, 0.63412195, 0.93513423, 0.98772764, 0.9700119, 0.99999434, 0.99996585, 0.9999272, 0.99880815, 0.9996765, 0.9966573, 0.42536137, 0.4934576, 0.9866301, 0.9800529, 0.9820841, 0.9731428, 0.98776513, 0.37833485, 0.74250793, 0.5308731, 0.99567056, 0.9677944, 0.99991053, 0.34759882, 0.9912449, 0.21506517, 0.25194874, 0.9896051, 0.88131636, 0.99970907, 0.99070084, 0.5025738, 0.8654409, 0.9939276, 0.9985005, 0.5288318, 0.6835018, 0.9749945, 0.2491923, 0.90474176, 0.9867497, 0.99977744, 0.9999585, 0.6064349, 0.9011463, 0.79976314, 0.99978507, 0.34480396, 0.99996954, 0.86333644, 0.13648054, 0.9978095, 0.9901679, 0.99997556, 0.8312269, 0.62350214, 0.9169751, 0.9997866, 0.20962068, 0.14783384, 0.95807153, 0.52213496, 0.1453583, 0.25011715, 0.15745829, 0.8788889, 0.9514016, 0.16952205, 0.15043335, 0.6769637, 0.2454213, 0.30582997, 0.6127762, 0.17724235, 0.27751258, 0.94994, 0.572481, 0.9791194, 0.97336143, 0.39850357, 0.16863735, 0.76951295, 0.88565856, 0.23660874, 0.3900958, 0.7173171, 0.1664953, 0.39194828, 0.20675623, 0.6552783, 0.9631799, 0.84731424, 0.17587923, 0.1430609, 0.13914424, 0.5267222, 0.16330674, 0.5335279, 0.48341972, 0.15466915, 0.5274232, 0.9159076, 0.8820719, 0.2931374, 0.28839386, 0.14691207, 0.59833735, 0.14737979, 0.84132934, 0.26426637, 0.15579936, 0.357572, 0.6307471, 0.42674643, 0.99487174, 0.9729085, 0.9549393, 0.9997708, 0.9998063, 0.71645474, 0.9997566, 0.9859607, 0.9998125, 0.999997, 0.99997145, 0.9999798, 0.9997856, 0.9999889, 0.9886722, 0.97939575, 0.9997654, 0.9999109, 0.99335647, 0.9999776, 0.8641797, 0.9999866, 0.9941136, 0.9541127, 0.9999948, 0.99972945, 0.7470181, 0.9999912, 0.99998003, 0.9554541, 0.98986036, 0.9912244, 0.9989481, 0.8137157, 0.9021909, 0.99187994, 0.69056135, 0.99618316, 0.924805, 0.990732, 0.22790456, 0.99985826, 0.9993405, 0.62380254, 0.9999945, 0.9998697, 0.9975191, 0.99976856, 0.60562557, 0.6131294, 0.9995702, 0.95981866, 0.98771065, 0.3508517, 0.99803174, 0.8831473, 0.9999983, 0.2702874, 0.9982694, 0.28552893, 0.99728674, 0.92469525, 0.9317226, 0.9998352, 0.9999872, 0.9997215, 0.9997603, 0.9996867, 0.9997368, 0.9828708, 0.99999213, 0.9999563, 0.99993604, 0.9987454, 0.99995404, 0.9999965, 0.999994, 0.99995303, 0.9999676, 0.99996334, 0.9999963, 0.99984527, 0.99999064, 0.9880754, 0.99949414, 0.9999379, 0.9999507, 0.99999565, 0.9997775, 0.9999147, 0.76737356, 0.99959064, 0.99989116, 0.9999851, 0.99999005, 0.9997667, 0.9999949, 0.9999556, 0.9998935, 0.999638, 0.99987507, 0.9999889, 0.9999885, 0.9999955, 0.99968404, 0.999994, 0.9999461, 0.99754256, 0.9999976, 0.9995571, 0.99971086, 0.99999315, 0.9999858, 0.9998567, 0.9999902, 0.9975553, 0.99962205, 0.99998236, 0.999929, 0.99980783, 0.9997536, 0.9988853, 0.9999862, 0.9990232, 0.9999942, 0.99998564, 0.9999653, 0.9999898, 0.99999374, 0.99997944, 0.99999774, 0.99999, 0.9999979, 0.9999647, 0.99989414, 0.9999955, 0.99996346, 0.9999957, 0.9999897, 0.9997427, 0.999876, 0.98285586, 0.9995472, 0.9999945, 0.99997085, 0.9999703, 0.99998534, 0.99999356, 0.9999876, 0.9999949, 0.9999948, 0.99987227, 0.99999666, 0.9999677, 0.9999866, 0.9999776, 0.99999046, 0.9988277, 0.9999582, 0.99996734, 0.99998254, 0.9999967, 0.99999195, 0.9999942, 0.9999772, 0.9999832, 0.9999925, 0.9999673, 0.9999501, 0.9999876, 0.99999386, 0.9999957, 0.99997175, 0.9999851, 0.9998951, 0.9999842, 0.99999934, 0.99999094, 0.99999285, 0.9999869, 0.99999315, 0.999972, 0.9999978, 0.99995464, 0.999945, 0.999993, 0.99985087, 0.9999545, 0.99987257, 0.99999535, 0.99775827, 0.99996364, 0.99999094, 0.9999897, 0.9999691, 0.99998, 0.99999106, 0.99999756, 0.99997216, 0.9999962, 0.9999956, 0.9999809, 0.99973863, 0.99998677, 0.9999921, 0.9999756, 0.9999765, 0.9999641, 0.99999475, 0.9999916, 0.9999603, 0.99996746, 0.9999992, 0.99996763, 0.99999744, 0.9999406, 0.9999961, 0.99999946, 0.9999983, 0.99999535, 0.9999913, 0.9999902, 0.9999929, 0.99999326, 0.9999946, 0.99999493, 0.9999942, 0.9999959, 0.99998915, 0.9999973, 0.9999965, 0.9999994, 0.99998885, 0.99999785, 0.99996626, 0.9999595, 0.99998915, 0.99998665, 0.99999654, 0.9999874, 0.9999807, 0.9999804, 0.9999875, 0.99997926, 0.9999851, 0.99999803, 0.99998915, 0.999978, 0.9999945, 0.99999434, 0.99999875, 0.99998796, 0.9999985, 0.99998397, 0.99995816, 0.9999879, 0.9999383, 0.99999595, 0.9999481, 0.9999793, 0.99999255, 0.9999988, 0.9999968, 0.999995, 0.999985, 0.9999673, 0.9999769, 0.9999958, 0.9999901, 0.9999906, 0.9999803, 0.99998343, 0.999957, 0.9999989, 0.9999865, 0.99999374, 0.9999917, 0.9999235, 0.99999356, 0.99999535, 0.9999571, 0.9999506, 0.99999684, 0.99992406, 0.99991876, 0.99998784, 0.9999634, 0.9999693, 0.99999654, 0.9999483, 0.999986, 0.9999988, 0.999966, 0.9999901, 0.9999991, 0.9999864, 0.99999416, 0.999994, 0.9999988, 0.9999004, 0.9999858, 0.9999837, 0.99999964, 0.9999971, 0.9999439, 0.99998647, 0.99981576, 0.9999983, 0.9999875, 0.9999938, 0.99999565, 0.99997616, 0.99998677, 0.99999607, 0.9999975, 0.99998754, 0.9999698, 0.9999962, 0.99995685, 0.9999923, 0.99995494, 0.9999969, 0.99995, 0.9999938, 0.9999782, 0.9999952, 0.99999774, 0.9999968, 0.99999017, 0.9999973, 0.9999969, 0.9999984, 0.9999977, 0.99999654, 0.99999624, 0.99998486, 0.9999924, 0.99999774, 0.9999891, 0.9999851, 0.9999795, 0.9999719, 0.9999949, 0.99999815, 0.9999855, 0.9999596, 0.99991304, 0.99999416, 0.9999724, 0.99996215, 0.9999971, 0.9999866, 0.9999976, 0.999996, 0.999991, 0.9999986, 0.99998665, 0.99998677, 0.9999946, 0.99999124, 0.9999919, 0.9999547, 0.9999987, 0.99999636, 0.99996555, 0.9999982, 0.9999947, 0.9999722, 0.9999928, 0.9999293, 0.999982, 0.99999857, 0.9999991, 0.99999404, 0.99996316, 0.99999815, 0.9999626, 0.99994326, 0.999996, 0.9999739, 0.9999832, 0.999995, 0.9999962, 0.9999968, 0.99998736, 0.9999567, 0.9999928, 0.9999925, 0.9999613, 0.99999803, 0.99987036, 0.9999914, 0.9999369, 0.99999803, 0.9999899, 0.9999979, 0.9998974, 0.9999991, 0.9999815, 0.99997514, 0.9999978, 0.9999487, 0.99999946, 0.9999877, 0.99993885, 0.99997646, 0.99998474, 0.9999988, 0.99997026, 0.99998933, 0.9999966, 0.9999624, 0.9999892, 0.99999464, 0.9999964, 0.9999719, 0.999989, 0.9999987, 0.9999944, 0.9999952, 0.99998987, 0.99999046, 0.9999878, 0.9999381, 0.99999946, 0.99998033, 0.9999966, 0.9999988, 0.99995387, 0.9999945, 0.99998116, 0.99999315, 0.9999917, 0.9999965, 0.999991, 0.99998343, 0.99999267, 0.9999805, 0.9999845, 0.9999979, 0.99985933, 0.9999887, 0.9999069, 0.9999753, 0.9999896, 0.99999815, 0.9999462, 0.9999788, 0.99994445, 0.9999919, 0.999776, 0.9999982, 0.99999064, 0.9999918, 0.99998, 0.9999909, 0.9999979, 0.99998856, 0.99998873, 0.9999952, 0.9999866, 0.99989974, 0.999992, 0.99998856, 0.99999326, 0.9999957, 0.9999013, 0.9999418, 0.9999989, 0.9999918, 0.99999696, 0.9999915, 0.99998885, 0.9999858, 0.9999964, 0.9999438, 0.9999983, 0.9999932, 0.9997461, 0.9999914, 0.9999969, 0.9999752, 0.9999935, 0.9999599, 0.99994886, 0.99999434, 0.9999871, 0.99986625, 0.99996805, 0.99998945, 0.99997383, 0.9999959, 0.99984163, 0.9999673, 0.9999504, 0.9998288, 0.9999896, 0.999998, 0.99998623, 0.99998206, 0.9999878, 0.99996734, 0.9999901, 0.99995506, 0.99997634, 0.99999726, 0.99990255, 0.9998176, 0.9997439, 0.9999942, 0.9999762, 0.99997735, 0.9998831, 0.99994147, 0.9999407, 0.9999969, 0.99984914, 0.9999681, 0.9999626, 0.9999566, 0.9996961, 0.9999719, 0.9999285, 0.99997884, 0.99999374, 0.9999198, 0.9999942, 0.99999166, 0.99988806, 0.9998389, 0.9999907, 0.99983096, 0.9999931, 0.9999367, 0.99999374, 0.99997336, 0.9999543, 0.99982375, 0.9996886, 0.99989617, 0.9999421, 0.9999758, 0.9998028, 0.9997244, 0.9997955, 0.99999344, 0.99992454, 0.9999477, 0.9999559, 0.9998902, 0.9999918, 0.99988073, 0.99991375, 0.9999967, 0.99999195, 0.999748, 0.9999915, 0.9999446, 0.9999959, 0.99975455, 0.99998915, 0.99994653, 0.9999975, 0.99998325, 0.99992895, 0.9999752, 0.9998671, 0.9999957, 0.9999357, 0.9997741, 0.9999978, 0.99995166, 0.9999817, 0.9999957, 0.9999683, 0.9999902, 0.99760437, 0.99916446, 0.9996326, 0.99996835, 0.99972236, 0.99895734, 0.99973655, 0.999936, 0.9998267, 0.99439585, 0.9999051, 0.9999591, 0.9996433, 0.9999101, 0.9991365, 0.99790883, 0.99988604, 0.9999083, 0.99978787, 0.9999424, 0.9999304, 0.9947716, 0.99998593, 0.9999109, 0.99999845, 0.9996311, 0.9994905, 0.99986714, 0.9995605, 0.9998056, 0.99991316, 0.99873453, 0.99797636, 0.9999881, 0.99978995, 0.9995113, 0.99995846, 0.9998239, 0.9977021, 0.99999154, 0.9998486, 0.9998975, 0.9996752, 0.99918306, 0.9997858, 0.9999745, 0.99973875, 0.9999506, 0.9999961, 0.9976376, 0.99949074, 0.9996781, 0.99972445, 0.99998903, 0.99998415, 0.9999421, 0.9998303, 0.9999595, 0.99877155, 0.9999648, 0.9999758, 0.9993909, 0.99943435, 0.9995299, 0.8648735, 0.99541575, 0.854086, 0.31669155, 0.78073764, 0.97414285, 0.39614472, 0.9184354, 0.99579525, 0.9063716, 0.96224517, 0.98383725, 0.9973053, 0.25191152, 0.64694905, 0.6746851, 0.8989548, 0.9119269, 0.94585335, 0.99792254, 0.91909176, 0.9060401, 0.43213534, 0.75890344, 0.50199413, 0.83025116, 0.88355863, 0.99424845, 0.97863287, 0.99859923, 0.88494176, 0.9084837, 0.92169154, 0.6447452, 0.6340713, 0.729025, 0.92527467, 0.57244045, 0.9762263, 0.78277946, 0.98673016, 0.8348498, 0.99832237, 0.908703, 0.7706992, 0.99663883, 0.9585868, 0.66750824, 0.5068153, 0.96534806, 0.38586545, 0.853038, 0.29024076, 0.9798475, 0.39552757, 0.6588809, 0.6176345, 0.645502, 0.97982794, 0.9697851, 0.37877226, 0.2543067, 0.9327716, 0.8534228, 0.99845535, 0.99998426, 0.99997187, 0.9993947, 0.9992973, 0.98904794, 0.97774446, 0.9891254, 0.99158305, 0.99871236, 0.99720687, 0.9963366, 0.9958209, 0.99779344, 0.9972567, 0.99984473, 0.99623144, 0.9881212, 0.99543077, 0.99543124, 0.9990198, 0.99906176, 0.99998266, 0.99862874, 0.9978667, 0.98541564, 0.99835354, 0.99618906, 0.99980205, 0.99623704, 0.990673, 0.98762655, 0.9984746, 0.99817806, 0.9997558, 0.99992347, 0.9933099, 0.99976987, 0.9974658, 0.99465233, 0.9869218, 0.9997812, 0.9974384, 0.99403334, 0.99938923, 0.9965526, 0.99468267, 0.98867816, 0.9958492, 0.9998233, 0.99922234, 0.97694844, 0.9996042, 0.9888529, 0.97809756, 0.9951243, 0.9988162, 0.9990751, 0.9999717, 0.99185336, 0.9982444, 0.98248816, 0.99748635, 0.99265903, 0.99998134, 0.9999102, 0.999679, 0.99987906, 0.9999952, 0.99978465, 0.9992326, 0.99994975, 0.99992126, 0.99998593, 0.99993366, 0.99993676, 0.9999759, 0.99998623, 0.9999844, 0.9999821, 0.99978906, 0.9999251, 0.99997854, 0.9999375, 0.9999484, 0.9999754, 0.9999003, 0.9999427, 0.99992657, 0.9999041, 0.999895, 0.9998499, 0.9999967, 0.9999663, 0.99999356, 0.99989814, 0.9999955, 0.9999461, 0.99996525, 0.99990034, 0.99991167, 0.9999155, 0.9999829, 0.99997336, 0.9999862, 0.9999228, 0.99974763, 0.9999819, 0.9997287, 0.99995357, 0.99980855, 0.99996865, 0.9998886, 0.99996316, 0.9999815, 0.9997055, 0.9978255, 0.9999923, 0.99983263, 0.9998634, 0.99993247, 0.99997866, 0.99992937, 0.9999558, 0.99997, 0.9999822, 0.99993795, 0.9998814, 0.99988174, 0.99995947, 0.999997, 0.99999577, 0.9999899, 0.9999913, 0.99999356, 0.99999523, 0.9999873, 0.9999965, 0.9999938, 0.99999774, 0.99999887, 0.99999726, 0.99999034, 0.9999958, 0.99998164, 0.99999774, 0.9999695, 0.999988, 0.9999872, 0.9999891, 0.99990416, 0.99995327, 0.9999852, 0.9999922, 0.9999952, 0.99990433, 0.9999554, 0.9999832, 0.9999944, 0.99999154, 0.9999943, 0.9999894, 0.999996, 0.99999654, 0.99999744, 0.99997175, 0.99999076, 0.9999958, 0.99999774, 0.9999629, 0.9999945, 0.9999918, 0.99988294, 0.9999847, 0.99999815, 0.9999896, 0.999988, 0.9999934, 0.99999875, 0.99986434, 0.999996, 0.99994785, 0.9999762, 0.999931, 0.99998564, 0.9999984, 0.99999774, 0.9999509, 0.99999934, 0.9999546, 0.99997944, 0.99998564, 0.99999696, 0.9999932, 0.99997383, 0.9999912, 0.99998003, 0.99969065, 0.99990135, 0.9999992, 0.9999948, 0.9999371, 0.9999894, 0.9999966, 0.9999215, 0.9999996, 0.99990326, 0.9999962, 0.9999991, 0.99999696, 0.9999909, 0.99998754, 0.99999887, 0.9999967, 0.9999987, 0.9999469, 0.9999939, 0.9999944, 0.99999946, 0.9999974, 0.9999157, 0.9999984, 0.99999756, 0.99999595, 0.999999, 0.99989027, 0.99999917, 0.9999996, 0.99984974, 0.9999987, 0.999999, 0.9999891, 0.9999945, 0.9997377, 0.99999297, 0.9999924, 0.9999961, 0.999996, 0.99999225, 0.99999803, 0.999994, 0.9999995, 0.99999946, 0.99999875, 0.99994826, 0.9999984, 0.99999714, 0.9999842, 0.99999774, 0.9999819, 0.9999986, 0.99999636, 0.99999964, 0.9999977, 0.9999971, 0.9999832, 0.99999946, 0.99992204, 0.999987, 0.9997875, 0.99999833, 0.99998426, 0.9999969, 0.99999744, 0.9999967, 0.9999999, 0.9999952, 0.99999714, 0.99999005, 0.9999958, 0.9999817, 0.99999297, 0.9997858, 0.99999297, 0.9999994, 0.99996144, 0.9998989, 0.9999903, 0.9999822, 0.99997985, 0.9999416, 0.99999636, 0.999996, 0.99999857, 0.9999993, 0.9999927, 0.9999957, 0.9998747, 0.99999684, 0.9999936, 0.9999879, 0.99999547, 0.99999213, 0.99995506, 0.99996954, 0.9999945, 0.99994695, 0.99995804, 0.99999803, 0.9999341, 0.9999921, 0.9999998, 0.99999774, 0.9999887, 0.999995, 0.9999992, 0.9999823, 0.9999336, 0.99984246, 0.9999247, 0.9999343, 0.9999988, 0.9999711, 0.9999827, 0.99998504, 0.99999785, 0.99999976, 0.9999875, 0.9999868, 0.9999858, 0.9999977, 0.9999884, 0.99999374, 0.9999938, 0.9999677, 0.999997, 0.99998766, 0.9999338, 0.9999964, 0.99984556, 0.9999877, 0.9999675, 0.99997795, 0.9999969, 0.99998134, 0.9999538, 0.99999815, 0.99999166, 0.9999796, 0.999997, 0.99999607, 0.9999427, 0.9997931, 0.9999982, 0.99978185, 0.9999806, 0.9999891, 0.9999972, 0.9999985, 0.99980766, 0.9999996, 0.99981, 0.99999875, 0.9997872, 0.9999142, 0.99999976, 0.9999982, 0.99999595, 0.99997437, 0.99998707, 0.99999976, 0.99999666, 0.9999304, 0.99999326, 0.99999833, 0.9999948, 0.9999766, 0.99999386, 0.9999795, 0.9999628, 0.9999884, 0.9997684, 0.9999997, 0.9999997, 0.99996465, 0.99999374, 0.99996233, 0.99993074, 0.9999963, 0.99999875, 0.9999981, 0.99999887, 0.99999815, 0.99993855, 0.9999966, 0.9999701, 0.99997354, 0.9999398, 0.9999963, 0.9999827, 0.9999991, 0.9999879, 0.99997693, 0.9999869, 0.99999136, 0.9999982, 0.9999999, 0.9999933, 0.9999979, 0.99999243, 0.9999418, 0.999979, 0.9999599, 0.9999871, 0.9999977, 0.99998945, 0.999996, 0.9999989, 0.999998, 0.999996, 0.99999887, 0.9999936, 0.9998868, 0.99999803, 0.9999923, 0.999996, 0.9999972, 0.99994504, 0.99999666, 0.9999953, 0.9999804, 0.9999972, 0.9999506, 0.9999978, 0.9999976, 0.9999797, 0.999997, 0.9999876, 0.9998983, 0.99999136, 0.99999523, 0.9999991, 0.99998564, 0.9999898, 0.99999505, 0.99997294, 0.9999993, 0.99994373, 0.9997653, 0.9999827, 0.9999991, 0.99999803, 0.99998313, 0.99999785, 0.9999941, 0.9999968, 0.99990857, 0.99999887, 0.99999744, 0.99996537, 0.9999324, 0.99996585, 0.99999654, 0.9999975, 0.9999994, 0.9999167, 0.99999976, 0.9999972, 0.99999654, 0.99986404, 0.9999708, 0.9999952, 0.99997944, 0.99990493, 0.9999988, 0.9999932, 0.9994732, 0.99999976, 0.9999925, 0.99999946, 0.9999958, 0.99999756, 0.99989855, 0.9999952, 0.99999857, 0.99999183, 0.9999961, 0.99998415, 0.9999816, 0.9999981, 0.99999976, 0.9999989, 0.99999577, 0.99985963, 0.9999939, 0.99990255, 0.9999771, 0.9999996, 0.9999978, 0.9999844, 0.9999926, 0.9999788, 0.9999897, 0.9993496, 0.99997073, 0.9998858, 0.99999696, 0.9998288, 0.99998456, 0.99998116, 0.9999987, 0.9999951, 0.99999076, 0.9999974, 0.9999992, 0.99999833, 0.99998456, 0.9999292, 0.9999709, 0.99999195, 0.99999726, 0.9999981, 0.9999977, 0.9999837, 0.99996895, 0.9999722, 0.9999832, 0.9999964, 0.9999976, 0.9999802, 0.99999607, 0.99998736, 0.9999763, 0.9999573, 0.99993634, 0.9997683, 0.9999847, 0.99994755, 0.99990624, 0.99999887, 0.9999993, 0.9999989, 0.99999803, 0.9999934, 0.9999733, 0.9999227, 0.9999368, 0.99997437, 0.9999968, 0.9999339, 0.9999656, 0.9999829, 0.99986047, 0.9999994, 0.9999952, 0.9999869, 0.9999715, 0.99999917, 0.9999184, 0.9999921, 0.99999964, 0.9999794, 0.9999948, 0.9999576, 0.9999976, 0.9998162, 0.9996962, 0.99995637, 0.99998736, 0.9999868, 0.9999977, 0.9995923, 0.99999905, 0.9999997, 0.99999255, 0.99998903, 0.9998958, 0.99999243, 0.9999915, 0.9999742, 0.99999636, 0.99999696, 0.9999849, 0.99997765, 0.9999983, 0.99997276, 0.9999968, 0.9999975, 0.99994683, 0.9999923, 0.9999937, 0.9999335, 0.9999287, 0.99999225, 0.99996614, 0.9998905, 0.9999317, 0.99999756, 0.9998993, 0.9999658, 0.99999166, 0.9999841, 0.9999932, 0.9999934, 0.99999845, 0.999989, 0.99999774, 0.9999968, 0.9999505, 0.9999835, 0.99998564, 0.9999943, 0.9999995, 0.99998707, 0.9998721, 0.99996406, 0.9999607, 0.9999961, 0.9999895, 0.9999819, 0.9999949, 0.9999689, 0.9999551, 0.9999803, 0.9998896, 0.9999974, 0.9999776, 0.99999636, 0.9999988, 0.9999805, 0.9999596, 0.9999787, 0.999947, 0.999961, 0.99999726, 0.9999968, 0.9999826, 0.99999744, 0.9999739, 0.99990207, 0.9999803, 0.99999875, 0.9999968, 0.9999973, 0.9998686, 0.9999982, 0.9999672, 0.99997777, 0.9996812, 0.9999961, 0.99999857, 0.9999794, 0.999995, 0.99998754, 0.9999942, 0.9999983, 0.99963534, 0.9999993, 0.9999831, 0.9999948, 0.9999646, 0.99991894, 0.99999905, 0.9999683, 0.99996936, 0.99998, 0.9999863, 0.9999611, 0.9999919, 0.999854, 0.99999934, 0.9999513, 0.9999647, 0.99999374, 0.999996, 0.99998933, 0.99991673, 0.9999713, 0.9999612, 0.9997181, 0.99999213, 0.99999726, 0.9999273, 0.99998796, 0.99988204, 0.9999672, 0.9999202, 0.9999999, 0.9999103, 0.99999326, 0.9999886, 0.99998486, 0.99999195, 0.99999475, 0.99983937, 0.9999945, 0.999999, 0.99999726, 0.9999243, 0.999963, 0.9999854, 0.9999988, 0.9999866, 0.99999315, 0.9998874, 0.99997973, 0.9999947, 0.9999958, 0.9999919, 0.9998211, 0.9999947, 0.9995766, 0.99991846, 0.99999905, 0.9999963, 0.99995977, 0.99999696, 0.99999684, 0.9999996, 0.99997383, 0.99999666, 0.99999905, 0.9999469, 0.9997781, 0.99999917, 0.99965394, 0.99999547, 0.9999955, 0.9999994, 0.9999783, 0.9998623, 0.99999475, 0.99999714, 0.999936, 0.9999958, 0.9999963, 0.99974537, 0.9998637, 0.99940217, 0.9999663, 0.9999973, 0.99999595, 0.99998844, 0.9999717, 0.9999741, 0.9999996, 0.9999992, 0.9998198, 0.9999926, 0.9999989, 0.9999369, 0.99997056, 0.9999753, 0.9999963, 0.9999826, 0.99999744, 0.9999978, 0.99995524, 0.9999987, 0.9999968, 0.9999913, 0.9999962, 0.99996686, 0.9999083, 0.99998116, 0.999973, 0.9999862, 0.99998134, 0.9999803, 0.99999815, 0.9999992, 0.9999424, 0.9996373, 0.99993074, 0.9999976, 0.9999978, 0.99997693, 0.9999864, 0.99999446, 0.9999927, 0.99996907, 0.9997209, 0.9999939, 0.9999859, 0.9999406, 0.9999981, 0.99994886, 0.99999374, 0.99999946, 0.99999404, 0.99999785, 0.99983317, 0.9999985, 0.9999948, 0.9999989, 0.99999905, 0.99999386, 0.999855, 0.99994534, 0.9999935, 0.9999581, 0.99999374, 0.999982, 0.9999972, 0.99999917, 0.9999842, 0.99999857, 0.99999857, 0.99999154, 0.9998997, 0.9999122, 0.99999, 0.9999975, 0.9999887, 0.9995854, 0.999944, 0.9999965, 0.9999916, 0.99999624, 0.9999321, 0.9999976, 0.9999916, 0.9999018, 0.9999993, 0.99999744, 0.9999601, 0.99999934, 0.9999977, 0.999967, 0.9999849, 0.99999523, 0.99987334, 0.99986154, 0.99999267, 0.9999794, 0.9998782, 0.99998534, 0.9999152, 0.9999971, 0.9999937, 0.9999987, 0.9999963, 0.99999213, 0.9999985, 0.99999595, 0.99998975, 0.9999983, 0.9998914, 0.99999654, 0.99999374, 0.9997409, 0.9999972, 0.99999714, 0.99999475, 0.9999974, 0.99990875, 0.9999988, 0.9999575, 0.9999872, 0.9999964, 0.99994415, 0.99999803, 0.99997896, 0.99997866, 0.9999433, 0.9997056, 0.99999946, 0.9996043, 0.99995, 0.9999986, 0.9999856, 0.9999947, 0.999996, 0.99998915, 0.9998449, 0.9999716, 0.9999909, 0.99996793, 0.9999976, 0.99997, 0.9999358, 0.99999654, 0.9998019, 0.99999535, 0.99999285, 0.9999974, 0.9999143, 0.9999958, 0.99997705, 0.9999985, 0.9999169, 0.99999315, 0.9999929, 0.99983555, 0.999997, 0.9999941, 0.9999934, 0.9998542, 0.999994, 0.9999874, 0.9999672, 0.99999905, 0.9999797, 0.99999523, 0.9999957, 0.9999486, 0.99997705, 0.9999945, 0.9999665, 0.9996177, 0.99999195, 0.9999627, 0.9999948, 0.99998486, 0.9999969, 0.9999303, 0.99989736, 0.99999696, 0.9998606, 0.99980867, 0.99999326, 0.999075, 0.999994, 0.99999285, 0.9999867, 0.99999344, 0.9999939, 0.9999953, 0.99990696, 0.9997123, 0.9999725, 0.99988204, 0.9999718, 0.9999584, 0.99999964, 0.9998354, 0.999996, 0.99999756, 0.9999955, 0.999738, 0.9997124, 0.99997926, 0.99999064, 0.9999932, 0.9994541, 0.99998516, 0.99997354, 0.9999996, 0.99998194, 0.9999869, 0.99999833, 0.99997747, 0.99973774, 0.9999961, 0.99999875, 0.99999624, 0.9999932, 0.9999999, 0.99997896, 0.9999897, 0.99999, 0.99999803, 0.9999922, 0.9999998, 0.9999741, 0.99999917, 0.9999594, 0.99997634, 0.99982214, 0.9999284, 0.9995762, 0.99999946, 0.99999213, 0.9999967, 0.999979, 0.99972224, 0.99999416, 0.99990153, 0.99999255, 0.9999886, 0.99993336, 0.9999659, 0.99995947, 0.9999542, 0.99992496, 0.9999794, 0.99985176, 0.99950665, 0.99999624, 0.99998796, 0.9999964, 0.9999629, 0.99993414, 0.9999904, 0.9999949, 0.9999861, 0.99996233, 0.9999944, 0.99998665, 0.99999243, 0.99998057, 0.99999464, 0.9999855, 0.99997395, 0.9999971, 0.99991006, 0.9999994, 0.9998601, 0.99991053, 0.999991, 0.99999106, 0.9998037, 0.9999708, 0.9999922, 0.99987465, 0.9999878, 0.99999505, 0.9997433, 0.9999993, 0.999997, 0.9999419, 0.9999996, 0.99999875, 0.99999285, 0.99999106, 0.99999493, 0.9999696, 0.9999986, 0.9999921, 0.9999914, 0.99999213, 0.99999124, 0.9999951, 0.9999824, 0.99999666, 0.99996966, 0.9999476, 0.99999624, 0.9999255, 0.9999966, 0.9994532, 0.99994344, 0.9996603, 0.99999803, 0.9999861, 0.9999586, 0.9992229, 0.99969804, 0.99998426, 0.9999999, 0.99999887, 0.9999958, 0.999996, 0.9999971, 0.9999963, 0.9999991, 0.99999774, 0.9999979, 0.9999995, 0.9999946, 0.99999917, 0.9999419, 0.99983907, 0.9999906, 0.99996126, 0.9999698, 0.99994534, 0.99992734, 0.9999652, 0.99999565, 0.9999811, 0.99991655, 0.9999763, 0.9999855, 0.9996864, 0.99999624, 0.9999885, 0.9999979, 0.9999994, 0.99976075, 0.9998472, 0.9999678, 0.9998587, 0.99998885, 0.99991137, 0.99999475, 0.99998, 0.99997956, 0.99998546, 0.9999993, 0.9999807, 0.99998814, 0.99999183, 0.9999973, 0.9999974, 0.99997973, 0.9999627, 0.9999646, 0.99998313, 0.9999301, 0.9999949, 0.9999976, 0.9998665, 0.99976784, 0.99999785, 0.9999234, 0.99984074, 0.9999934, 0.9999982, 0.99996936, 0.9999958, 0.99995804, 0.99999774, 0.9999726, 0.9999955, 0.99992055, 0.99997854, 0.999698, 0.999929, 0.9999791, 0.999943, 0.9995895, 0.9999331, 0.9999679, 0.999589, 0.9998979, 0.99991655, 0.9998656, 0.9999992, 0.99999017, 0.9999427, 0.9999983, 0.99995095, 0.999691, 0.9999937, 0.9999804, 0.9999956, 0.9999841, 0.9999572, 0.9999708, 0.9996971, 0.9998202, 0.9999934, 0.9999703, 0.9999898, 0.9999969, 0.99999905, 0.99999815, 0.99999666, 0.99998736, 0.9999961, 0.9999982, 0.9999787, 0.9999676, 0.99981284, 0.9999957, 0.9993375, 0.9999888, 0.99995023, 0.99999726, 0.99998975, 0.9999831, 0.99835855, 0.99997526, 0.99999714, 0.99996805, 0.9999905, 0.999991, 0.99994457, 0.9999741, 0.9999622, 0.99999934, 0.99988806, 0.99909806, 0.99997437, 0.9999694, 0.9997937, 0.9999508, 0.99993736, 0.99989307, 0.99369395, 0.9999969, 0.9999948, 0.99986374, 0.99996674, 0.99992967, 0.9999976, 0.99984854, 0.999963, 0.9999967, 0.999987, 0.99979293, 0.9999921, 0.99998975, 0.9999842, 0.99989754, 0.999886, 0.9999943, 0.9999836, 0.9999725, 0.99924046, 0.9999391, 0.9999363, 0.9999676, 0.9999288, 0.99990785, 0.9999898, 0.9999822, 0.9998743, 0.99994993, 0.999988, 0.99997354, 0.9999699, 0.99994206, 0.9997893, 0.99998313, 0.999754, 0.99983, 0.9999163, 0.99996865, 0.99998957, 0.99997205, 0.99974954, 0.9999717, 0.9999416, 0.9994165, 0.99998283, 0.7332714, 0.9982285, 0.99976134, 0.9998837, 0.25069627, 0.8896898, 0.9536972, 0.95446444, 0.9958782, 0.99869883, 0.9998267, 0.999421, 0.65120804, 0.9986427, 0.94679207, 0.99804723, 0.96145415, 0.99929285, 0.9983424, 0.9923084, 0.99279535, 0.99932003, 0.9999326, 0.99175334, 0.30508792, 0.798989, 0.9980292, 0.9998805, 0.99531186, 0.9983555, 0.9822959, 0.9991691, 0.9857483, 0.9983947, 0.9975775, 0.99895436, 0.99944055, 0.99678886, 0.9993867, 0.85944825, 0.99964285, 0.9802518, 0.99932796, 0.993493, 0.9855829, 0.993903, 0.92564327, 0.98647314, 0.9989176, 0.99857455, 0.99908566, 0.9928012, 0.99938846, 0.99777913, 0.9972408, 0.9992449, 0.97460747, 0.9972953, 0.9970955, 0.9973749, 0.67717683, 0.8308265, 0.99869806, 0.64640224, 0.56175107, 0.9668325, 0.6077384, 0.75875884, 0.25114593, 0.9629428, 0.35077885, 0.30452713, 0.7052814, 0.5705251, 0.9630824, 0.42912418, 0.7979924, 0.72773, 0.68628657, 0.2197686, 0.51354307, 0.161481, 0.9112718, 0.8901617, 0.7516236, 0.9976633, 0.71286184, 0.349305, 0.17869468, 0.96146226, 0.99500555, 0.43325117, 0.49178147, 0.378811, 0.785948, 0.990744, 0.8610667, 0.24678768, 0.9529741, 0.76594126, 0.20254329, 0.46507624, 0.9956863, 0.69851804, 0.27567464, 0.6433235, 0.39973265, 0.9961623, 0.57349735, 0.9430518, 0.6218998, 0.28955752, 0.44161212, 0.3015731, 0.3461029, 0.31834787, 0.747687, 0.99722576, 0.9803512, 0.8553765, 0.8846469, 0.9008462, 0.32877624, 0.8415385, 0.9709213, 0.9157606, 0.9752565, 0.99992484, 0.9999656, 0.9997486, 0.9999756, 0.9999359, 1e-07, 0.99999547, 0.99998486, 0.9999532, 0.9999796, 0.9999746, 0.999983, 0.9998577, 0.9996403, 0.99997, 0.999991, 0.99999136, 0.9998665, 0.9998129, 0.9999963, 0.99949485, 0.9999029, 0.9999103, 0.99999815, 0.9999773, 0.99980855, 0.99919575, 0.9999138, 0.99988097, 0.9998522, 0.9999938, 0.9999956, 0.99997634, 0.99997324, 0.99999094, 0.99995565, 0.9999921, 0.999993, 0.9998774, 0.9999894, 0.9999639, 0.99999624, 0.9999958, 0.9996067, 0.99999243, 0.99999005, 0.9999427, 0.99999136, 0.9999871, 0.9995349, 0.999869, 0.9999306, 0.9998891, 0.9999994, 0.99969786, 0.99996245, 0.999996, 0.99878526, 0.99912477, 0.99975175, 0.99999374, 0.9999945, 0.9994907, 0.999907, 0.99994516, 0.99989647, 0.9999649, 0.99999154, 0.9998724, 0.99999547, 0.999937, 0.9999979, 0.9999757, 0.99982387, 0.99995494, 0.99995077, 0.9999367, 0.99994326, 0.9999034, 0.9996781, 0.9999723, 0.9999551, 0.99997264, 0.9999715, 0.9999257, 0.9999044, 0.9999982, 0.99999, 0.99993515, 0.9999887, 0.99996006, 0.9999931, 0.99999386, 0.99999374, 0.9999965, 0.9999077, 0.9998637, 0.9999598, 0.9999978, 0.9999952, 0.99999726, 0.9999881, 0.99972755, 0.99976516, 0.9999646, 0.999953, 0.99993527, 0.9993745, 0.999865, 0.9999992, 0.9999657, 0.9999538, 0.99998844, 0.99996084, 0.9999901, 0.9999505, 0.99994266, 0.9999969, 0.9999676, 0.9999305, 0.99998415, 0.9999954, 0.9996857, 0.9998382, 0.9997041, 0.9999533, 0.99994123, 0.99877304, 0.9995763, 0.99994636, 0.99997026, 0.9999672, 0.9999739, 0.9998816, 0.999993, 0.99982685, 0.99944794, 0.99995434, 0.9999357, 0.9998756, 0.9999751, 0.9999574, 0.99998504, 0.99954283, 0.9999465, 0.9999948, 0.999438, 0.9999829, 0.9999869, 0.99998516, 0.99992454, 0.99996614, 0.999997, 0.99999106, 0.9999975, 0.99999386, 0.9999877, 0.99972665, 0.99999386, 0.9999329, 0.999984, 0.9999989, 0.9999588, 0.9999918, 0.9999936, 0.9998117, 0.99999595, 0.9999697, 0.9999744, 0.9999342, 0.9993728, 0.9994345, 0.99957895, 0.99994236, 0.99995285, 0.99969226, 0.9997326, 0.99979866, 0.99980813, 0.9999389, 0.99999624, 0.99999183, 0.99975485, 0.9999666, 0.99996537, 0.99986506, 0.9999965, 0.99998385, 0.99997175, 0.999991, 0.9999615, 0.99996954, 0.9999645, 0.9998872, 0.9999146, 0.9999792, 0.9999987, 0.999892, 0.9999919, 0.9999333, 0.9994681, 0.9985634, 0.99935114, 0.99927604, 0.9999416, 0.99956423, 0.99947, 0.99999386, 0.9971989, 0.99992573, 0.9998674, 0.99999046, 0.99976027, 0.9999248, 0.9997238, 0.9992714, 0.9998341, 0.99624705, 0.99995464, 0.99991345, 0.99983114, 0.99996704, 0.9999689, 0.9999673, 0.99986863, 0.9999823, 0.99995947, 0.9999599, 0.99999267, 0.999993, 0.9999618, 0.99989337, 0.999144, 0.9999967, 0.9999819, 0.99988765, 0.99990016, 0.9998999, 0.9999349, 0.9999657, 0.9999458, 0.9999979, 0.9999803, 0.99997187, 0.99999756, 0.9998044, 0.9999948, 0.99992365, 0.9999702, 0.9999386, 0.99994624, 0.99992865, 0.9999694, 0.9999722, 0.9999784, 0.9989047, 0.9928711, 0.9996695, 0.9998832, 0.99995726, 0.9997911, 0.99994665, 0.999626, 0.9997126, 0.99982256, 0.9998299, 0.9962124, 0.99985904, 0.99714863, 0.9990166, 0.9999293, 0.997468, 0.9990844, 0.99947876, 0.9997736, 0.9997211, 0.99982566, 0.999614, 0.9999949, 0.9987687, 0.9875494, 0.99989486, 0.9990746, 0.9942651, 0.9772948, 0.99638414, 0.99826497, 0.99899304, 0.999542, 0.99938375, 0.99978656, 0.9996763, 0.998738, 0.99998003, 0.9989881, 0.9952756, 0.99785256, 0.9985951, 0.9992362, 0.9999001, 0.9999214, 0.9999652, 0.9996564, 0.99986774, 0.99874496, 0.9983708, 0.9996866, 0.998274, 0.9994922, 0.99840236, 0.99984884, 0.99776316, 0.99325955, 0.9998427, 0.99466306, 0.99994296, 0.999353, 0.9999953, 0.9994629, 0.99998903, 0.99185854, 0.99718416, 0.9979081, 0.8474787, 0.9891718, 0.99841124, 0.9992183, 0.9993989, 0.95820624, 0.35853878, 0.9975495, 0.95090175, 0.91731596, 0.99235916, 0.9922171, 0.99903476, 0.99925613, 0.9956588, 0.9988289, 0.97791, 0.99562985, 0.99944603, 0.9840869, 0.99938285, 0.9999396, 0.9914195, 0.99927014, 0.9989045, 0.9985514, 0.98914963, 0.9678842, 0.9974205, 0.99838793, 0.9966037, 0.6813058, 0.9836274, 0.99921465, 0.9939554, 0.99793726, 0.9943045, 0.9994577, 0.9996593, 0.97145975, 0.99856216, 0.987969, 0.9150854, 0.9930614, 0.94768655, 0.97085965, 0.99866813, 0.9998721, 0.9985202, 0.51670647, 0.95212483, 0.99897105, 0.99493015, 0.973541, 0.99106824, 0.9909923, 0.99303615, 0.9624806, 0.9783663, 0.9994881, 0.9977091, 0.5929775, 0.8403303, 0.98012245, 0.7670234, 0.9960801, 0.8830679, 0.7746993, 0.14812279, 0.99923766, 0.7042018, 0.14482011, 0.64931643, 0.99890757, 0.96579236, 0.1608264, 0.268416, 0.8770199, 0.927488, 0.8729752, 0.9763687, 0.9346729, 0.5717127, 0.26591703, 0.47423926, 0.8589839, 0.30631465, 0.8321088, 0.6451131, 0.22432457, 0.99363875, 0.15354598, 0.7213499, 0.99744254, 0.99150693, 0.49191993, 0.9801009, 0.3061743, 0.7108659, 0.85726166, 0.8691654, 0.870553, 0.9796889, 0.9677539, 0.92572653, 0.95017594, 0.46524003, 0.42765388, 0.71208, 0.5982167, 0.9891443, 0.6088584, 0.5397357, 0.6633271, 0.8130467, 0.74954206, 0.4279303, 0.9486989, 0.49130815, 0.24955812, 0.6546686, 0.96996695, 0.9442212, 0.2043893, 0.26669687, 0.30243173, 0.31622255, 0.5615609, 0.13551377, 0.31713423, 0.10362157, 0.9808514, 0.13645421, 0.90787476, 0.31928423, 0.11970995, 0.9461037, 0.47015968, 0.4571148, 0.6552083, 0.7005867, 0.14422949, 0.40803164, 0.09912288, 0.16244169, 0.2349683, 0.24674809, 0.84445024, 0.8789319, 0.72965497, 0.22192085, 0.68389094, 0.90861607, 0.21103051, 0.33410454, 0.34430844, 0.14058836, 0.7564829, 0.122169316, 0.12548944, 0.3110965, 0.15915795, 0.96723795, 0.71046096, 0.7183254, 0.21729448, 0.3057604, 0.9401251, 0.3816294, 0.9937404, 0.25392258, 0.06582139, 0.9698495, 0.48920947, 0.9263624, 0.14835787, 0.78240705, 0.9123932, 0.7676968, 0.15816312, 0.8173407, 0.4595014, 0.05467654, 0.44307685, 0.8334518, 0.24940573, 0.33035937, 0.23328339, 0.98606163, 0.9998191, 0.94363916, 0.80932623, 0.99965703, 0.76910865, 0.98421955, 0.7992026, 0.9993693, 0.9155987, 0.97903836, 0.979517, 0.96336997, 0.6988219, 0.9974874, 0.9998376, 0.9839586, 0.9827669, 0.9860857, 0.99729615, 0.33994833, 0.99972844, 0.91044915, 0.9991318, 0.93131727, 0.9993322, 0.9995571, 0.9424546, 0.9999779, 0.9975827, 0.9996265, 0.9804098, 0.9933502, 0.9992357, 0.9961963, 0.9956543, 0.8886026, 0.9978666, 0.82100046, 0.9995174, 0.9999475, 0.98973787, 0.99600863, 0.99139965, 0.97903866, 0.7459602, 0.9682901, 0.80237514, 0.78808844, 0.9059879, 0.9157461, 0.69449526, 0.8925058, 0.96592885, 0.99885464, 0.98965216, 0.94222623, 0.9994933, 0.75791544, 0.99994016, 0.9802879, 0.9978168, 0.99980795, 0.90110546, 0.9994545, 0.99642175, 0.98353505, 0.9975088, 0.991521, 0.99987835, 0.94816685, 0.997794, 0.99884117, 0.92734015, 0.99667513, 0.99834204, 0.9975952, 0.99802816, 0.9963499, 0.9998704, 0.06834173, 0.9998408, 0.9861185, 0.9969366, 0.77585, 0.99912876, 0.96822274, 0.99984807, 0.98507714, 0.9207797, 0.99752927, 0.9966576, 0.9342883, 0.9952291, 0.9629463, 0.80649817, 0.8994397, 0.9984066, 0.97061735, 0.8908617, 0.997836, 0.75799674, 0.9836277, 0.9814772, 0.99374855, 0.99837214, 0.9936552, 0.98891133, 0.9979448, 0.9973991, 0.99356407, 0.83516747, 0.99221045, 0.9715347, 0.9958244, 0.9937043, 0.98519945, 0.9997412, 0.95726806, 0.9806786, 0.948893, 0.9662475, 0.9752129, 0.9997915, 0.9981559, 0.995772, 0.99493396, 0.9945488, 0.971179, 0.9879896, 0.9212722, 1e-07, 0.09474308, 0.9961088, 0.9751498, 0.7371876, 0.6549432, 0.31260976, 0.9986235, 0.20303202, 0.8491605, 0.9876581, 0.972882, 0.8477035, 0.9926579, 0.88071215, 0.75955904, 0.9807861, 0.13049333, 0.14926474, 0.34614348, 0.22209811, 0.99832666, 0.30592734, 0.9774243, 0.9916933, 0.9689586, 0.47687614, 0.40826723, 0.7418402, 0.9971311, 0.2702644, 0.9415657, 0.5672237, 0.96178424, 0.5969337, 0.941321, 0.70161885, 0.96619964, 0.14550985, 0.84322816, 0.9159523, 0.9649245, 0.7810809, 0.9817836, 0.97712356, 0.7806276, 0.5932043, 0.5761445, 0.16768052, 0.17295751, 0.9678684, 0.16790758, 0.9984427, 0.36044014, 0.20811185, 0.69908404, 0.9889714, 0.18705368, 0.16046774, 0.9966221, 0.61869156, 0.97269607, 0.22279237, 0.5036314, 0.5601118, 0.56654656, 0.43655387, 0.41214997, 0.20535561, 0.7388213, 0.17840171, 0.43880728, 0.92231035, 0.7002592, 0.43619007, 0.91117424, 0.14294265, 0.9753337, 0.84699935, 0.8541949, 0.86115766, 0.20903626, 0.36526006, 0.7439465, 0.44176936, 0.8663985, 0.99098754, 0.7819363, 0.28604484, 0.77252156, 0.8661469, 0.4092155, 0.26968816, 0.55974966, 0.9946053, 0.14370407, 0.1889966, 0.637545, 0.37551206, 0.52187407, 0.5167995, 0.7195214, 0.6700811, 0.8592196, 0.26560497, 0.4554428, 0.8651198, 0.8951658, 0.15591434, 0.7437882, 0.18384212, 0.31159577, 0.8662283, 0.5423985, 0.21318185, 0.24722253, 0.64899737, 0.82322866, 0.42746854, 0.7038752, 0.29265746, 0.9390325, 0.9958072, 0.7276702, 0.19593641, 0.14390463, 0.19157735, 0.9984195, 0.9978292, 0.94733834, 0.29832503, 0.7221331, 0.3398204, 0.83515817, 0.9355908, 0.99748045, 0.98064446, 0.99571383, 0.60123885, 0.9104382, 0.81584555, 0.5490561, 0.9993554, 0.87604487, 0.9731468, 0.20819056, 0.9997443, 0.66401756, 0.49655348, 0.8802505, 0.99349564, 0.92251515, 0.8738782, 0.9731446, 0.42015895, 0.98667943, 0.85993236, 0.86150146, 0.95502776, 0.31081122, 0.9879846, 0.86212534, 0.9987489, 0.9998695, 0.86983615, 0.56332374, 0.9980206, 0.9993369, 0.50324106, 0.97358644, 0.4584675, 0.8086999, 0.99413764, 0.9953243, 0.40375882, 0.8372363, 0.9805954, 0.31713584, 0.99820286, 0.9043917, 0.99342656, 0.8762332, 0.99173874, 0.9989369, 0.9815632, 0.9996961, 0.9924896, 0.80600756, 0.9965726, 0.97820604, 0.97301173, 0.8275456, 0.84267014, 0.99972945, 0.9854424, 0.8939048, 0.77685535, 0.9984067, 0.7708748, 0.99524903, 0.99002737, 0.48208824, 0.98408836, 0.98516905, 0.5749802, 0.9926214, 0.9966168, 0.99130356, 0.97670346, 0.6569776, 0.99976856, 0.98957443, 0.28441322, 0.9979732, 0.99977237, 0.9992501, 0.35704723, 0.96632403, 0.81534755, 0.7445496, 0.9985384, 0.83603376, 0.9978447, 0.98241377, 0.7626752, 0.9552075, 0.97567356, 0.926889, 0.99975604, 0.96877074, 0.9813753, 0.9997283, 0.9970376, 0.8142184, 0.99425894, 0.9974685, 0.35580492, 0.9602337, 0.86013806, 0.8128153, 0.9376485, 0.9176386, 0.98994493, 0.6295554, 0.9907352, 0.9430292, 0.7015986, 0.89567494, 0.9299443, 0.9919364, 0.99988866, 0.9937426, 0.53346497, 0.65658087, 0.1409595, 0.9966004, 0.34936622, 0.9700909, 0.84113, 0.7776008, 0.31783307, 0.8524885, 0.9516297, 0.81379926, 0.40280455, 0.13609987, 0.19380322, 0.2394907, 0.28369704, 0.95160514, 0.16376558, 0.5179005, 0.80340254, 0.1549588, 0.22003724, 0.14850861, 0.58965963, 0.29009372, 0.9089836, 0.84566444, 0.15607062, 0.51086825, 0.17714253, 0.150434, 0.5316927, 0.14766876, 0.45796993, 0.15033571, 0.20125216, 0.17084028, 0.5534485, 0.39647317, 0.97602993, 0.18482494, 0.7110439, 0.9735421, 0.6673888, 0.21138842, 0.34563974, 0.5210007, 0.640842, 0.8344526, 0.7757854, 0.19436298, 0.630174, 0.16227198, 0.23315361, 0.51866704, 0.4510056, 0.8409419, 0.18159628, 0.1628739, 0.33142444, 0.15938163, 0.5548988, 0.60199064, 0.877858, 0.4091396, 0.19777972, 0.95113796, 0.96188825, 0.9943784, 0.7714421, 0.99353033, 0.9983006, 0.98593956, 0.7877606, 0.99991757, 0.9612556, 0.9972371, 0.9996613, 0.99383104, 0.9587306, 0.77100456, 0.88493395, 0.9944326, 0.9907309, 0.9939864, 0.9838942, 0.99470913, 0.9937656, 0.9976012, 0.9942162, 0.9963665, 0.91295433, 0.9936693, 0.99639624, 0.89115596, 0.96606565, 0.9950685, 0.94278806, 0.99816287, 0.8123066, 0.9944193, 0.9225881, 0.9539984, 0.77346885, 0.9788327, 0.993751, 0.98410434, 0.9701615, 0.8971381, 0.9597729, 0.9978201, 0.4003123, 0.9732043, 0.98621035, 0.99890345, 0.6761023, 0.94259423, 0.255324, 0.98876697, 0.99290925, 0.94082755, 0.71847713, 0.96393394, 0.7468849, 0.71561414, 0.9974989, 0.9112249, 0.94255924, 0.6129343, 0.46030316, 0.3902945, 0.49232572, 0.17885737, 0.9836986, 0.28736347, 0.18236868, 0.048281945, 0.10824928, 0.9778256, 0.84583783, 0.739677, 0.2542702, 0.9001478, 0.25268632, 0.83493346, 0.5670013, 0.9435749, 0.515487, 0.13201888, 0.40079498, 0.5287495, 0.9924125, 0.24216075, 0.38912538, 0.23688419, 0.24732435, 0.3008478, 0.9608217, 0.8528917, 0.7268499, 0.28178805, 0.2568852, 0.3563099, 0.8779326, 0.2655837, 0.3570486, 0.9984499, 0.8167503, 0.84926796, 0.66089284, 0.88884205, 0.8656056, 0.37252495, 0.94987607, 0.7125356, 0.2576648, 0.96704084, 0.6263921, 0.869503, 0.92312783, 0.76816326, 0.8740675, 0.6157746, 0.7582157, 0.56822926, 0.61217445, 0.9374755, 0.19979624, 0.5182577, 0.7115939, 0.2502349, 0.5553604, 0.30267382, 0.45453793, 0.9913541, 0.99798316, 0.9920301, 0.42285675, 0.9976752, 0.9796895, 0.7568482, 0.8465626, 0.96241814, 0.99823093, 0.99680924, 0.98067325, 0.99365443, 0.99211067, 0.9890406, 0.3062515, 0.901215, 0.60278875, 0.99740016, 0.9673996, 0.9590162, 0.9241694, 0.9873799, 0.8473872, 0.9920804, 0.97173434, 0.99465156, 0.9855946, 0.9882599, 0.99544215, 0.9856601, 0.99935496, 0.92333543, 0.99463415, 0.998693, 0.9893541, 0.6746794, 0.9981281, 0.999823, 0.94087416, 0.73260695, 0.9882673, 0.9968024, 0.99843943, 0.9983451, 0.99720716, 0.9937705, 0.9789444, 0.98534137, 0.9512042, 0.99166507, 0.97531796, 0.98083043, 0.9985507, 0.99082804, 0.9999878, 0.9770947, 0.9990878, 0.9830057, 0.9343587, 0.9889972, 0.96418285, 0.518197, 0.8681288, 0.6473867, 0.42027286, 0.3012064, 0.9230249, 0.99673223, 0.9815804, 0.95167774, 0.8845453, 0.23909006, 0.4107519, 0.86709094, 0.56800336, 0.85068345, 0.9377182, 0.69145066, 0.40931135, 0.98800033, 0.28615698, 0.3074189, 0.18241332, 0.45511156, 0.5421852, 0.96674114, 0.17703097, 0.7933963, 0.47691664, 0.46212834, 0.73213035, 0.42836165, 0.24878174, 0.49717844, 0.9946899, 0.5811933, 0.4256833, 0.8682056, 0.98003286, 0.21673696, 0.9819592, 0.572913, 0.8316438, 0.14659828, 0.19698192, 0.94334215, 0.8094795, 0.20465277, 0.21169795, 0.98203933, 0.23144095, 0.463485, 0.7499216, 0.3099321, 0.19838214, 0.4076447, 0.87894374, 0.18015559, 0.17709984, 0.6624373, 0.9746481, 0.33296993, 0.47645044, 0.9192618, 0.6822116, 0.18841305, 0.9998862, 0.99998546, 0.9997726, 0.999743, 0.9999751, 0.99934995, 0.99957776, 0.99941075, 0.99962384, 0.9995499, 0.9995604, 0.9997257, 0.9998953, 0.9999211, 0.99981964, 0.9996807, 0.9997734, 0.9999846, 0.9998488, 0.99991924, 0.99982524, 0.99993706, 0.99901766, 0.9999937, 0.99968046, 0.99936706, 0.9999628, 0.9987078, 0.9997896, 0.99961543, 0.9980419, 0.99994445, 0.9989208, 0.999588, 0.9991338, 0.9997639, 0.99985117, 0.9989099, 0.9999793, 0.99994916, 0.99996084, 0.9999737, 0.9998066, 0.99919176, 0.999634, 0.9999769, 0.99407965, 0.99942195, 0.9979245, 0.9999661, 0.9999823, 0.9997405, 0.99972194, 0.9999843, 0.99946904, 0.9988308, 0.9998425, 0.9985356, 0.9997164, 0.9998523, 0.9996962, 0.999968, 0.99995, 0.99603105, 0.99912643, 0.99950004, 0.9983862, 0.92701393, 0.9786539, 0.9999802, 0.9900624, 0.99989825, 0.9995708, 0.9115877, 0.9425681, 0.99960166, 0.988827, 0.99382347, 0.8922145, 0.96054316, 0.99996275, 0.99854237, 0.99987346, 0.9930092, 0.9998771, 0.9452154, 0.9998871, 0.99943244, 0.997788, 0.98764986, 0.9838139, 0.9993106, 0.9969413, 0.99712163, 0.99927044, 0.76101446, 0.9950057, 0.9888853, 0.99954087, 0.96235996, 0.99988985, 0.99059117, 0.747729, 0.99994165, 0.9999571, 0.9953842, 0.98703444, 0.998351, 0.99912626, 0.9986082, 0.9996006, 0.99781996, 0.9998062, 0.960156, 0.99696344, 0.9995819, 0.8219866, 0.99975806, 0.96690035, 0.9844943, 0.8210037, 0.99870163, 0.99984145, 0.981361, 0.9999722, 0.9902889, 0.9496722, 0.99985427, 0.41791606, 0.36447784, 0.06478399, 0.91774327, 0.5253007, 0.30957535, 0.6755757, 0.13501191, 0.95940727, 0.66107994, 0.5176276, 0.38012794, 0.29135203, 0.9339891, 0.2181097, 0.24447936, 0.9974691, 0.15694037, 0.40177518, 0.6574468, 0.71950424, 0.27943662, 0.5323138, 0.3812036, 0.20720832, 0.4107638, 0.9776704, 0.13401923, 0.70673597, 0.872535, 0.88911915, 0.3714951, 0.5744116, 0.26587164, 0.9807486, 0.96951777, 0.64269507, 0.2422484, 0.8383943, 0.26696393, 0.7811247, 0.6577291, 0.1562345, 0.29187596, 0.481959, 0.7698113, 0.45677462, 0.9620006, 0.8061617, 0.070689976, 0.48358402, 0.28904757, 0.9356547, 0.1551779, 0.6042238, 0.31592232, 0.7228488, 0.18589215, 0.47080484, 0.317288, 0.98035103, 0.5463305, 0.5211998, 0.96710885, 0.999953, 0.9999963, 0.99991685, 0.99997777, 0.9999203, 0.99997544, 0.9989972, 0.99997747, 0.99997216, 0.9999654, 0.9999469, 0.9999825, 0.99955326, 0.999915, 0.9999294, 0.99997365, 0.99994814, 0.99990857, 0.99999034, 0.99991375, 0.99999386, 0.9995941, 0.9999801, 0.99999547, 0.99969393, 0.9999574, 0.99999386, 0.9999698, 0.99998003, 0.9999781, 0.99997616, 0.99995005, 0.9998467, 0.9997601, 0.99996746, 0.9999857, 0.9994774, 0.99998564, 0.9999673, 0.9999002, 0.99999404, 0.9996614, 0.99988693, 0.9994069, 0.9999735, 0.9997545, 0.9999037, 0.9999279, 0.99998736, 0.995046, 0.9994928, 0.99996847, 0.99992543, 0.99996984, 0.99994934, 0.99999756, 0.9966277, 0.999983, 0.99979913, 0.9999863, 0.99964076, 0.9999544, 0.9999897, 0.9999205, 0.9999919, 0.99999714, 0.9998828, 0.9999742, 0.9999783, 0.9999284, 0.9998808, 0.9999776, 0.9999883, 0.9999993, 0.99939156, 0.9999497, 0.99982727, 0.99998784, 0.9999911, 0.99999136, 0.9999783, 0.9999843, 0.99998355, 0.999883, 0.9999924, 0.9999788, 0.99973047, 0.9999901, 0.9999816, 0.9999718, 0.99998444, 0.99982363, 0.9999582, 0.9999838, 0.99999285, 0.9999831, 0.9999575, 0.9999837, 0.9996939, 0.9999993, 0.9999975, 0.99999636, 0.99999255, 0.99999726, 0.99999356, 0.99998343, 0.9999607, 0.99953306, 0.9999732, 0.9999752, 0.9999891, 0.9999971, 0.9999969, 0.9999915, 0.9999735, 0.99999714, 0.99957085, 0.99984294, 0.99950594, 0.9999979, 0.9999156, 0.9999281, 0.99994624, 0.9999749, 0.9999856, 0.9997233, 0.9999748, 0.9999749]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "89765f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "\n",
    "def r2_loss(output, target):\n",
    "    output=torch.tensor(output)\n",
    "    target= torch.tensor(target)\n",
    "    target_mean = torch.mean(target)\n",
    "    ss_tot = torch.sum((target - target_mean) ** 2)\n",
    "    ss_res = torch.sum((target - output) ** 2)\n",
    "    r2 = 1 - ss_res / ss_tot\n",
    "    return r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "aa7b3f06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.8879923397130671"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(ytrue,ypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "698fda85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.002974167602880679"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_loss(ypred,ytrue)\n",
    "np.corrcoef(ypred, ytrue)[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "88d8676a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SpearmanrResult(correlation=-0.01741669195973143, pvalue=0.24721051831535745)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "masked_array(\n",
       "  data=[[1.0, 0.0029741676028806796],\n",
       "        [0.0029741676028806796, 1.0]],\n",
       "  mask=[[False, False],\n",
       "        [False, False]],\n",
       "  fill_value=1e+20)"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy.ma as ma\n",
    "# print(scipy.stats.pearsonr( ytrue, ypred, nan_policy='omit'))\n",
    "print(scipy.stats.spearmanr(ytrue, ypred, nan_policy='omit'))\n",
    "scipy.stats.kendalltau(ytrue, ypred)\n",
    "x,y=scipy.stats.spearmanr(ytrue, ypred, nan_policy='omit')\n",
    "ma.corrcoef(ytrue,ypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "9878b2fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 57.3543,  37.9292,  36.9135, -50.1016, -48.0833,  16.1333],\n",
       "        [ 57.2833,  37.8723,  36.8749, -49.9836, -47.9970,  16.1162],\n",
       "        [ 52.0045,  34.8809,  34.9369, -46.2354, -42.7874,  16.4383],\n",
       "        [ 57.3537,  37.9299,  36.9139, -50.1031, -48.0837,  16.1340],\n",
       "        [ 52.6372,  35.3053,  35.2500, -46.7575, -43.3693,  16.4615],\n",
       "        [ 52.6414,  35.3104,  35.2545, -46.7702, -43.3835,  16.4427]])"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=torch.tensor([[ 57.3543,  37.9292,  36.9135, -50.1016, -48.0833,  16.1333],\n",
    "        [ 57.2833,  37.8723,  36.8749, -49.9836, -47.9970,  16.1162],\n",
    "        [ 52.0045,  34.8809,  34.9369, -46.2354, -42.7874,  16.4383],\n",
    "        [ 57.3537,  37.9299,  36.9139, -50.1031, -48.0837,  16.1340],\n",
    "        [ 52.6372,  35.3053,  35.2500, -46.7575, -43.3693,  16.4615],\n",
    "        [ 52.6414,  35.3104,  35.2545, -46.7702, -43.3835,  16.4427]])\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "3d2e30a4",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_9532/1221237739.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: __init__() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "torch.tanh(a)\n",
    "torch.nn.Tanh(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "02994510",
   "metadata": {},
   "outputs": [],
   "source": [
    "t=torch.tensor([[ 7.2932e-01,  7.4810e-01, -6.7753e-01,  9.6794e-01,\n",
    "          -9.9452e-01, -7.3781e-04],\n",
    "         [ 9.5957e-01,  9.6300e-01, -7.4124e-01,   9.8340e-01,\n",
    "          -9.9489e-01, -3.8238e-04],\n",
    "         [ 9.9419e-01,  9.9489e-01, -7.4874e-01,   9.8242e-01,\n",
    "          -9.9538e-01, -2.6892e-04],\n",
    "         [ 1.6123e-04, -6.9869e-02,  8.0796e-01, 9.7620e-01,\n",
    "          -6.9407e-02, -3.9553e-04],\n",
    "         [ 2.2822e-04, -8.2518e-02,  7.6946e-01,   9.2510e-01,\n",
    "          -6.0868e-03, -1.2799e-03],\n",
    "         [ 3.1881e-04, -9.8367e-02,  7.2299e-01,  6.6754e-01,\n",
    "           1.0458e-02, -6.5697e-03]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "73ccf4e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 100, 35, 45])"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randn(20, 100, 35, 45).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0449bbed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
