{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b44ee0f",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Semantic textual similarity deals with determining how similar a pair of text documents are. The goal of the first task is to implement a new architecture by combining the ideas from papers\n",
    "- Siamese Recurrent Architectures for Learning Sentence Similarity, Jonas Mueller et. al (will be referred as the AAAI paper)\n",
    "- A Structured Self-Attentive Sentence Embedding, Zhouhan Lin et. al (will be referred as the ICLR paper) <br/><br/>\n",
    "Furthermore, you'd be evaluating whether the new architecture improves the results of **Siamese Recurrent Architectures for Learning Sentence Similarity, Jonas Mueller et. al**. Your overall network architecture should look similar to the following figure. \n",
    "![Untitled%20Diagram.drawio%20%281%29.png](https://raw.githubusercontent.com/shahrukhx01/ocr-test/main/download.png)\n",
    "<br/><br/>\n",
    "\n",
    "\n",
    "Moreover, you'd be required to implement further helper functions that these papers propose i.e., attention penalty term for loss, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbed585c",
   "metadata": {},
   "source": [
    "### SICK dataset\n",
    "We will use SICK dataset throughout the project (at least in the first two tasks). To get more information about the dataset you can refer to the original [paper](http://www.lrec-conf.org/proceedings/lrec2014/pdf/363_Paper.pdf) on the dataset. You can download the dataset using one of the following links:\n",
    "- [dataset page 1](https://marcobaroni.org/composes/sick.html)\n",
    "- [dataset page 2](https://huggingface.co/datasets/sick)    \n",
    "\n",
    "The relevant columns for the project are `sentence_A`, `sentence_B`, `relatedness_score`, where `relatedness_score` is the label. <br><br>\n",
    "**Hint: For each task make sure to decide whether the label should be normalized or not.**<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c132db85",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69b52c03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:numexpr.utils:Note: NumExpr detected 16 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "INFO:numexpr.utils:NumExpr defaulting to 8 threads.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_30866/3019151540.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mevaluate_test_set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0msts_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mimportlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mreload\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/local/home/ibrahimssd/Desktop/UD_Semester_2/NNIT/Updated_NLP (semantic)/nlp_project_nnti/task1/sts_data.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpreprocess\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPreprocess\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSTSDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/pandas/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tester\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtesting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/pandas/testing.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m from pandas._testing import (\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0massert_extension_array_equal\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0massert_frame_equal\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/pandas/_testing/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0mbdate_range\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m )\n\u001b[0;32m---> 58\u001b[0;31m from pandas._testing._io import (  # noqa:F401\n\u001b[0m\u001b[1;32m     59\u001b[0m     \u001b[0mclose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0mnetwork\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/pandas/_testing/_io.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_testing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_random\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrands\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_testing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontexts\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mensure_clean\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_spec\u001b[0;34m(name, path, target)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mfind_spec\u001b[0;34m(cls, fullname, path, target)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36m_get_spec\u001b[0;34m(cls, fullname, path, target)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mfind_spec\u001b[0;34m(self, fullname, target)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36m_path_isfile\u001b[0;34m(path)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36m_path_is_mode_type\u001b[0;34m(path, mode)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36m_path_stat\u001b[0;34m(path)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from test import evaluate_test_set\n",
    "import sts_data\n",
    "from importlib import reload"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d3a701",
   "metadata": {},
   "source": [
    "## Part 1. Data pipeline (3 points)\n",
    "Before starting working on the model, we must configure the data pipeline to load the data in the correct format. Please, implement the functions for processing the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af7f4ee",
   "metadata": {},
   "source": [
    "### Part 1.1 Loading and preprocessing the data (1 point)\n",
    "Download the SICK dataset and store it in [pandas](https://pandas.pydata.org/docs/index.html) `Dataframe`'s. You should use the official data split.  \n",
    "\n",
    "Implement `load_data` method of `STSData` class in `sts_data.py`. The method must download the dataset and perform basic preprocessing. Minimal preprocessing required:  \n",
    "1. normalize text to lower case\n",
    "2. remove punctuations  \n",
    "3. remove [stopwords](https://en.wikipedia.org/wiki/Stop_word) - we provided you with the list of English stopwords.\n",
    "4. Optionally, any other preprocessing that you deem necessary.\n",
    "\n",
    "All the preprocessing code must be contained in the `preprocessing.py` file.  \n",
    "You can use Hugginface's [datasets library](https://huggingface.co/docs/datasets/) for easy dataset download."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd87f192",
   "metadata": {},
   "source": [
    "### Part 1.2 Building vocabulary (1 point)\n",
    "Before we can feed our text to the model it must be vectorized. We use 300 dimensional pretrained [FastText embeddings](https://fasttext.cc/docs/en/english-vectors.html) for mapping words to vectors. To know more general information about embeddings you can refer to [this video](https://www.youtube.com/watch?v=ERibwqs9p38) (even though, we use different types of embeddings - FastText vs Word2Vec described in the video - the general purpose of them is the same).  \n",
    "In order to apply the embedding, we must first construct the vocabulary for data. Complete the `create_vocab` method of `STSData` class in `sts_data.py` where you concatenate each sentence pair, tokenize it and construct the vocabulary for the whole training data. You should use [torchtext](https://torchtext.readthedocs.io/en/latest/data.html\n",
    ") for processing the data. For tokenization, you can use any library (or write your own tokenizer), but we recommend you to use tokenizer by [spacy](https://spacy.io/). Use the `fasttext.simple.300d` as pretrained vectors.  \n",
    "In the end, you must have a vocabulary object capable of mapping your input to corresponding vectors. Remember that the vocabulary is created using only training data (not touching validation or test data)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b90724d",
   "metadata": {},
   "source": [
    "### Part 1.3 Creating DataLoader (1 point)\n",
    "Implement `get_data_loader` method of `STSData` class in `sts_data.py`. It must perform the following operations on each of the data splits:\n",
    "1. vectorize each pair of the sentences by replacing all tokens with their index in vocabulary\n",
    "2. normalize labels\n",
    "3. convert everything to PyTorch tensors\n",
    "4. pad every sentence so that all of them have the same length\n",
    "5. create `STSDataset` from `dataset.py`\n",
    "6. create PyTorch DataLoader out of the created dataset. \n",
    "\n",
    "\n",
    "We have provided you with the interfaces of possible helper functions, but you can change them as you need.   \n",
    "In the end, you must have 3 data loaders for each of the splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b40225",
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(sts_data)\n",
    "from sts_data import STSData\n",
    "\n",
    "columns_mapping = {\n",
    "        \"sent1\": \"sentence_A\",\n",
    "        \"sent2\": \"sentence_B\",\n",
    "        \"label\": \"relatedness_score\",\n",
    "    }\n",
    "dataset_name = \"sick\"\n",
    "sick_data = STSData(\n",
    "    dataset_name=dataset_name,\n",
    "    columns_mapping=columns_mapping,\n",
    "    normalize_labels=True,\n",
    "    normalization_const=5.0,\n",
    ")\n",
    "batch_size = 64\n",
    "sick_dataloaders = sick_data.get_data_loader(batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98c6526",
   "metadata": {},
   "source": [
    "## Part 2. Model Configuration & Hyperparameter Tuning (3 points)\n",
    "In this part, you are required to define a model capable of learning self-attentive sentence embeddings described in [this ICLR paper](https://arxiv.org/pdf/1703.03130.pdf). The sentence embedding learned by this model will be used for computing the similarity score instead of the simpler embeddings described in the original AAAI paper.  \n",
    "Please familiarize yourself with the model described in the ICLR paper and implement `SiameseBiLSTMAttention` and `SelfAttention` classes in `siamese_lstm_attention.py`. Remember that you must run the model on each sentence in the sentence pair to calculate the similarity between them. You can use `similarity_score` from `utils.py` to compute the similarity score between two sentences. \n",
    "  \n",
    "To get more theoretical information about attention mechanisms you can refer to [this chapter](https://web.stanford.edu/~jurafsky/slp3/10.pdf) of [\"Speech and Language Processing\" book](https://web.stanford.edu/~jurafsky/slp3/) by Dan Jurafsky and James H. Martin, where the attention mechanism is described in the context of the machine translation task. \n",
    "\n",
    "Finally, once your implementation works on the default parameters stated below, make sure to perform **hyperparameter tuning** to find the best combination of hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a530ec28",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_size = 1\n",
    "hidden_size = 128\n",
    "vocab_size = len(sick_data.vocab)\n",
    "embedding_size = 300\n",
    "embedding_weights = sick_data.vocab.vectors\n",
    "lstm_layers = 4\n",
    "learning_rate = 0.01\n",
    "fc_hidden_size = 64\n",
    "max_epochs = 100\n",
    "bidirectional = True\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "## self attention config\n",
    "self_attention_config = {\n",
    "    \"hidden_size\": 150,  ## refers to variable 'da' in the ICLR paper\n",
    "    \"output_size\": 20,  ## refers to variable 'r' in the ICLR paper\n",
    "    \"penalty\": 1.0,  ## refers to penalty coefficient term in the ICLR paper\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a879b228",
   "metadata": {},
   "outputs": [],
   "source": [
    "## init siamese lstm\n",
    "\n",
    "from siamese_lstm_attention import SiameseBiLSTMAttention\n",
    "\n",
    "siamese_lstm_attention = SiameseBiLSTMAttention(\n",
    "    batch_size=batch_size,\n",
    "    output_size=output_size,\n",
    "    hidden_size=hidden_size,\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_size=embedding_size,\n",
    "    embedding_weights=embedding_weights,\n",
    "    lstm_layers=lstm_layers,\n",
    "    self_attention_config=self_attention_config,\n",
    "    fc_hidden_size=fc_hidden_size,\n",
    "    device=device,\n",
    "    bidirectional=bidirectional,\n",
    ")\n",
    "\n",
    "# optimizer = torch.optim.SGD(siamese_lstm_attention.parameters(), lr=0.001, momentum=0.9)\n",
    "optimizer = torch.optim.Adam(siamese_lstm_attention.parameters(),lr=learning_rate, betas=(0.9, 0.999), eps=1e-08)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n",
    "## move model to device\n",
    "siamese_lstm_attention.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b100de",
   "metadata": {},
   "source": [
    "## Part 3. Training (2 points)  \n",
    "Perform the final training of the model by implementing functions in `train.py` after setting values of your best-chosen hyperparameters. Note you can use the same training function when performing hyperparameter tuning.\n",
    "- **What is a good choice of performance metric here for evaluating your model?** [Max 2-3 lines]\n",
    "- **What other performance evaluation metric can we use here for this task? Motivate your answer.**[Max 2-3 lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6bd014a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5a31ce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from train import train_model\n",
    "import pickle\n",
    "\n",
    "siamese_lstm_attention = train_model(\n",
    "    model=siamese_lstm_attention,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    dataloader=sick_dataloaders,\n",
    "    data=sick_data,\n",
    "    max_epochs=max_epochs,\n",
    "    clip=True,\n",
    "    config_dict={\n",
    "        \"device\": device,\n",
    "        \"model_name\": \"siamese_lstm_attention\",\n",
    "        \"self_attention_config\": self_attention_config,\n",
    "    },\n",
    ")\n",
    "\n",
    "#load and save model\n",
    "model_filename = \"lstm_model.sav\"\n",
    "saved_model = pickle.dump(siamese_lstm_attention, open(model_filename,'wb'))\n",
    "print('Model is saved into to disk successfully')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47fe9cef",
   "metadata": {},
   "source": [
    "## Part 4. Evaluation and Analysis (2 points)  \n",
    "Implement function evaluate_test_set to calculate the final accuracy of the performance evaluation metric on the test data.  \n",
    "Compare the result with the original AAAI paper. Ð¡omment on effect of penalty loss on model capacity. Did the inclusion of the self-attention block improve the results? If yes, then how? Can you think of additional techniques to improve the results? Briefly answer these questions in the markdown cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143154f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "model_filename = \"lstm_model.sav\"\n",
    "lstm_model = pickle.load(open(model_filename, 'rb'))\n",
    "evaluate_test_set(\n",
    "    model=lstm_model,\n",
    "    data_loader=sick_dataloaders,\n",
    "    config_dict={\n",
    "        \"device\": device,\n",
    "        \"model_name\": \"siamese_lstm_attention\",\n",
    "        \"self_attention_config\": self_attention_config,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ff98a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
